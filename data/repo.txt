<repo-to-text>
Directory: exllamav3

Directory Structure:
<directory_structure>
.
.
├── ./convert.py
├── ./eval
│   ├── ./eval/compare_q_exllamav2.py
│   ├── ./eval/compare_q_exllamav3.py
│   ├── ./eval/compare_q_llamacpp.py
│   ├── ./eval/compare_q_logits.py
│   ├── ./eval/compare_q_transformers.py
│   ├── ./eval/compare_q.py
│   ├── ./eval/humaneval.py
│   ├── ./eval/longctx.py
│   ├── ./eval/model_diff.py
│   ├── ./eval/ppl.py
│   ├── ./eval/prequant_test.py
│   └── ./eval/spec
│       ├── ./eval/spec/llama3.1-70b-instruct_aqlm.json
│       ├── ./eval/spec/llama3.1-70b-instruct_awq.json
│       ├── ./eval/spec/llama3.1-70b-instruct_exl2.json
│       ├── ./eval/spec/llama3.1-70b-instruct_exl3.json
│       ├── ./eval/spec/llama3.1-70b-instruct_gguf.json
│       ├── ./eval/spec/llama3.1-70b-instruct_vptq.json
│       ├── ./eval/spec/llama3.1-8b-instruct_aqlm.json
│       ├── ./eval/spec/llama3.1-8b-instruct_autoround.json
│       ├── ./eval/spec/llama3.1-8b-instruct_exl2.json
│       ├── ./eval/spec/llama3.1-8b-instruct_exl3.json
│       ├── ./eval/spec/llama3.1-8b-instruct_gguf.json
│       ├── ./eval/spec/llama3.1-8b-instruct_hf.json
│       ├── ./eval/spec/llama3.1-8b-instruct_vptq.json
│       ├── ./eval/spec/llama3.2-1b-instruct_aqlm.json
│       ├── ./eval/spec/llama3.2-1b-instruct_awq.json
│       ├── ./eval/spec/llama3.2-1b-instruct_bnb.json
│       ├── ./eval/spec/llama3.2-1b-instruct_exl2.json
│       ├── ./eval/spec/llama3.2-1b-instruct_exl3.json
│       ├── ./eval/spec/llama3.2-1b-instruct_gguf.json
│       ├── ./eval/spec/mistral-7b-instruct-v0.3_awq.json
│       ├── ./eval/spec/mistral-7b-instruct-v0.3_exl2.json
│       ├── ./eval/spec/mistral-7b-instruct-v0.3_exl3.json
│       ├── ./eval/spec/mistral-7b-instruct-v0.3_gguf.json
│       ├── ./eval/spec/wiki2_llama3_large.json
│       ├── ./eval/spec/wiki2_llama3.json
│       └── ./eval/spec/wiki2_mistral_large.json
├── ./examples
│   ├── ./examples/async_generator.py
│   ├── ./examples/banned_strings.py
│   ├── ./examples/batched_translation.py
│   ├── ./examples/chat_console.py
│   ├── ./examples/chat_templates.py
│   ├── ./examples/chat_util.py
│   ├── ./examples/chat.py
│   ├── ./examples/common.py
│   ├── ./examples/dynamic_gen.py
│   ├── ./examples/generation_loop.py
│   ├── ./examples/generator.py
│   └── ./examples/loading.py
├── ./exllamav3
│   ├── ./exllamav3/__init__.py
│   ├── ./exllamav3/cache
│   │   ├── ./exllamav3/cache/__init__.py
│   │   ├── ./exllamav3/cache/cache.py
│   │   ├── ./exllamav3/cache/fp16.py
│   │   └── ./exllamav3/cache/quant.py
│   ├── ./exllamav3/constants.py
│   ├── ./exllamav3/conversion
│   │   ├── ./exllamav3/conversion/__init__.py
│   │   ├── ./exllamav3/conversion/allocation.py
│   │   ├── ./exllamav3/conversion/calibration_data.py
│   │   ├── ./exllamav3/conversion/compile.py
│   │   ├── ./exllamav3/conversion/convert_model.py
│   │   └── ./exllamav3/conversion/quant_config.py
│   ├── ./exllamav3/device.py
│   ├── ./exllamav3/exllamav3_ext
│   │   ├── ./exllamav3/exllamav3_ext/activation.cu
│   │   ├── ./exllamav3/exllamav3_ext/activation.cuh
│   │   ├── ./exllamav3/exllamav3_ext/bindings.cpp
│   │   ├── ./exllamav3/exllamav3_ext/cache
│   │   │   ├── ./exllamav3/exllamav3_ext/cache/q_cache_kernels.cuh
│   │   │   ├── ./exllamav3/exllamav3_ext/cache/q_cache.cu
│   │   │   └── ./exllamav3/exllamav3_ext/cache/q_cache.cuh
│   │   ├── ./exllamav3/exllamav3_ext/compat.cuh
│   │   ├── ./exllamav3/exllamav3_ext/generator
│   │   │   ├── ./exllamav3/exllamav3_ext/generator/cache.cu
│   │   │   ├── ./exllamav3/exllamav3_ext/generator/cache.cuh
│   │   │   ├── ./exllamav3/exllamav3_ext/generator/gumbel.cu
│   │   │   ├── ./exllamav3/exllamav3_ext/generator/gumbel.cuh
│   │   │   ├── ./exllamav3/exllamav3_ext/generator/rep_pen.cu
│   │   │   ├── ./exllamav3/exllamav3_ext/generator/rep_pen.cuh
│   │   │   ├── ./exllamav3/exllamav3_ext/generator/sampling_basic.cu
│   │   │   ├── ./exllamav3/exllamav3_ext/generator/sampling_basic.cuh
│   │   │   ├── ./exllamav3/exllamav3_ext/generator/strings.cpp
│   │   │   └── ./exllamav3/exllamav3_ext/generator/strings.h
│   │   ├── ./exllamav3/exllamav3_ext/hadamard.cpp
│   │   ├── ./exllamav3/exllamav3_ext/hadamard.h
│   │   ├── ./exllamav3/exllamav3_ext/hgemm.cu
│   │   ├── ./exllamav3/exllamav3_ext/hgemm.cuh
│   │   ├── ./exllamav3/exllamav3_ext/histogram.cu
│   │   ├── ./exllamav3/exllamav3_ext/histogram.cuh
│   │   ├── ./exllamav3/exllamav3_ext/libtorch
│   │   │   ├── ./exllamav3/exllamav3_ext/libtorch/blocksparse_mlp.cpp
│   │   │   └── ./exllamav3/exllamav3_ext/libtorch/blocksparse_mlp.h
│   │   ├── ./exllamav3/exllamav3_ext/norm.cu
│   │   ├── ./exllamav3/exllamav3_ext/norm.cuh
│   │   ├── ./exllamav3/exllamav3_ext/ptx.cuh
│   │   ├── ./exllamav3/exllamav3_ext/quant
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/codebook.cuh
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/comp_units
│   │   │   │   ├── ./exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_1.cu
│   │   │   │   ├── ./exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_1.cuh
│   │   │   │   ├── ./exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_2.cu
│   │   │   │   ├── ./exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_2.cuh
│   │   │   │   ├── ./exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_3.cu
│   │   │   │   ├── ./exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_3.cuh
│   │   │   │   ├── ./exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_4.cu
│   │   │   │   ├── ./exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_4.cuh
│   │   │   │   ├── ./exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_5.cu
│   │   │   │   ├── ./exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_5.cuh
│   │   │   │   ├── ./exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_6.cu
│   │   │   │   ├── ./exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_6.cuh
│   │   │   │   ├── ./exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_7.cu
│   │   │   │   ├── ./exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_7.cuh
│   │   │   │   ├── ./exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_8.cu
│   │   │   │   └── ./exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_8.cuh
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/exl3_devctx.cu
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/exl3_devctx.cuh
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/exl3_dq.cuh
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/exl3_gemm_inner.cuh
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/exl3_gemm_kernel.cuh
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/exl3_gemm.cu
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/exl3_gemm.cuh
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/exl3_kernel_map.cu
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/exl3_kernel_map.cuh
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/hadamard_inner.cuh
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/hadamard.cu
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/hadamard.cuh
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/pack.cu
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/pack.cuh
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/quantize.cu
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/quantize.cuh
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/reconstruct.cu
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/reconstruct.cuh
│   │   │   ├── ./exllamav3/exllamav3_ext/quant/util.cu
│   │   │   └── ./exllamav3/exllamav3_ext/quant/util.cuh
│   │   ├── ./exllamav3/exllamav3_ext/reduction.cuh
│   │   ├── ./exllamav3/exllamav3_ext/rope.cu
│   │   ├── ./exllamav3/exllamav3_ext/rope.cuh
│   │   ├── ./exllamav3/exllamav3_ext/softcap.cu
│   │   ├── ./exllamav3/exllamav3_ext/softcap.cuh
│   │   ├── ./exllamav3/exllamav3_ext/stloader_cu.cu
│   │   ├── ./exllamav3/exllamav3_ext/stloader_cu.cuh
│   │   ├── ./exllamav3/exllamav3_ext/stloader.cpp
│   │   ├── ./exllamav3/exllamav3_ext/stloader.h
│   │   ├── ./exllamav3/exllamav3_ext/util.cuh
│   │   └── ./exllamav3/exllamav3_ext/util.h
│   ├── ./exllamav3/ext.py
│   ├── ./exllamav3/generator
│   │   ├── ./exllamav3/generator/__init__.py
│   │   ├── ./exllamav3/generator/async_generator.py
│   │   ├── ./exllamav3/generator/generator.py
│   │   ├── ./exllamav3/generator/job.py
│   │   ├── ./exllamav3/generator/pagetable.py
│   │   ├── ./exllamav3/generator/sampler
│   │   │   ├── ./exllamav3/generator/sampler/__init__.py
│   │   │   ├── ./exllamav3/generator/sampler/custom.py
│   │   │   ├── ./exllamav3/generator/sampler/presets.py
│   │   │   └── ./exllamav3/generator/sampler/sampler.py
│   │   └── ./exllamav3/generator/visualizer.py
│   ├── ./exllamav3/loader
│   │   ├── ./exllamav3/loader/__init__.py
│   │   └── ./exllamav3/loader/safetensors.py
│   ├── ./exllamav3/model_init.py
│   ├── ./exllamav3/models
│   │   ├── ./exllamav3/models/__init__.py
│   │   ├── ./exllamav3/models/architectures.py
│   │   ├── ./exllamav3/models/cohere.py
│   │   ├── ./exllamav3/models/cohere2.py
│   │   ├── ./exllamav3/models/config.py
│   │   ├── ./exllamav3/models/decilm.py
│   │   ├── ./exllamav3/models/gemma2.py
│   │   ├── ./exllamav3/models/gemma3.py
│   │   ├── ./exllamav3/models/glm4.py
│   │   ├── ./exllamav3/models/llama.py
│   │   ├── ./exllamav3/models/mistral.py
│   │   ├── ./exllamav3/models/mixtral.py
│   │   ├── ./exllamav3/models/model.py
│   │   ├── ./exllamav3/models/phi3.py
│   │   ├── ./exllamav3/models/qwen2.py
│   │   ├── ./exllamav3/models/qwen3_moe.py
│   │   └── ./exllamav3/models/qwen3.py
│   ├── ./exllamav3/modules
│   │   ├── ./exllamav3/modules/__init__.py
│   │   ├── ./exllamav3/modules/attn.py
│   │   ├── ./exllamav3/modules/block_sparse_mlp.py
│   │   ├── ./exllamav3/modules/embedding.py
│   │   ├── ./exllamav3/modules/layernorm.py
│   │   ├── ./exllamav3/modules/linear.py
│   │   ├── ./exllamav3/modules/mlp.py
│   │   ├── ./exllamav3/modules/module.py
│   │   ├── ./exllamav3/modules/multilinear.py
│   │   ├── ./exllamav3/modules/quant
│   │   │   ├── ./exllamav3/modules/quant/__init__.py
│   │   │   ├── ./exllamav3/modules/quant/exl3_lib
│   │   │   │   ├── ./exllamav3/modules/quant/exl3_lib/__init__.py
│   │   │   │   └── ./exllamav3/modules/quant/exl3_lib/quantize.py
│   │   │   ├── ./exllamav3/modules/quant/exl3.py
│   │   │   └── ./exllamav3/modules/quant/fp16.py
│   │   ├── ./exllamav3/modules/rmsnorm.py
│   │   └── ./exllamav3/modules/transformer.py
│   ├── ./exllamav3/tokenizer
│   │   ├── ./exllamav3/tokenizer/__init__.py
│   │   └── ./exllamav3/tokenizer/tokenizer.py
│   ├── ./exllamav3/util
│   │   ├── ./exllamav3/util/__init__.py
│   │   ├── ./exllamav3/util/arch_list.py
│   │   ├── ./exllamav3/util/file.py
│   │   ├── ./exllamav3/util/hadamard_data
│   │   │   ├── ./exllamav3/util/hadamard_data/hadamard_1.txt
│   │   │   ├── ./exllamav3/util/hadamard_data/hadamard_100.txt
│   │   │   ├── ./exllamav3/util/hadamard_data/hadamard_116.txt
│   │   │   ├── ./exllamav3/util/hadamard_data/hadamard_156.txt
│   │   │   ├── ./exllamav3/util/hadamard_data/hadamard_172.txt
│   │   │   ├── ./exllamav3/util/hadamard_data/hadamard_188.txt
│   │   │   ├── ./exllamav3/util/hadamard_data/hadamard_236.txt
│   │   │   ├── ./exllamav3/util/hadamard_data/hadamard_244.txt
│   │   │   ├── ./exllamav3/util/hadamard_data/hadamard_428.txt
│   │   │   ├── ./exllamav3/util/hadamard_data/hadamard_52.txt
│   │   │   ├── ./exllamav3/util/hadamard_data/hadamard_92.txt
│   │   │   └── ./exllamav3/util/hadamard_data/primes.txt
│   │   ├── ./exllamav3/util/hadamard.py
│   │   ├── ./exllamav3/util/measures.py
│   │   ├── ./exllamav3/util/memory.py
│   │   ├── ./exllamav3/util/misc.py
│   │   ├── ./exllamav3/util/profile_opt.py
│   │   ├── ./exllamav3/util/progress.py
│   │   ├── ./exllamav3/util/rope.py
│   │   └── ./exllamav3/util/tensor.py
│   └── ./exllamav3/version.py
├── ./LICENSE
├── ./MANIFEST.in
├── ./README.md
├── ./requirements_eval.txt
├── ./requirements_examples.txt
├── ./requirements.txt
├── ./science
│   ├── ./science/codebook_eval.py
│   ├── ./science/gumbel_eval.py
│   ├── ./science/kv_quant_exp.py
│   └── ./science/qgemm_benchmark.py
├── ./setup.py
├── ./tests
│   ├── ./tests/generator_stresstest.py
│   ├── ./tests/test_cache_rotate.py
│   ├── ./tests/test_ext_norm.py
│   ├── ./tests/test_kv_quant.py
│   ├── ./tests/test_qgemm.py
│   ├── ./tests/test_quant_fn.py
│   ├── ./tests/test_rope.py
│   ├── ./tests/test_sampler.py
│   └── ./tests/util.py
└── ./util
    └── ./util/add_quant_config.py

</directory_structure>

<content full_path="LICENSE">
MIT License

Copyright (c) 2025 Turboderp

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</content>

<content full_path="requirements.txt">
torch>=2.6.0
flash_attn>=2.7.4.post1
tokenizers>=0.21.1
numpy>=2.1.0
rich
typing_extensions
safetensors>=0.3.2
ninja
</content>

<content full_path="convert.py">
from exllamav3.conversion.convert_model import parser, main, prepare

# Script included in package: ./exllamav3/conversion/convert_model.py

if __name__ == "__main__":
    _args = parser.parse_args()
    _in_args, _job_state, _ok, _err = prepare(_args)
    if not _ok:
        print(f" !! Error: {_err}")
    else:
        main(_in_args, _job_state)
</content>

<content full_path="MANIFEST.in">
include exllamav3/util/hadamard_data/*
recursive-include exllamav3/exllamav3_ext *
</content>

<content full_path="requirements_eval.txt">
transformers>=4.50.0
adjustText>=1.3.0
aqlm>=1.1.6
autoawq>=0.2.8
bitsandbytes>=0.45.4
datasets
exllamav2>=0.2.8
gguf>=0.14.0
human_eval==1.0.3
matplotlib>=3.10.0
vptq>=0.0.5
tabulate

</content>

<content full_path="README.md">

# <img src="doc/cat.png" width="40"> ExLlamaV3

ExLlamaV3 is still in development. Please note: ↙

- The framework <u>is not yet fully optimized</u>. Performance is lacking, especially on Ampere, and there may be a significant CPU bottleneck on slower processors until the extension functions are fully built out.
- AMD GPUs (ROCm) are not yet supported.
- [FlashAttention-2](https://github.com/Dao-AILab/flash-attention) is currently required. I hope to switch over to [FlashInfer](https://github.com/flashinfer-ai/flashinfer/tree/main) in time, but there are some obstacles to overcome first. 
- A number of important features are yet to be added, such as tensor parallelism and multimodal support.

## Why?

As the name implies, the original intention for ExLlama was to run inference on quantized Llama models. ExLlamaV2 was able to support a number of other architectures by treating every new model as (more or less) a Llama variant with optional features. However, as new models are increasingly moving away from the basic transformer template, this approach is no longer sustainable.  

Additionally, ExLlamaV2 is largely designed to run in a single process and CUDA doesn't like this very much when spreading a workload across multiple GPUs. It's a fundamental design feature in the CUDA runtime, and it has become a major obstacle to tensor-parallel inference, demand for which seems to keep increasing. This shortcoming is not easily addressed without a rewrite. Moreover, the **EXL2** format doesn't lend itself well to parallel inference in the first place due its input channel permutation.

 Aside from lifting a few of the most successful features from V2 (such as the generator), ExLlamaV3 is largely rewritten from scratch to provide a cleaner, more modular framework for supporting newer architectures. It also introduces a new SOTA quantization format based on [**QTIP**](https://github.com/Cornell-RelaxML/qtip) (see below).

## What's missing?

There's much that still needs to be added and/or ported over from ExLlamaV2. I've decided to release ExLlamaV3 in its current state to invite testing, feedback and contributions, but please be aware that it's not yet a viable replacement for ExLlamaV2. Currently on the to-do list:

- Support for more architectures
- Constrained sampling (JSON filters etc.)
- Multimodal support
- LoRA support
- ROCm support
- Tensor-parallel inference
- Lots of optimization

As for what is implemented, expect that some things may be a little broken at first. Please be patient and/or contribute. 👉👈 

## How to?

### Installation

Detailed installation instructions are coming soon, along with prebuilt wheels. For the time being, start by making sure you have the appropriate version of [PyTorch](https://pytorch.org/get-started/locally/) installed (CUDA 12.4 or later). Then:

```sh
# Full installation
pip install -r requirements.txt
pip install .

# JIT mode
EXLLAMA_NOCOMPILE=1 pip install . 
```

Note that the included scripts can run in JIT mode from the repo directory without installing the library. Until there are precompiled wheels you will also need the CUDA Toolkit installed. 

### Conversion

To convert a model to EXL3 format, use:

```sh
# Convert model
python convert.py -i <input_dir> -o <output_dir> -w <working_dir> -b <bitrate>

# Resume an interrupted quant job
convert.py -w <working_dir> -r

# More options
convert.py -h
```

The working directory is temporary storage for state checkpoints and for storing quantized tensors until the converted model can be compiled. It should have enough free space to store an entire copy of the output model. Note that while EXL2 conversion by default resumes an interrupted job when pointed to an existing folder, EXL3 needs you to explicitly resume with the `-r`/`--resume` argument.    

See [here](doc/convert.md) for more information.

### Examples

A number of example scripts are provided to showcase the features of the backend and generator. Some of them have hardcoded model paths and should be edited before you run them, but there is a simple CLI chatbot that you can start with:

```sh
python examples/chat.py -m <input_dir> -mode <prompt_mode> 

# E.g.:
python examples/chat.py -m /mnt/models/llama3.1-8b-instruct-exl3 -mode llama3
```

## EXL3 quantization

<div align="center">
    <a href="doc/exl3.md" target="_blank">
        <img src="doc/llama31_8b_instruct_bpw.png" width="640">
    </a>
</div>

Despite their amazing achievements, most SOTA quantization techniques remain cumbersome or even prohibitively expensive to use. For instance, **AQLM** quantization of a 70B model takes around **720 GPU-hours** on an A100 server, costing $850 US at the time of writing. ExLlamaV3 aims to address this with the **EXL3** format, which is a streamlined variant of [**QTIP**](https://github.com/Cornell-RelaxML/qtip) from Cornell RelaxML. The conversion process is designed to be simple and efficient and requires only an input model (in HF format) and a target bitrate. By computing Hessians on the fly and thanks to a fused Viterbi kernel, the quantizer can convert a model in a single step, taking a couple of minutes for smaller models, up to a few hours for larger ones (70B+) (on a single RTX 4090 or equivalent GPU.)

The [Marlin](https://github.com/IST-DASLab/marlin)-inspired GEMM kernel achieves roughly memory-bound latency under optimal conditions (4bpw, RTX 4090), though it still needs some work to achieve the same efficiency on Ampere GPUs and to remain memory-bound at lower bitrates.

Since converted models largely retain the original file structure (unlike **EXL2** which renames some tensors in its quest to turn every model into a Llama variant), it will be possible to extend **EXL3** support to other frameworks like HF Transformers and vLLM.

There are some benchmark results [here](doc/exl3.md), and a full writeup on the format is coming soon.

Fun fact: Llama-3.1-70B-EXL3 is coherent at 1.6 bpw. With the output layer quantized to 3 bpw and a 4096-token cache, inference is possible in under 16 GB of VRAM. 

A selection of EXL3-quantized models is available on [🤗 Hugging Face](https://huggingface.co/collections/turboderp/exl3-models-67f2dfe530f05cb9f596d21a).


## Acknowledgements

This project owes its existence to a wonderful community of FOSS developers and some very generous supporters (🐈❤️!) The following projects in particular deserve a special mention:

- [TabbyAPI](https://github.com/theroyallab/tabbyAPI/)
- [PyTorch](https://github.com/pytorch/pytorch)
- [FlashAttention](https://github.com/Dao-AILab/flash-attention)
- [QTIP](https://github.com/Cornell-RelaxML/qtip)
- [Transformers](https://github.com/huggingface/transformers)
- [Marlin](https://github.com/IST-DASLab/marlin)
</content>

<content full_path="setup.py">
from setuptools import setup
import importlib.util
import os

if torch := importlib.util.find_spec("torch") is not None:
    from torch.utils import cpp_extension
    from torch import version as torch_version

extension_name = "exllamav3_ext"
precompile = "EXLLAMA_NOCOMPILE" not in os.environ
verbose = "EXLLAMA_VERBOSE" in os.environ
ext_debug = "EXLLAMA_EXT_DEBUG" in os.environ

if precompile and not torch:
    print("Cannot precompile unless torch is installed.")
    print("To explicitly JIT install run EXLLAMA_NOCOMPILE= pip install <xyz>")

windows = os.name == "nt"

extra_cflags = ["/Ox"] if windows else ["-O3"]

if ext_debug:
    extra_cflags += ["-ftime-report", "-DTORCH_USE_CUDA_DSA"]

extra_cuda_cflags = ["-lineinfo", "-O3"]

if torch and torch_version.hip:
    extra_cuda_cflags += ["-DHIPBLAS_USE_HIP_HALF"]

extra_compile_args = {
    "cxx": extra_cflags,
    "nvcc": extra_cuda_cflags,
}

library_dir = "exllamav3"
sources_dir = os.path.join(library_dir, extension_name)
sources = [
    os.path.relpath(os.path.join(root, file), start=os.path.dirname(__file__))
    for root, _, files in os.walk(sources_dir)
    for file in files
    if file.endswith(('.c', '.cpp', '.cu'))
]

print (sources)

setup_kwargs = (
    {
        "ext_modules": [
            cpp_extension.CUDAExtension(
                extension_name,
                sources,
                extra_compile_args=extra_compile_args,
                libraries=["cublas"] if windows else [],
            )
        ],
        "cmdclass": {"build_ext": cpp_extension.BuildExtension},
    }
    if precompile and torch
    else {}
)

version_py = {}
with open("exllamav3/version.py", encoding="utf8") as fp:
    exec(fp.read(), version_py)
version = version_py["__version__"]
print("Version:", version)

setup(
    name="exllamav3",
    version=version,
    packages=[
        "exllamav3",
        "exllamav3.generator",
        "exllamav3.generator.sampler",
        "exllamav3.conversion",
        "exllamav3.models",
        "exllamav3.modules",
        "exllamav3.modules.quant",
        "exllamav3.modules.quant.exl3_lib",
        "exllamav3.tokenizer",
        "exllamav3.cache",
        "exllamav3.loader",
        "exllamav3.util",
    ],
    url="https://github.com/turboderp/exllamav3",
    license="MIT",
    author="turboderp",
    install_requires=[
        "torch>=2.6.0",
        "flash_attn>=2.7.4.post1",
        "tokenizers>=0.21.1",
        "numpy>=2.1.0",
        "rich",
        "typing_extensions",
        "ninja",
        "safetensors>=0.3.2"
    ],
    include_package_data=True,
    package_data = {
        "": ["py.typed"],
    },
    verbose=verbose,
    **setup_kwargs,
)

</content>

<content full_path="requirements_examples.txt">
blessed
prompt_toolkit
pyperclip
</content>

<content full_path="exllamav3/version.py">
__version__ = "0.0.3"
</content>

<content full_path="exllamav3/device.py">
from __future__ import annotations
import torch
import torch.nn.functional as F
from torch import nn
from .constants import PAGE_SIZE
from .models import Config

device_contexts = {}

class DeviceContext:

    def __init__(
        self,
        config: Config,
        device: torch.Device,
    ):
        self.reference_count = 0
        self.device = device
        self.config = config


def get_key(
    config: Config,
    device: torch.Device,
):
    return f"{str(config.uuid)},{str(device)}"


def get_device_context(config: Config, device: torch.device):
    key = get_key(config, device)
    if key not in device_contexts:
        device_contexts[key] = DeviceContext(config, device)
    dc = device_contexts[key]
    dc.reference_count += 1
    return dc


def release_device_context(config: Config, device: torch.device):
    key = get_key(config, device)
    assert key in device_contexts
    dc = device_contexts[key]
    dc.reference_count -= 1
    if dc.reference_count == 0:
        del device_contexts[key]

</content>

<content full_path="exllamav3/constants.py">
# Fixed page size for generator
PAGE_SIZE = 256

# Maximum MLP size we can realistically quantize on one GPU. Wider MLP layers are split while quantizing
MAX_MLP_INTERMEDIATE = 55296
</content>

<content full_path="exllamav3/__init__.py">
from .models.config import Config
from .models.model import Model
from .tokenizer import Tokenizer
from .cache import Cache, CacheLayer_fp16, CacheLayer_quant
from .generator import Generator, Job, AsyncGenerator, AsyncJob
from .generator.sampler import *
</content>

<content full_path="exllamav3/model_init.py">
from . import Model, Config, Cache, Tokenizer
from .cache import CacheLayer_fp16, CacheLayer_quant
from argparse import ArgumentParser
import torch

def add_args(
    parser: ArgumentParser,
    cache: bool = True,
    default_cache_size = 8192,
):
    """
    Add standard model loading arguments to command line parser

    :param parser:
        argparse.ArgumentParser

    :param cache:
        bool, include cache arguments. If present, model_init.init() will also return cache

    :param default_cache_size:
        Default value for -cs / --cache_size argument
    """
    parser.add_argument("-m", "--model_dir", type = str, help = "Path to model directory", required = True)
    parser.add_argument("-gs", "--gpu_split", type = str, help = "Maximum amount of VRAM to use per device, in GB.")
    parser.add_argument("-lm", "--load_metrics", action = "store_true", help = "Show metrics from loader")

    if cache:
        parser.add_argument("-cs", "--cache_size", type = int, help = f"Total cache size in tokens, default: {default_cache_size}", default = default_cache_size)
        parser.add_argument("-cq", "--cache_quant", type = str, help = "Use quantized cache. Specify either kv_bits or k_bits,v_bits pair")

    # TODO:
    # parser.add_argument("-tp", "--tensor_parallel", action = "store_true", help = "Load in tensor-parallel mode")


def init(
    args,
    load_tokenizer: bool = True,
    quiet: bool = False,
    progress: bool = True,
    override_dynamic_seq_len: int | None = None,
    **kwargs
):
    """
    Create

    :param args:
        argparse.Namespace returned by parse_args()

    :param load_tokenizer:
        bool, also load tokenizer

    :param quiet:
        bool, no console output

    :param progress:
        bool, show rich progress bar while loading

    :param override_dynamic_seq_len:
        (optional) Some models (Like Phi4) have two RoPE modes and adjust their positional embeddings depending on
        sequence length. This argument sets the expected max context length to help select the right mode at load time.
        Mostly relevant if you know ahead of time that you're going to use a long-context model with a short context.

    :param kwargs:
        Additional parametes to forwart to Model.load()

    :return:
        tuple of (Model, Config, Cache | None, Tokenizer | None)
    """

    def printp(p: bool, s: str):
        if p: print(s)

    # Config
    config = Config.from_directory(args.model_dir)
    if override_dynamic_seq_len: config.override_dynamic_seq_len(override_dynamic_seq_len)
    model = Model.from_config(config)

    # Cache
    if "cache_size" in vars(args):
        if args.cache_quant is not None:
            split = [int(bits) for bits in args.cache_quant.split(",")]
            if len(split) == 1:
                k_bits = v_bits = split[0]
            elif len(split) == 2:
                k_bits, v_bits = tuple(split)
            else:
                raise ValueError("Specify either one or two bitrates for cache quantization")
            cache = Cache(
                model,
                max_num_tokens = args.cache_size,
                layer_type = CacheLayer_quant,
                k_bits = k_bits,
                v_bits = v_bits
            )
        else:
            cache = Cache(
                model,
                max_num_tokens = args.cache_size,
                layer_type = CacheLayer_fp16
            )
    else:
        cache = None

    # Split
    if args.gpu_split is None or args.gpu_split == "auto":
        split = None
    else:
        split = [float(alloc) for alloc in args.gpu_split.split(",")]

    # Load model
    printp(not quiet, f" -- Loading {args.model_dir}")
    model.load(use_per_device = split, progressbar = progress, **kwargs)

    # Load tokenizer
    if load_tokenizer:
        printp(not quiet, f" -- Loading tokenizer...")
        tokenizer = Tokenizer.from_config(config)
    else:
        tokenizer = None

    # Metrics
    if args.load_metrics:
        config.stc.metrics.print()

    return model, config, cache, tokenizer
</content>

<content full_path="exllamav3/ext.py">
from __future__ import annotations
import torch
from torch.utils.cpp_extension import load
import os, glob
import sys
import platform
import threading
from .util.arch_list import maybe_set_arch_list_env

extension_name = "exllamav3_ext"
verbose = False  # Print wall of text when compiling
ext_debug = False  # Compile with debug options

# Determine if we're on Windows

windows = (os.name == "nt")

# Determine if extension is already installed or needs to be built

build_jit = False
try:
    import exllamav3_ext
except ModuleNotFoundError:
    build_jit = True

if build_jit:

    # Kludge to get compilation working on Windows

    if windows:

        def find_msvc():

            # Possible locations for MSVC, in order of preference

            program_files_x64 = os.environ["ProgramW6432"]
            program_files_x86 = os.environ["ProgramFiles(x86)"]

            msvc_dirs = \
            [
                a + "\\Microsoft Visual Studio\\" + b + "\\" + c + "\\VC\\Tools\\MSVC\\"
                for b in ["2022", "2019", "2017"]
                for a in [program_files_x64, program_files_x86]
                for c in ["BuildTools", "Community", "Professional", "Enterprise", "Preview"]
            ]

            for msvc_dir in msvc_dirs:
                if not os.path.exists(msvc_dir): continue

                # Prefer the latest version

                versions = sorted(os.listdir(msvc_dir), reverse = True)
                for version in versions:

                    compiler_dir = msvc_dir + version + "\\bin\\Hostx64\\x64"
                    if os.path.exists(compiler_dir) and os.path.exists(compiler_dir + "\\cl.exe"):
                        return compiler_dir

            # No path found

            return None

        import subprocess

        # Check if cl.exe is already in the path

        try:

            subprocess.check_output(["where", "/Q", "cl"])

        # If not, try to find an installation of Visual Studio and append the compiler dir to the path

        except subprocess.CalledProcessError as e:

            cl_path = find_msvc()
            if cl_path:
                if verbose:
                    print(" -- Injected compiler path:", cl_path)
                os.environ["path"] += ";" + cl_path
            else:
                print(" !! Unable to find cl.exe; compilation will probably fail", file = sys.stderr)

    # gcc / cl.exe flags

    if windows:
        extra_cflags = ["/Ox"]
    else:
        extra_cflags = ["-Ofast"]

    if ext_debug:
        extra_cflags += ["-ftime-report", "-DTORCH_USE_CUDA_DSA"]

    # nvcc flags

    extra_cuda_cflags = ["-lineinfo", "-O3"]

    if torch.version.hip:
        extra_cuda_cflags += ["-DHIPBLAS_USE_HIP_HALF"]

    if verbose:
        extra_cuda_cflags += ["--ptxas-options=-v"]

    # linker flags

    extra_ldflags = []

    if windows:
        extra_ldflags += ["cublas.lib"]
        if sys.base_prefix != sys.prefix:
            extra_ldflags += [f"/LIBPATH:{os.path.join(sys.base_prefix, 'libs')}"]

    # sources

    library_dir = os.path.dirname(os.path.abspath(__file__))
    sources_dir = os.path.join(library_dir, extension_name)
    sources = [
        os.path.abspath(os.path.join(root, file))
        for root, _, files in os.walk(sources_dir)
        for file in files
        if file.endswith(('.c', '.cpp', '.cu'))
    ]

    # Load extension

    maybe_set_arch_list_env()
    exllamav3_ext = load(
        name = extension_name,
        sources = sources,
        extra_include_paths = [sources_dir],
        verbose = verbose,
        extra_ldflags = extra_ldflags,
        extra_cuda_cflags = extra_cuda_cflags,
        extra_cflags = extra_cflags
    )

</content>

<content full_path="exllamav3/cache/fp16.py">
from __future__ import annotations
from typing_extensions import override
import torch
import torch.nn.functional as F
from torch import nn
from ..constants import PAGE_SIZE
from ..models import Model, Config
from .cache import CacheLayer
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from ..modules import Attention

class CacheLayer_fp16(CacheLayer):

    def __init__(
        self,
        config: Config,
        attention: Attention,
        max_num_tokens: int,
    ):
        super().__init__(config, attention, max_num_tokens)

        assert max_num_tokens % PAGE_SIZE == 0, \
            f"max_num_tokens must be a multiple of {PAGE_SIZE}."

        self.shape = (
            (max_num_tokens // PAGE_SIZE, PAGE_SIZE, attention.num_kv_heads, attention.head_dim)
            if attention else None
        )
        self.k = None
        self.v = None
        self.device = None


    @override
    def alloc(self, device: torch.device):
        self.device = device
        self.k = torch.zeros(self.shape, dtype = torch.half, device = device) if self.shape else None
        self.v = torch.zeros(self.shape, dtype = torch.half, device = device) if self.shape else None


    @override
    def free(self):
        self.device = None
        self.k = None
        self.v = None


    @override
    def get_kv(self, cache_seqlens: torch.Tensor, block_table: torch.Tensor) -> tuple:
        return self.k, self.v


    @override
    def update_kv(
        self,
        cache_seqlens: torch.Tensor,
        block_table: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        length: int
    ):
        pass


    @override
    def copy_page(self, source: CacheLayer_fp16, from_page: int, to_page: int, num_tokens: int):
        assert self.shape == source.shape
        self.k[to_page, :num_tokens, :, :].copy_(source.k[from_page, :num_tokens, :, :], non_blocking = True)
        self.v[to_page, :num_tokens, :, :].copy_(source.v[from_page, :num_tokens, :, :], non_blocking = True)

    @override
    def get_tensors(self):
        return [self.k, self.v]

</content>

<content full_path="exllamav3/cache/cache.py">
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Type
import torch
import torch.nn.functional as F
from torch import nn
from ..models import Model, Config
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from ..modules import Attention

class CacheLayer(ABC):

    def __init__(
        self,
        config: Config,
        attention: Attention,
        max_num_tokens: int,
        **kwargs
    ):
        self.config = config
        self.attention = attention
        self.max_num_tokens = max_num_tokens

    @abstractmethod
    def alloc(self, device: torch.device):
        pass

    @abstractmethod
    def free(self):
        pass

    @abstractmethod
    def get_kv(self, cache_seqlens: torch.Tensor, block_table: torch.Tensor) -> tuple:
        pass

    @abstractmethod
    def update_kv(
        self,
        cache_seqlens: torch.Tensor,
        block_table: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        length: int
    ):
        pass

    @abstractmethod
    def copy_page(self, source: CacheLayer, from_page: int, to_page: int, num_tokens: int):
        pass

    @abstractmethod
    def get_tensors(self):
        pass


class Cache:

    def __init__(
        self,
        model: Model,
        max_num_tokens: int,
        layer_type: Type[CacheLayer] | None = None,
        **kwargs
    ):
        """
        Create cache for model

        :param model:
            Model for which to create the cache. Once created, the cache is tied to the model. Loading the model
            will create cache tensors and unloading the model will destroy them. To delete the cache itself without
            deleting the reference to the model, use detach_from_model

        :param layer_type:
            Cache layer class, CacheLayer_fp16 or CacheLayer_quant

        :param max_num_tokens:
            Max number of total tokens in the cache. Must be a multiple of the page size (256). For use with the
            dynamic generator, this is the total number of tokens that can be allocated across concurrent jobs. For
            batched inference, seq_len * batch_size <= max_num_tokens

        :param k_bits:
            If layer_type == CacheLayer_quant, bits per element of the quantized keys tensor

        :param v_bits:
            If layer_type == CacheLayer_quant, bits per element of the quantized values tensor

        """
        self.model = model
        self.config = model.config
        self.max_num_tokens = max_num_tokens

        from .fp16 import CacheLayer_fp16
        self.layer_type = layer_type or CacheLayer_fp16

        self.num_layers = len(self.model.get_cache_layers())
        self.layers = [
            self.layer_type(self.config, attn, self.max_num_tokens, **kwargs)
            for attn in self.model.get_cache_layers()
        ]
        self.attach_to_model()


    def attach_to_model(self, model: Model | None = None):
        """
        Attach cache to model. Registering the cache with the model (done automatically by the Cache constructor)
        is necessary in order to tie loading of the model to allocation of cache tensors. Multiple caches can be
        attached to the same model.
        """
        if model is None:
            model = self.model
        model_num_layers = len(model.get_cache_layers())
        assert model_num_layers == self.num_layers, \
            f"Cannot attach cache with {self.num_layers} layers to model with {model_num_layers} layers."
        for layer, module in zip(self.layers, model.get_cache_layers()):
            assert layer not in module.cache_layers, \
                "Cannot attach cache twice to the same model."
            module.cache_layers.append(layer)


    def detach_from_model(self, model: Model | None = None):
        """
        Detach cache from model. Must be called if you want to delete a cache without deleting the model.
        """
        if model is None:
            model = self.model
        model_num_layers = len(model.get_cache_layers())
        assert model_num_layers == self.num_layers, \
            f"Cannot detach cache with {self.num_layers} layers from model with {model_num_layers()} layers."
        for layer, module in zip(self.layers, model.get_cache_layers()):
            module.cache_layers.remove(layer)


    def get_layer(self, idx: int, cache_seqlens: torch.Tensor, block_table: torch.Tensor) -> tuple:
        return self.layers[idx].get_kv(cache_seqlens, block_table)


    def update_layer(
        self,
        idx: int,
        cache_seqlens: torch.Tensor,
        block_table: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        length: int
    ):
        return self.layers[idx].update_kv(cache_seqlens, block_table, k, v, length)


    def copy_page(
        self,
        target: Cache,
        from_page: int,
        to_page: int,
        num_tokens: int,
    ):
        assert target.num_layers == self.num_layers
        for src, dst in zip(target.layers, self.layers):
            assert type(src) is type(dst)
            dst.copy_page(src, from_page, to_page, num_tokens)


    def get_all_tensors(self):
        tensors = []
        for layer in self.layers:
            tensors += layer.get_tensors()
        return tensors

</content>

<content full_path="exllamav3/cache/__init__.py">
from .cache import Cache, CacheLayer
from .fp16 import CacheLayer_fp16
from .quant import CacheLayer_quant
</content>

<content full_path="exllamav3/cache/quant.py">
from __future__ import annotations
from typing_extensions import override
import torch
import torch.nn.functional as F
from torch import nn
from ..constants import PAGE_SIZE
from ..models import Model, Config
from .cache import CacheLayer
from typing import TYPE_CHECKING
from exllamav3.ext import exllamav3_ext as ext
if TYPE_CHECKING:
    from ..modules import Attention

class CacheLayer_quant(CacheLayer):

    def __init__(
        self,
        config: Config,
        attention: Attention,
        max_num_tokens: int,
        k_bits: int,
        v_bits: int,
    ):
        super().__init__(config, attention, max_num_tokens)

        assert max_num_tokens % PAGE_SIZE == 0, \
            f"max_num_tokens must be a multiple of {PAGE_SIZE}."
        assert (2 <= k_bits <= 8) and (2 <= v_bits <= 8), "quantized cache must be from 2 to 8 bits"

        self.shape = (
            (max_num_tokens // PAGE_SIZE, PAGE_SIZE, attention.num_kv_heads, attention.head_dim)
            if attention else None
        )

        self.k_bits = k_bits
        self.v_bits = v_bits
        self.token_dim = attention.num_kv_heads * attention.head_dim
        self.qshape_k = ((max_num_tokens // PAGE_SIZE, PAGE_SIZE, self.token_dim // 32 * k_bits) if attention else None)
        self.qshape_v = ((max_num_tokens // PAGE_SIZE, PAGE_SIZE, self.token_dim // 32 * v_bits) if attention else None)
        self.qshape_s = ((max_num_tokens // PAGE_SIZE, PAGE_SIZE, self.token_dim // 32) if attention else None)

        self.qk = None
        self.qv = None
        self.sk = None
        self.sv = None
        self.device = None


    @override
    def alloc(self, device: torch.device):
        self.device = device
        self.qk = torch.zeros(self.qshape_k, dtype = torch.int, device = device) if self.shape else None
        self.qv = torch.zeros(self.qshape_v, dtype = torch.int, device = device) if self.shape else None
        self.sk = torch.zeros(self.qshape_s, dtype = torch.half, device = device) if self.shape else None
        self.sv = torch.zeros(self.qshape_s, dtype = torch.half, device = device) if self.shape else None


    @override
    def free(self):
        self.device = None
        self.qk = None
        self.qv = None
        self.sk = None
        self.sv = None


    @override
    def get_kv(self, cache_seqlens: torch.Tensor, block_table: torch.Tensor):
        k = torch.empty(self.shape, dtype = torch.half, device = self.device)
        v = torch.empty(self.shape, dtype = torch.half, device = self.device)
        ext.dequant_cache_paged(self.qk, self.sk, k, self.qv, self.sv, v, cache_seqlens, block_table, PAGE_SIZE)
        return k, v


    @override
    def update_kv(
        self,
        cache_seqlens: torch.Tensor,
        block_table: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        length: int
    ):
        ext.quant_cache_paged(
            k, self.qk, self.sk,
            v, self.qv, self.sv,
            cache_seqlens, block_table,
            PAGE_SIZE,
            length
        )


    @override
    def copy_page(self, source: CacheLayer_quant, from_page: int, to_page: int, num_tokens: int):
        assert self.qshape_k == source.qshape_k
        assert self.qshape_v == source.qshape_v
        self.qk[to_page, :num_tokens, :].copy_(source.qk[from_page, :num_tokens, :], non_blocking = True)
        self.qv[to_page, :num_tokens, :].copy_(source.qv[from_page, :num_tokens, :], non_blocking = True)
        self.sk[to_page, :num_tokens, :].copy_(source.sk[from_page, :num_tokens, :], non_blocking = True)
        self.sv[to_page, :num_tokens, :].copy_(source.sv[from_page, :num_tokens, :], non_blocking = True)

    @override
    def get_tensors(self):
        return [self.qk, self.qv, self.sk, self.sv]

</content>

<content full_path="exllamav3/util/misc.py">
import math
import threading
import time
import torch


lock = threading.RLock()

def synchronized(func):
    def wrapper(*args, **kwargs):
        with lock:
            return func(*args, **kwargs)
    return wrapper

def align_to(value, alignment):
    return int(math.ceil(value / alignment) * alignment)


class Timer:
    """
    Context manager to record duration
    """

    def __enter__(self):
        self.start_time = time.time()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end_time = time.time()
        self.interval = self.end_time - self.start_time


def cuda_sync_active():
    """
    Calling torch.cuda.synchronize() will create a CUDA context on CUDA:0 even if that device is not being used.
    This function synchronizes only devices actively used by Torch in the current process.
    """
    for device_id in range(torch.cuda.device_count()):
        device = torch.device(f'cuda:{device_id}')
        if torch.cuda.memory_allocated(device) > 0:
            torch.cuda.synchronize(device)


def next_power_of_2(x):
    return 1 if x == 0 else 2**(x - 1).bit_length()


def human_time(seconds: float) -> str:
    seconds = round(seconds)
    minutes = seconds // 60
    hours = minutes // 60
    minutes -= hours * 60
    if hours:
        if minutes:
            hs = "s" if hours > 1 else ""
            ms = "s" if minutes > 1 else ""
            return f"{hours} hour{hs}, {minutes} minute{ms}"
        else:
            hs = "s" if hours > 1 else ""
            return f"{hours} hour{hs}"
    elif minutes:
        ms = "s" if minutes > 1 else ""
        return f"{minutes} minute{ms}"
    else:
        return f"< 1 minute"


def first_not_none(*values):
    return next((v for v in values if v is not None), None)

</content>

<content full_path="exllamav3/util/profile_opt.py">
"""
Turn @profile into a no-op when it's not injected by kernprof
"""

import builtins

if not hasattr(builtins, "profile"):
    def _noop(func):
        return func
    builtins.profile = _noop

profile = builtins.profile
__all__ = ["profile"]

</content>

<content full_path="exllamav3/util/memory.py">
from dataclasses import dataclass
from collections import deque
import torch
import gc
import sys

# @lru_cache
# def init_pynvml():
#     pynvml.nvmlInit()

# Try to make sure device is live for correct measurement of free VRAM
def touch_device(device: int):
    d = torch.empty((32, 32), device = device, dtype = torch.float)
    d = d @ d
    d = d + d


# Reserve byte amount on device
def set_memory_fraction_reserve(
    reserve: int,
    device: int
):
    touch_device(device)
    free, total = torch.cuda.mem_get_info(device)
    fraction = (free - reserve) / total
    torch.cuda.set_per_process_memory_fraction(fraction, device = device)


# Reserve all but byte amount on device
def set_memory_fraction_use(
    use: int,
    device: int
):
    touch_device(device)
    free, total = torch.cuda.mem_get_info(device)
    baseline = torch.cuda.memory_allocated(device)
    fraction = min((baseline + use) / total, 1.0)
    torch.cuda.set_per_process_memory_fraction(fraction, device = device)


# Un-reserve VRAM
def unset_memory_fraction(active_devices: list[int]):
    for i in active_devices:
        torch.cuda.set_per_process_memory_fraction(1.0, device = i)


# Free unused VRAM
def free_mem():
    gc.collect()
    torch.cuda.empty_cache()



def list_gpu_tensors(min_size: int = 1, cuda_only: bool = True):
    """
    Search the current process for referenced CUDA tensors and list them.

    :param min_size:
        Ignore tensors smaller than this size, in megabytes

    :param cuda_only:
        Only list CUDA tensors
    """

    import threading
    import warnings
    from tabulate import tabulate

    # Suppress FutureWarning from Torch every time we try to access certain objects
    warnings.simplefilter(action = 'ignore', category = FutureWarning)

    @dataclass
    class Result:
        paths: list[str]
        shape: tuple
        dtype: torch.dtype
        device: str
        size: int

    results = {}
    visited = set()

    # Helper function to filter and collect items
    def collect(path, item):
        nonlocal results

        # Only collect CUDA tensors
        if not isinstance(item, torch.Tensor) or (cuda_only and not item.is_cuda):
            return

        # Tensor size in MB, filter anything smaller than the minimum size
        size = item.nelement() * item.element_size() // (1024**2)
        if size < min_size:
            return

        # Skip tensors in paths containing specific debug substrings
        if any(x in path for x in [
            ".stderr.dbg.",
            "dbg.value_resolve_thread_list",
            "global_vars[",
            "local_vars[",
            "updated_globals[",
        ]):
            return

        # Adjust the path display for objects defined in __main__
        if ".__main__." in path:
            path = path[path.find(".__main__.") + 10:]

        # If tensor is already recorded, just record the additional path
        obj_id = id(item)
        if obj_id in results and path not in results[obj_id].paths:
            results[obj_id].paths.append(path)
        else:
            results[obj_id] = Result(
                paths = [path],
                shape = item.shape,
                dtype = item.dtype,
                device = str(item.device),
                size = size
            )

    # Queue of items to scan recursively
    queue = deque()

    # Collect items that are global variables, and add to the queue
    for name, obj in globals().items():
        collect(name, obj)
        queue.append((name, obj))

    # Traverse each thread's frame stack, collecting items and queueing items
    for thread_id, frame in sys._current_frames().items():
        prefix = ""

        # Skip the current frame for the current thread to avoid recursion issues
        if thread_id == threading.get_ident():
            frame = frame.f_back

        # Collect/queue each local variable in the frame, extend the relative path prefix
        # and walk the stack
        while frame:
            for name, obj in frame.f_locals.items():
                # We actually start three levels deep but want variables in the "current" frame
                # (i.e. the frame of the function calling list_gpu_tensors) to have a prefix of "."
                new_path = f"{prefix[2:]}.{name}"
                collect(new_path, obj)
                queue.append((name, obj))
            frame = frame.f_back
            prefix += "."

    # Process the queue by examining attributes, dictionary entries, and sequence items
    while queue:
        path, obj = queue.popleft()

        # Iterate over entries in object with __dict__ attribute
        if hasattr(obj, '__dict__'):
            for attr, value in obj.__dict__.items():
                new_path = f"{path}.{attr}"
                collect(new_path, value)
                if id(value) not in visited:
                    visited.add(id(value))
                    queue.append((new_path, value))

        # If object is a dictionary, iterate through all its items
        if isinstance(obj, dict):
            try:
                for key, value in obj.items():
                    new_path = f"{path}['{key}']"
                    collect(new_path, value)
                    if id(value) not in visited:
                        visited.add(id(value))
                        queue.append((new_path, value))
            except:
                pass

        # Same for list, tuple, set
        if isinstance(obj, (list, tuple, set)):
            for idx, item in enumerate(obj):
                new_path = f"{path}[{idx}]"
                collect(new_path, item)
                if id(item) not in visited:
                    visited.add(id(item))
                    queue.append((new_path, item))

    # Sort tensors by descending size
    items = list(results.values())
    items.sort(key = lambda x: -x.size)

    # Build output table, grouped by device
    devices: dict[str, list] = {}
    for v in items:
        if v.device not in devices:
            devices[v.device] = []
        dev = devices[v.device]
        dev.append([
            v.size,
            v.paths[0],
            tuple(v.shape),
            str(v.dtype).replace("torch.", "")
        ])
        for p in v.paths[1:]:
            dev.append([
                None,
                " + " + p,
                None,
                None
            ])

    # Print tables to console
    for k in sorted(devices.keys()):
        print()
        print(f"--------------")
        print(f"| {k:10} |")
        print(f"--------------")
        print()
        headers = ["size // MB", "path", "shape", "dtype"]
        print(tabulate(devices[k], headers = headers, tablefmt = "github", intfmt=","))


</content>

<content full_path="exllamav3/util/__init__.py">
from .misc import *
</content>

<content full_path="exllamav3/util/hadamard.py">
from __future__ import annotations
import torch
import os, glob
from functools import lru_cache
from ..ext import exllamav3_ext as ext

had_dict: dict[int: torch.Tensor] | None = {}
primes: set[int]

def load_constants():
    global had_dict, primes

    module_dir = os.path.dirname(os.path.abspath(__file__))
    had_dir = os.path.join(module_dir, "hadamard_data")
    file_pattern = os.path.join(had_dir, "hadamard_*.txt")
    files = glob.glob(file_pattern)
    had_dict = {}

    for file_path in files:
        with open(file_path, 'r') as file:
            lines = file.readlines()
            lines = [line.strip() for line in lines if line.strip()]
            dim = len(lines)
            assert all(len(line) == dim for line in lines), "Non-square matrix in " + file_path
            matrix = [[1 if char == '+' else -1 for char in line] for line in lines]
            tensor = torch.tensor(matrix, dtype = torch.float16)
            had_dict[dim] = tensor

    prime_path = os.path.join(had_dir, "primes.txt")
    with open(prime_path, "r") as f:
        lines = f.readlines()
        primes = set([int(line) for line in lines if line.strip()])

def sylvester(h: torch.Tensor):
    d = h.shape[0]
    assert d == h.shape[1], "h not square"
    s = torch.empty((d * 2, d * 2), dtype = h.dtype, device = h.device)
    s[:d, :d] = h
    s[:d, d:] = h
    s[d:, :d] = h
    s[d:, d:] = -h
    return s

def is_quadratic_residue(a: int, p: int):
    return pow(a, (p - 1) // 2, p) == 1

def paley_torch(n: int):
    h = torch.empty((n, n), dtype = torch.half)
    p = n - 1
    for i in range(p):
        for j in range(p):
            if i == j:
                h[i + 1][j + 1] = 1
            else:
                residue = (i - j) % p
                if is_quadratic_residue(residue, p):
                    h[i + 1][j + 1] = 1
                else:
                    h[i + 1][j + 1] = -1
    h[0, :] = 1
    h[:, 0] = -1
    h[0, 0] = 1
    return h

def paley(n: int):
    h = torch.empty((n, n), dtype = torch.half)
    ext.had_paley(h)
    # ref = paley_torch(n)
    # assert torch.all(h == ref)
    return h

def paley2_torch(n: int):
    h = torch.empty((n, n), dtype = torch.half)
    p = n // 2 - 1
    for i in range(n // 2):
        i0 = 2 * i + 0
        i1 = 2 * i + 1
        for j in range(n // 2):
            j0 = 2 * j + 0
            j1 = 2 * j + 1
            if j == i:
                h[i0, j0] = 1
                h[i0, j1] = -1
                h[i1, j0] = -1
                h[i1, j1] = -1
            else:
                residue = (i - j) % p
                if i == 0 or j == 0 or is_quadratic_residue(residue, p):
                    h[i0, j0] = 1
                    h[i0, j1] = 1
                    h[i1, j0] = 1
                    h[i1, j1] = -1
                else:
                    h[i0, j0] = -1
                    h[i0, j1] = -1
                    h[i1, j0] = -1
                    h[i1, j1] = 1
    return h

def paley2(n: int):
    h = torch.empty((n, n), dtype = torch.half)
    ext.had_paley2(h)
    # ref = paley2_torch(n)
    # assert torch.all(h == ref)
    return h

@lru_cache(maxsize = 100)
def get_hadamard(n: int):
    global had_dict, primes

    if not had_dict:
        load_constants()

    if n in had_dict: return had_dict[n]

    # Sylvester's construction
    if n % 2 == 0:
        s = get_hadamard(n // 2)
        if s is not None:
            s = sylvester(s)
            return s

    # Paley construction
    if n % 4 == 0 and (n - 1) % 4 == 3 and (n - 1) in primes:
        return paley(n)

    # Other Paley construction
    if n % 4 == 0 and (n // 2) - 1 in primes:
        return paley2(n)

    return None

@lru_cache(maxsize = 100)
def get_hadamard_dt(n: int, device: torch.device | str, dtype: torch.dtype, scale = 1.0):
    had = get_hadamard(n).to(device = device, dtype = dtype, copy = True)
    had *= scale
    return had

</content>

<content full_path="exllamav3/util/tensor.py">
from __future__ import annotations
import torch

class SeqTensor:

    PAGE_SIZE = 256

    tensor: torch.Tensor
    seq_dim: int
    seq_len: int
    seq_cap: int

    def __init__(
        self,
        shape: tuple,
        dtype: torch.dtype,
        seq_dim: int,
        device: torch.device = "cpu",
        init_cap: int = -1
    ):
        if seq_dim < 0: seq_dim = len(shape) + seq_dim
        self.seq_dim = seq_dim
        self.seq_len = 0
        if init_cap == -1:
            init_cap = self.PAGE_SIZE
        else:
            init_cap = (init_cap // self.PAGE_SIZE + 1) * self.PAGE_SIZE
        shape = list(shape)
        shape[seq_dim] = self.seq_cap = init_cap
        shape = tuple(shape)
        self.tensor = torch.empty(shape, dtype = dtype, device = device)

    def __len__(self):
        return self.seq_len

    def __bool__(self):
        return self.seq_len > 0

    @staticmethod
    def from_tensor(tensor: torch.Tensor, seq_dim: int):
        s = SeqTensor(tensor.shape, tensor.dtype, seq_dim, tensor.device, init_cap = tensor.shape[seq_dim])
        s.append(tensor)
        return s

    def clone(self, drop: int | None = None):
        if drop and drop <= self.seq_len:
            return SeqTensor.from_tensor(self.torch_slice(None, self.seq_len - drop), self.seq_dim)
        else:
            return SeqTensor.from_tensor(self.torch(), self.seq_dim)

    def clear(self):
        self.seq_len = 0

    def set(self, new_data: SeqTensor | torch.tensor | None = None):
        self.clear()
        self.append(new_data)

    def append(self, new_data: SeqTensor | torch.tensor | None):
        if new_data is None: return
        if isinstance(new_data, SeqTensor):
            new_data = new_data.torch()
        new_len = new_data.shape[self.seq_dim]
        end_pos = self.seq_len + new_len
        if end_pos >= self.seq_cap:
            new_cap = (end_pos // self.PAGE_SIZE + 1) * self.PAGE_SIZE
            grow_shape = list(new_data.shape)
            grow_shape[self.seq_dim] = new_cap - self.seq_cap
            grow_shape = tuple(grow_shape)
            grow_tensor = torch.empty(grow_shape, dtype = self.tensor.dtype, device = self.tensor.device)
            self.tensor = torch.cat((self.tensor, grow_tensor), dim = self.seq_dim)
            self.seq_cap = new_cap
        s = self.tensor.narrow(self.seq_dim, self.seq_len, end_pos - self.seq_len)
        s.copy_(new_data)
        self.seq_len += new_len

    def truncate(self, new_len: int):
        assert new_len <= self.seq_len
        self.seq_len = new_len

    def torch(self):
        s = self.tensor.narrow(self.seq_dim, 0, self.seq_len)
        return s

    def slice(self, a: int | None, b: int | None):
        return SeqTensor.from_tensor(self.torch_slice(a, b), self.seq_dim)

    def torch_slice(self, a: int | None, b: int | None):
        if a is None and b is None:
            return self.torch()
        elif b is None:
            s = self.tensor.narrow(self.seq_dim, a, self.seq_len - a)
        elif a is None:
            s = self.tensor.narrow(self.seq_dim, 0, b)
        else:
            s = self.tensor.narrow(self.seq_dim, a, b - a)
        return s


no_default = object()

def get_for_device(
    input_dict: dict,
    key: str | int,
    device: torch.device,
    default = no_default,
) -> torch.Tensor | None:
    """
    Read a tensor from a dict and ensure it is available on the specified device. Caches access per device and may
    break if the tensor is updated after being accessed in this way. Intended for tensors that are read-only for the
    lifetime of the dict, such as RoPE coefficients during a single forward pass.
    """
    if key not in input_dict and default is not no_default:
        return default

    if "dev_cache" not in input_dict:
        cache = {}
        input_dict["dev_cache"] = cache
    else:
        cache = input_dict["dev_cache"]

    cache_key = f"{key}[{str(device)}]"
    if cache_key in cache:
        return cache[cache_key]

    v = input_dict[key]
    dv = None if v is None else input_dict[key].to(device)
    cache[cache_key] = dv
    return dv


buffered_aranges = {}
def buffered_arange(r: int, device: torch.device):
    if r not in buffered_aranges:
        buffered_aranges[r] = torch.arange(r)
    return get_for_device(buffered_aranges, r, device)


def to2(
    x: torch.Tensor,
    dtype1: torch.dtype | None,
    dtype2: torch.dtype | None = None
):
    if dtype1 is not None:
        x = x.to(dtype1)
    elif dtype2 is not None:
        x = x.to(dtype2)
    return x


def save_tensor_image(
    t: torch.Tensor,
    path: str,
):
    import matplotlib.cm as cm
    from PIL import Image

    t = t.detach().to("cpu", copy = True).float()

    k = 3
    mu, sigma = t.mean(), t.std()
    lo, hi = mu - k * sigma, mu + k * sigma
    t.clamp_(lo, hi)
    t -= lo
    t /= (hi - lo + 1e-8)

    rgba = cm.get_cmap("gnuplot2")(t.numpy())
    rgb8 = (rgba[..., :3] * 255).astype("uint8")
    im = Image.fromarray(rgb8)
    im.save(path)
</content>

<content full_path="exllamav3/util/measures.py">
import torch
import torch.nn.functional as F

def sqnr(a: torch.Tensor, b: torch.Tensor, eps: float = 1e-8):
    a_flat = a.view(a.shape[0], -1)
    b_flat = b.view(b.shape[0], -1)
    signal_power = torch.sum(b_flat ** 2, dim = 1)
    noise_power = torch.sum((a_flat - b_flat) ** 2, dim = 1) + eps
    return 10.0 * torch.log10(signal_power / noise_power).mean().item() # dB

def cosine_error(a: torch.Tensor, b: torch.Tensor, eps: float = 1e-8):
    a_flat = a.view(a.shape[0], -1)
    b_flat = b.view(b.shape[0], -1)
    cos_sim = F.cosine_similarity(a_flat, b_flat, dim = 1, eps = eps)
    return 1.0 - cos_sim.mean().item()

</content>

<content full_path="exllamav3/util/file.py">
import os
import shelve
import sys
import json
from typing import Any, Dict, List, TypeVar, Union, cast
from functools import lru_cache, wraps


def disk_lru_cache_name(filename):
    directory = os.path.join(os.path.dirname(os.path.abspath(sys.argv[0])), "__disk_lru_cache__")
    os.makedirs(directory, exist_ok = True)
    path = os.path.join(directory, filename + ".lru")
    return str(path)

def disk_lru_cache_clear(filename, *args, **kwargs):
    """
    Clear disk cache. Takes name of function and input arguments to forget.
    """
    with shelve.open(disk_lru_cache_name(filename)) as db:
        key = str((json.dumps(args, sort_keys = True), json.dumps(kwargs, sort_keys = True)))
        if key in db:
            del db[key]

def disk_lru_cache(filename):
    """
    Disk cache function decorator, mostly for quick-and-dirty caching of eval results. Creates a
    __disk_lru_cache__  subdirectory in the module's directory to store cached outputs. The cache dictionary
    is identified by the name of the calling function, so multiple functions in the same directory with the
    same name would result in a conflict.

    Requires all arguments to have a full string representation, and the function output must be
    serializable by the shelve library.
    """
    def decorator(func):
        @wraps(func)
        def disk_cached(*args, **kwargs):
            with shelve.open(disk_lru_cache_name(filename)) as db:
                key = str((json.dumps(args, sort_keys = True), json.dumps(kwargs, sort_keys = True)))
                if key in db:
                    return db[key]
                result = func(*args, **kwargs)
                db[key] = result
                return result
        return disk_cached
    return decorator

no_default = object()
no_value = object()
T = TypeVar('T')


def read_dict(
        input_dict: dict[str, Any],
        expected_types: type | list[type],
        keys: str | list[str],
        default = no_default,
) -> T:
    """
    Utility function to read typed value from (nested) dictionary

    :param input_dict:
        The dict to read from

    :param expected_types:
        Type or list of types to expect the value to be. Raise an exception if the key exists but value is
        of the wrong type. If expected_type is None, any value type is accepted

    :param keys:
        Key or list of keys to look for. If multiple keys in the list would match, the first matching key is
        used. Keys can index nested dictionaries with a "->" separator, e.g. "text_model->hidden_size" would
        be equivalent to dict["text_model"]["hidden_size"]

    :param default:
        Default value to return if the key isn't found, e.g. None. If this is the special value no_default
        and no keys are matched, raise an exception instead.

    :return:
        Requested value if key found, otherwise default value
    """

    if expected_types is not None and not isinstance(expected_types, list):
        expected_types = [expected_types]

    if isinstance(keys, str):
        keys = [keys]

    for key in keys:
        input_dict_s = input_dict

        key_split = key.split("->")
        for subk in key_split[:-1]:
            input_dict_s = input_dict_s.get(subk, None)
            if not input_dict_s:
                key = None
                break
        if key is None: continue
        key = key_split[-1]

        x = input_dict_s.get(key, None)
        if x is not None:
            if expected_types is None:
                return x
            else:
                for t in expected_types:
                    # Always cast int to float
                    if t == float and isinstance(x, int):
                        x = float(x)
                    # Allow casting float to int if no rounding error
                    if t == int and isinstance(x, float) and x == int(x):
                        x = int(x)
                    if isinstance(x, t):
                        return cast(T, x)
            raise TypeError(f"Value for {key} is not of expected type: {expected_types}")

    if default != no_default:
        return default
    raise ValueError(f"Missing any of the following keys: {keys}")


def maybe_read_json(path):
    """
    Read JSON file to dict, or return empty dict if file does not exist
    """
    if os.path.exists(path):
        with open(path, encoding="utf8") as f:
            return json.load(f)
    else:
        return {}

</content>

<content full_path="exllamav3/util/rope.py">
from dataclasses import dataclass

import torch
import math
from enum import IntEnum
from ..ext import exllamav3_ext as ext

# Reference:
# https://github.com/huggingface/transformers/blob/2e24ee4dfa39cc0bc264b89edbccc373c8337086/src/transformers/modeling_rope_utils.py

class RopeStyle(IntEnum):
    NONE = 0
    GPTJ = 1
    NEOX = 2

@dataclass
class RopeSettings:
    head_dim: int = 128
    rope_theta: float = 10000.0
    rope_scaling: dict | None = None
    partial_rotary_factor: float = 1.0
    max_position_embeddings: int | None = None
    original_max_position_embeddings: int | None = None
    rope_style: RopeStyle = RopeStyle.NEOX
    override_max_position_embeddings: int | None = None


def _rotate_half_neox(x):
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2:]
    return torch.cat((-x2, x1), dim = -1)

def _apply_rope_embed_q_neox(q, sin, cos):
    q = q.transpose(1, 2)
    cos = cos.unsqueeze(1)
    sin = sin.unsqueeze(1)
    q = (q * cos) + (_rotate_half_neox(q) * sin)
    q = q.transpose(1, 2)
    return q

def _apply_rope_embed_qk_neox(q, k, sin, cos):
    return (
        _apply_rope_embed_q_neox(q, sin, cos),
        _apply_rope_embed_q_neox(k, sin, cos)
    )


def _rotate_half_gptj(x):
    x1 = x[..., 0::2]
    x2 = x[..., 1::2]
    return torch.stack((-x2, x1), dim=-1).flatten(-2)

def _apply_rope_embed_q_gptj(q, sin, cos):
    q = q.transpose(1, 2)
    cos = cos.unsqueeze(1)
    sin = sin.unsqueeze(1)
    q = (q * cos) + (_rotate_half_gptj(q) * sin)
    q = q.transpose(1, 2)
    return q

def _apply_rope_embed_qk_gptj(q, k, sin, cos):
    return (
        _apply_rope_embed_q_gptj(q, sin, cos),
        _apply_rope_embed_q_gptj(k, sin, cos)
    )


class RoPE:

    # TODO: Alpha and linear scaling overrides (?)

    def __init__(
        self,
        device: torch.device | str,
        rope_settings: RopeSettings,
    ):
        self.device = device
        self.rope_settings = rope_settings

        self.cached_sin = None
        self.cached_cos = None
        self.cached_sincos_max = 0

        t = None
        rs = self.rope_settings
        if rs.rope_scaling is not None:
            t = rs.rope_scaling.get("rope_type", rs.rope_scaling.get("type"))
        match t:
            case None:
                self.inv_freq, self.attn_factor = self._rope_params_default()
            case "llama3":
                self.inv_freq, self.attn_factor = self._rope_params_llama3()
            case "linear":
                self.inv_freq, self.attn_factor = self._rope_params_linear()
            case "yarn":
                self.inv_freq, self.attn_factor = self._rope_params_yarn()
            case "longrope" | "su":
                self.inv_freq, self.attn_factor = self._rope_params_longrope()
            case _:
                raise ValueError(f"Unknown rope_type: {t}")


    def _rope_params_default(self):
        rs = self.rope_settings
        base = rs.rope_theta
        dim = int(rs.head_dim * rs.partial_rotary_factor)
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype = torch.int64, device = self.device).float() / dim))
        return inv_freq, 1.0


    def _rope_params_llama3(self):
        rs = self.rope_settings
        inv_freq, attention_factor = self._rope_params_default()
        factor = rs.rope_scaling.get("factor", 8.0)
        low_freq_factor = rs.rope_scaling.get("low_freq_factor", 1.0)
        high_freq_factor = rs.rope_scaling.get("high_freq_factor", 4.0)
        old_context_len = rs.rope_scaling.get("original_max_position_embeddings", 8192)
        low_freq_wavelen = old_context_len / low_freq_factor
        high_freq_wavelen = old_context_len / high_freq_factor
        wavelen = 2 * math.pi / inv_freq
        inv_freq_llama = torch.where(wavelen > low_freq_wavelen, inv_freq / factor, inv_freq)
        smooth_factor = (old_context_len / wavelen - low_freq_factor) / (high_freq_factor - low_freq_factor)
        smoothed_inv_freq = (1 - smooth_factor) * inv_freq_llama / factor + smooth_factor * inv_freq_llama
        is_medium_freq = (wavelen >= high_freq_wavelen) * (wavelen <= low_freq_wavelen)
        inv_freq = torch.where(is_medium_freq, smoothed_inv_freq, inv_freq_llama)
        return inv_freq, 1.0


    def _rope_params_linear(self):
        rs = self.rope_settings
        inv_freq, attention_factor = self._rope_params_default()
        factor = rs.rope_scaling.get("factor", 1.0)
        inv_freq /= factor
        return inv_freq, attention_factor


    def _rope_params_yarn(self):
        rs = self.rope_settings
        assert rs.max_position_embeddings is not None, \
            "YaRN scaling requires explicit max_position_embeddings"
        base = rs.rope_theta
        dim = int(rs.head_dim * rs.partial_rotary_factor)
        factor = rs.rope_scaling.get("factor")
        attn_factor = rs.rope_scaling.get("attention_factor", 0.1 * math.log(factor) + 1.0)
        beta_fast = rs.rope_scaling.get("beta_fast", 32)
        beta_slow = rs.rope_scaling.get("beta_slow", 1)
        def find_correction_dim(num_rotations):
            return (dim * math.log(rs.max_position_embeddings / (num_rotations * 2 * math.pi))) / (2 * math.log(base))
        def find_correction_range(low_rot, high_rot):
            _low = math.floor(find_correction_dim(low_rot))
            _high = math.ceil(find_correction_dim(high_rot))
            return max(_low, 0), min(_high, dim - 1)
        def linear_ramp_factor(_min, _max, _dim):
            if _min == _max:
                _max += 0.001
            linear_func = (torch.arange(_dim, dtype = torch.float32, device = self.device) - _min) / (_max - _min)
            ramp_func = torch.clamp(linear_func, 0, 1)
            return ramp_func
        pos_freqs = base ** (torch.arange(0, dim, 2, device = self.device).float() / dim)
        inv_freq_extrapolation = 1.0 / pos_freqs
        inv_freq_interpolation = 1.0 / (factor * pos_freqs)
        low, high = find_correction_range(beta_fast, beta_slow)
        inv_freq_extrapolation_factor = 1 - linear_ramp_factor(low, high, dim // 2).float()
        inv_freq = inv_freq_interpolation * (1 - inv_freq_extrapolation_factor)
        inv_freq += inv_freq_extrapolation * inv_freq_extrapolation_factor
        return inv_freq, attn_factor


    def _rope_params_longrope(self):
        rs = self.rope_settings
        base = rs.rope_theta
        dim = int(rs.head_dim * rs.partial_rotary_factor)
        a = rs.max_position_embeddings
        a_override = rs.override_max_position_embeddings or a
        b = rs.rope_scaling.get("original_max_position_embeddings", rs.original_max_position_embeddings)
        if a_override > b:
            factors = rs.rope_scaling.get("long_factor")
            ext_factors = torch.tensor(factors, dtype = torch.float32, device = self.device)
        else:
            factors = rs.rope_scaling.get("short_factor")
            ext_factors = torch.tensor(factors, dtype = torch.float32, device = self.device)
        if a > b:
            scaling = math.sqrt(1 + math.log(a / b) / math.log(b))
        else:
            scaling = 1.0
        inv_freq = 1.0 / (ext_factors * base ** (torch.arange(0, dim, 2, device = self.device).float() / dim))
        return inv_freq, scaling


    def compute_sincos(self, position_ids: torch.Tensor):
        rs = self.rope_settings
        freqs = torch.einsum("i,j->ij", position_ids.float(), self.inv_freq)
        sin = freqs.sin()
        cos = freqs.cos()
        if self.attn_factor != 1.0:
            sin *= self.attn_factor
            cos *= self.attn_factor
        match rs.rope_style:
            case RopeStyle.NEOX:
                sin = torch.cat((sin, sin), dim = -1)
                cos = torch.cat((cos, cos), dim = -1)
            case RopeStyle.GPTJ:
                sin = torch.repeat_interleave(sin, 2, dim = -1)
                cos = torch.repeat_interleave(cos, 2, dim = -1)
        return sin, cos


    def expand_cache(self, pos_id_end: int):
        interval = 2048
        if pos_id_end >= self.cached_sincos_max:
            pmax = self.cached_sincos_max
            nmax = (pos_id_end // interval + 1) * interval
            nsin, ncos = self.compute_sincos(torch.arange(pmax, nmax, device = self.device))
            self.cached_sin = torch.cat((self.cached_sin, nsin), dim = 0) if pmax > 0 else nsin
            self.cached_cos = torch.cat((self.cached_cos, ncos), dim = 0) if pmax > 0 else ncos
            self.cached_sincos_max = nmax


    def apply_torch(
        self,
        q: torch.Tensor,
        k: torch.Tensor | None,
        pos: int = 0,
        positions: torch.Tensor | None = None,
        position_ids: torch.Tensor | None = None,
        in_place = False,
    ):
        # TODO: partial rotary factor
        if in_place:
            q_ = q
            k_ = k

        q = q.float()
        k = k.float()

        if len(q.shape) == 3:
            q = q.unsqueeze(0)
            k = k.unsqueeze(0) if k is not None else k
            squeeze = True
        else:
            squeeze = False
        bsz, qlen, numheads_q, dim = q.shape

        if positions is not None:
            position_ids = torch.arange(qlen, device = self.device).unsqueeze(0).repeat(bsz, 1)
            position_ids += positions.unsqueeze(1)

        if position_ids is not None:
            if len(position_ids.shape) == 1:
                position_ids = position_ids.unsqueeze(0)
            else:
                assert position_ids.shape[0] == bsz
            self.expand_cache(position_ids.max().item())
            sin = self.cached_sin[position_ids]
            cos = self.cached_cos[position_ids]

        else:
            self.expand_cache(pos + qlen)
            sin = self.cached_sin[pos : pos + qlen].unsqueeze(0)
            cos = self.cached_cos[pos : pos + qlen].unsqueeze(0)

        if k is not None:
            if self.rope_settings.rope_style == RopeStyle.NEOX:
                q, k = _apply_rope_embed_qk_neox(q, k, sin, cos)
            else:
                q, k = _apply_rope_embed_qk_gptj(q, k, sin, cos)
        else:
            if self.rope_settings.rope_style == RopeStyle.NEOX:
                q = _apply_rope_embed_q_neox(q, sin, cos)
            else:
                q = _apply_rope_embed_q_gptj(q, sin, cos)

        if squeeze:
            q = q.squeeze(0)
            k = k.squeeze(0) if k is not None else k

        q = q.half()
        k = k.half()

        if in_place:
            q_.copy_(q)
            k_.copy_(k)
            return q_, k_
        else:
            return q, k


    def apply(
        self,
        q: torch.Tensor,
        k: torch.Tensor | None = None,
        position: int = 0,
        positions: torch.Tensor | None = None,
        position_ids: torch.Tensor | None = None,
        in_place = False,
        q_norm: torch.Tensor | None = None,
        k_norm: torch.Tensor | None = None,
        norm_eps: float = 1e-6,
        norm_constant_bias: float = 0.0
    ):
        q = q.contiguous()
        if k is not None: k = k.contiguous()
        if positions is not None: positions = positions.contiguous()
        if position_ids is not None: position_ids = position_ids.contiguous()

        if len(q.shape) == 3:
            q = q.unsqueeze(0)
            k = k.unsqueeze(0)
            squeeze = True
        else:
            squeeze = False

        if not in_place:
            out_q = torch.empty_like(q)
            out_k = torch.empty_like(k) if k is not None else None
        else:
            out_q = q
            out_k = k

        ext.rope(
            q, out_q,
            k, out_k,
            self.inv_freq,
            position,
            positions,
            position_ids,
            self.rope_settings.rope_style,
            self.attn_factor,
            q_norm,
            k_norm,
            norm_eps,
            norm_constant_bias
        )
            
        if squeeze:
            out_q = out_q.squeeze(0)
            out_k = out_k.squeeze(0) if out_k is not None else None

        return out_q, out_k

</content>

<content full_path="exllamav3/util/arch_list.py">
import os
import torch

# Since Torch 2.3.0 an annoying warning is printed every time the C++ extension is loaded, unless the
# TORCH_CUDA_ARCH_LIST variable is set. The default behavior from pytorch/torch/utils/cpp_extension.py
# is copied in the function below, but without the warning.

def maybe_set_arch_list_env():

    if os.environ.get('TORCH_CUDA_ARCH_LIST', None):
        return

    if not torch.version.cuda:
        return

    arch_list = []
    for i in range(torch.cuda.device_count()):
        capability = torch.cuda.get_device_capability(i)
        supported_sm = [int(arch.split('_')[1])
                        for arch in torch.cuda.get_arch_list() if 'sm_' in arch]
        if not supported_sm:
            continue
        max_supported_sm = max((sm // 10, sm % 10) for sm in supported_sm)
        # Capability of the device may be higher than what's supported by the user's
        # NVCC, causing compilation error. User's NVCC is expected to match the one
        # used to build pytorch, so we use the maximum supported capability of pytorch
        # to clamp the capability.
        capability = min(max_supported_sm, capability)
        arch = f'{capability[0]}.{capability[1]}'
        if arch not in arch_list:
            arch_list.append(arch)
    if not arch_list:
        return
    arch_list = sorted(arch_list)
    arch_list[-1] += '+PTX'

    os.environ["TORCH_CUDA_ARCH_LIST"] = ";".join(arch_list)

maybe_set_arch_list_env()
</content>

<content full_path="exllamav3/util/progress.py">
import sys
from rich.progress import Progress, BarColumn, TextColumn, TimeElapsedColumn, TimeRemainingColumn

class ProgressBar:

    def __init__(self, text: str, count: int, transient: bool = True):
        self.text = text
        self.count = count
        self.transient = transient
        if self.text:
            self.progress = Progress(
                TextColumn("[progress.description]{task.description}"),
                BarColumn(bar_width = None),
                "[progress.percentage]{task.percentage:>3.0f}%",
                TimeElapsedColumn(),
                TimeRemainingColumn(),
                transient = transient,
            )
            self.task_id = self.progress.add_task(text, total = count)

    def __enter__(self):
        if self.text:
            self.progress.start()
            sys.stdout.flush()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.text:
            if not self.transient:
                self.progress.update(self.task_id, completed = self.count)
            self.progress.stop()

    def update(self, value: int):
        if self.text:
            self.progress.update(self.task_id, completed = value)
            sys.stdout.flush()

    def new_task(self, text: str, count: int):
        self.text = text
        self.count = count
        if self.text:
            self.progress.update(self.task_id, description = self.text, total = count, progress = 0)



</content>

<content full_path="exllamav3/util/hadamard_data/hadamard_172.txt">
+---++--++++-+-+++-++--++-+++-+-++++--++---++-++++++----+-+--++-++-++--+-+----++++++-++++-+-++--+-+-++++-+----+-++++-+-+--++-+-++++---++++-+--+--++--------++--+--+-++++---+
-+---++--++++-+-+++-++--++-+++-+-++++--++--+++-++++++----+-+--++-++-++--+-+----++++++-++++-+-++--+-+-++++-+----+-++++-+-+--++-+-++++---++++-+--+--++--------++--+--+-++++---
--+---++--++++-+-+++-++--++-+++-+-++++--++--+++-++++++----+-+--++-++-++--+-+----+++++++++++-+-++--+-+-++++-+----+-++++-+-+--++-+--+++---++++-+--+--++--------++--+--+-++++--
---+---++--++++-+-+++-++--++-+++-+-++++--+++-+++-++++++----+-+--++-++-++--+-+----+++++-+++++-+-++--+-+-++++-+----+-++++-+-+--++-+--+++---++++-+--+--++--------++--+--+-++++-
+---+---++--++++-+-+++-++--++-+++-+-++++--+++-+++-++++++----+-+--++-++-++--+-+----+++++-+++++-+-++--+-+-++++-+----+-++++-+-+--++----+++---++++-+--+--++--------++--+--+-++++
++---+---++--++++-+-+++-++--++-+++-+-++++--+++-+++-++++++----+-+--++-++-++--+-+----+++-+-+++++-+-++--+-+-++++-+----+-++++-+-+--+++---+++---++++-+--+--++--------++--+--+-+++
-++---+---++--++++-+-+++-++--++-+++-+-++++-++++-+++-++++++----+-+--++-++-++--+-+----+++-+-+++++-+-++--+-+-++++-+----+-++++-+-+--+++---+++---++++-+--+--++--------++--+--+-++
--++---+---++--++++-+-+++-++--++-+++-+-+++++++++-+++-++++++----+-+--++-++-++--+-+----+++-+-+++++-+-++--+-+-++++-+----+-++++-+-+--+++---+++---++++-+--+--++--------++--+--+-+
+--++---+---++--++++-+-+++-++--++-+++-+-+++++++++-+++-++++++----+-+--++-++-++--+-+-----++-+-+++++-+-++--+-+-++++-+----+-++++-+-+-++++---+++---++++-+--+--++--------++--+--+-
++--++---+---++--++++-+-+++-++--++-+++-+-++-++++++-+++-++++++----+-+--++-++-++--+-+-----++-+-+++++-+-++--+-+-++++-+----+-++++-+-+-++++---+++---++++-+--+--++--------++--+--+
+++--++---+---++--++++-+-+++-++--++-+++-+-+--++++++-+++-++++++----+-+--++-++-++--+-+--+--++-+-+++++-+-++--+-+-++++-+----+-++++-+-+-++++---+++---++++-+--+--++--------++--+--
++++--++---+---++--++++-+-+++-++--++-+++-+----++++++-+++-++++++----+-+--++-++-++--+-+--+--++-+-+++++-+-++--+-+-++++-+----+-++++-+-+-++++---+++---++++-+--+--++--------++--+-
-++++--++---+---++--++++-+-+++-++--++-+++-+----++++++-+++-++++++----+-+--++-++-++--+-++-+--++-+-+++++-+-++--+-+-++++-+----+-++++---+-++++---+++---++++-+--+--++--------++--+
+-++++--++---+---++--++++-+-+++-++--++-+++-+----++++++-+++-++++++----+-+--++-++-++--+--+-+--++-+-+++++-+-++--+-+-++++-+----+-+++++--+-++++---+++---++++-+--+--++--------++--
-+-++++--++---+---++--++++-+-+++-++--++-+++-+----++++++-+++-++++++----+-+--++-++-++--++-+-+--++-+-+++++-+-++--+-+-++++-+----+-+++-+--+-++++---+++---++++-+--+--++--------++-
+-+-++++--++---+---++--++++-+-+++-++--++-+++-+----++++++-+++-++++++----+-+--++-++-++--++-+-+--++-+-+++++-+-++--+-+-++++-+----+-++--+--+-++++---+++---++++-+--+--++--------++
++-+-++++--++---+---++--++++-+-+++-++--++-+-+-+----++++++-+++-++++++----+-+--++-++-++-+++-+-+--++-+-+++++-+-++--+-+-++++-+----+-++--+--+-++++---+++---++++-+--+--++--------+
+++-+-++++--++---+---++--++++-+-+++-++--++---+-+----++++++-+++-++++++----+-+--++-++-++++++-+-+--++-+-+++++-+-++--+-+-++++-+----+-++--+--+-++++---+++---++++-+--+--++--------
-+++-+-++++--++---+---++--++++-+-+++-++--+++--+-+----++++++-+++-++++++----+-+--++-++-+-++++-+-+--++-+-+++++-+-++--+-+-++++-+----+-++--+--+-++++---+++---++++-+--+--++-------
+-+++-+-++++--++---+---++--++++-+-+++-++--+++--+-+----++++++-+++-++++++----+-+--++-++-+-++++-+-+--++-+-+++++-+-++--+-+-++++-+------++--+--+-++++---+++---++++-+--+--++------
++-+++-+-++++--++---+---++--++++-+-+++-++---++--+-+----++++++-+++-++++++----+-+--++-++-+-++++-+-+--++-+-+++++-+-++--+-+-++++-+------++--+--+-++++---+++---++++-+--+--++-----
-++-+++-+-++++--++---+---++--++++-+-+++-++-+-++--+-+----++++++-+++-++++++----+-+--++-+--+-++++-+-+--++-+-+++++-+-++--+-+-++++-+------++--+--+-++++---+++---++++-+--+--++----
--++-+++-+-++++--++---+---++--++++-+-+++-++++-++--+-+----++++++-+++-++++++----+-+--++----+-++++-+-+--++-+-+++++-+-++--+-+-++++-+------++--+--+-++++---+++---++++-+--+--++---
+--++-+++-+-++++--++---+---++--++++-+-+++-+-++-++--+-+----++++++-+++-++++++----+-+--++----+-++++-+-+--++-+-+++++-+-++--+-+-++++-+------++--+--+-++++---+++---++++-+--+--++--
++--++-+++-+-++++--++---+---++--++++-+-+++-+-++-++--+-+----++++++-+++-++++++----+-+--++----+-++++-+-+--++-+-+++++-+-++--+-+-++++--------++--+--+-++++---+++---++++-+--+--++-
-++--++-+++-+-++++--++---+---++--++++-+-+++++-++-++--+-+----++++++-+++-++++++----+-+---+----+-++++-+-+--++-+-+++++-+-++--+-+-++++--------++--+--+-++++---+++---++++-+--+--++
+-++--++-+++-+-++++--++---+---++--++++-+-++-++-++-++--+-+----++++++-+++-++++++----+-+-+-+----+-++++-+-+--++-+-+++++-+-++--+-+-++++--------++--+--+-++++---+++---++++-+--+--+
++-++--++-+++-+-++++--++---+---++--++++-+-+--++-++-++--+-+----++++++-+++-++++++----+-+++-+----+-++++-+-+--++-+-+++++-+-++--+-+-++++--------++--+--+-++++---+++---++++-+--+--
+++-++--++-+++-+-++++--++---+---++--++++-+-+--++-++-++--+-+----++++++-+++-++++++----+-+++-+----+-++++-+-+--++-+-+++++-+-++--+-+-+-++--------++--+--+-++++---+++---++++-+--+-
-+++-++--++-+++-+-++++--++---+---++--++++-+-+--++-++-++--+-+----++++++-+++-++++++----+++++-+----+-++++-+-+--++-+-+++++-+-++--+-+---++--------++--+--+-++++---+++---++++-+--+
+-+++-++--++-+++-+-++++--++---+---++--++++-+-+--++-++-++--+-+----++++++-+++-++++++-----++++-+----+-++++-+-+--++-+-+++++-+-++--+-++--++--------++--+--+-++++---+++---++++-+--
-+-+++-++--++-+++-+-++++--++---+---++--++++-+-+--++-++-++--+-+----++++++-+++-++++++---+-++++-+----+-++++-+-+--++-+-+++++-+-++--+--+--++--------++--+--+-++++---+++---++++-+-
+-+-+++-++--++-+++-+-++++--++---+---++--+++--+-+--++-++-++--+-+----++++++-+++-++++++---+-++++-+----+-++++-+-+--++-+-+++++-+-++--+--+--++--------++--+--+-++++---+++---++++-+
++-+-+++-++--++-+++-+-++++--++---+---++--++---+-+--++-++-++--+-+----++++++-+++-++++++-+-+-++++-+----+-++++-+-+--++-+-+++++-+-++--+--+--++--------++--+--+-++++---+++---++++-
+++-+-+++-++--++-+++-+-++++--++---+---++--+----+-+--++-++-++--+-+----++++++-+++-++++++-+-+-++++-+----+-++++-+-+--++-+-+++++-+-++--+--+--++--------++--+--+-++++---+++---++++
++++-+-+++-++--++-+++-+-++++--++---+---++--+----+-+--++-++-++--+-+----++++++-+++-+++++--+-+-++++-+----+-++++-+-+--++-+-+++++-+-+++-+--+--++--------++--+--+-++++---+++---+++
-++++-+-+++-++--++-+++-+-++++--++---+---++-++----+-+--++-++-++--+-+----++++++-+++-+++++--+-+-++++-+----+-++++-+-+--++-+-+++++-+-+++-+--+--++--------++--+--+-++++---+++---++
--++++-+-+++-++--++-+++-+-++++--++---+---+++++----+-+--++-++-++--+-+----++++++-+++-+++++--+-+-++++-+----+-++++-+-+--++-+-+++++-+-+++-+--+--++--------++--+--+-++++---+++---+
+--++++-+-+++-++--++-+++-+-++++--++---+---+++++----+-+--++-++-++--+-+----++++++-+++-++-++--+-+-++++-+----+-++++-+-+--++-+-+++++-+++++-+--+--++--------++--+--+-++++---+++---
++--++++-+-+++-++--++-+++-+-++++--++---+---+++++----+-+--++-++-++--+-+----++++++-+++-++-++--+-+-++++-+----+-++++-+-+--++-+-+++++--++++-+--+--++--------++--+--+-++++---+++--
-++--++++-+-+++-++--++-+++-+-++++--++---+--++++++----+-+--++-++-++--+-+----++++++-+++--+-++--+-+-++++-+----+-++++-+-+--++-+-+++++--++++-+--+--++--------++--+--+-++++---+++-
--++--++++-+-+++-++--++-+++-+-++++--++---+--++++++----+-+--++-++-++--+-+----++++++-++++-+-++--+-+-++++-+----+-++++-+-+--++-+-++++---++++-+--+--++--------++--+--+-++++---+++
---++--++++-+-+++-++--++-+++-+-++++--++---++-++++++----+-+--++-++-++--+-+----++++++-++++-+-++--+-+-++++-+----+-++++-+-+--++-+-++++---++++-+--+--++--------++--+--+-++++---++
--+------++++-+-++--+--+--++-+-++++------+-+---++--++++-+-+++-++--++-+++-+-++++--++-----+++----+-++-++--++++++++--++-++-+----+++-+++-+-++--+-+-++++-+----+-++++-+-+--++-+-++
---+------++++-+-++--+--+--++-+-++++------+-+---++--++++-+-+++-++--++-+++-+-++++--++-----+++----+-++-++--++++++++--++-++-+----+++++++-+-++--+-+-++++-+----+-++++-+-+--++-+-+
+---+------++++-+-++--+--+--++-+-++++--------+---++--++++-+-+++-++--++-+++-+-++++--++-+---+++----+-++-++--++++++++--++-++-+----+++++++-+-++--+-+-++++-+----+-++++-+-+--++-+-
-+---+------++++-+-++--+--+--++-+-++++--------+---++--++++-+-+++-++--++-+++-+-++++--++++---+++----+-++-++--++++++++--++-++-+----+-+++++-+-++--+-+-++++-+----+-++++-+-+--++-+
--+---+------++++-+-++--+--+--++-+-++++----+---+---++--++++-+-+++-++--++-+++-+-++++--++++---+++----+-++-++--++++++++--++-++-+----+-+++++-+-++--+-+-++++-+----+-++++-+-+--++-
---+---+------++++-+-++--+--+--++-+-++++---++---+---++--++++-+-+++-++--++-+++-+-++++---+++---+++----+-++-++--++++++++--++-++-+----+-+++++-+-++--+-+-++++-+----+-++++-+-+--++
----+---+------++++-+-++--+--+--++-+-++++---++---+---++--++++-+-+++-++--++-+++-+-++++---+++---+++----+-++-++--++++++++--++-++-+--+-+-+++++-+-++--+-+-++++-+----+-++++-+-+--+
-----+---+------++++-+-++--+--+--++-+-++++---++---+---++--++++-+-+++-++--++-+++-+-++++---+++---+++----+-++-++--++++++++--++-++-+-++-+-+++++-+-++--+-+-++++-+----+-++++-+-+--
------+---+------++++-+-++--+--+--++-+-+++++--++---+---++--++++-+-+++-++--++-+++-+-+++----+++---+++----+-++-++--++++++++--++-++-+-++-+-+++++-+-++--+-+-++++-+----+-++++-+-+-
+------+---+------++++-+-++--+--+--++-+-+++++--++---+---++--++++-+-+++-++--++-+++-+-+++----+++---+++----+-++-++--++++++++--++-++---++-+-+++++-+-++--+-+-++++-+----+-++++-+-+
++------+---+------++++-+-++--+--+--++-+-+++++--++---+---++--++++-+-+++-++--++-+++-+-+-+----+++---+++----+-++-++--++++++++--++-+++--++-+-+++++-+-++--+-+-++++-+----+-++++-+-
+++------+---+------++++-+-++--+--+--++-+-+++++--++---+---++--++++-+-+++-++--++-+++-+-+-+----+++---+++----+-++-++--++++++++--++-+-+--++-+-+++++-+-++--+-+-++++-+----+-++++-+
++++------+---+------++++-+-++--+--+--++-+--++++--++---+---++--++++-+-+++-++--++-+++-+++-+----+++---+++----+-++-++--++++++++--++-+-+--++-+-+++++-+-++--+-+-++++-+----+-++++-
-++++------+---+------++++-+-++--+--+--++-++-++++--++---+---++--++++-+-+++-++--++-+++--++-+----+++---+++----+-++-++--++++++++--++-+-+--++-+-+++++-+-++--+-+-++++-+----+-++++
+-++++------+---+------++++-+-++--+--+--++--+-++++--++---+---++--++++-+-+++-++--++-++++-++-+----+++---+++----+-++-++--++++++++--++-+-+--++-+-+++++-+-++--+-+-++++-+----+-+++
-+-++++------+---+------++++-+-++--+--+--+++-+-++++--++---+---++--++++-+-+++-++--++-++++-++-+----+++---+++----+-++-++--++++++++--++-+-+--++-+-+++++-+-++--+-+-++++-+----+-++
+-+-++++------+---+------++++-+-++--+--+--+++-+-++++--++---+---++--++++-+-+++-++--++-+-++-++-+----+++---+++----+-++-++--++++++++-+++-+-+--++-+-+++++-+-++--+-+-++++-+----+-+
++-+-++++------+---+------++++-+-++--+--+--+++-+-++++--++---+---++--++++-+-+++-++--++---++-++-+----+++---+++----+-++-++--++++++++++++-+-+--++-+-+++++-+-++--+-+-++++-+----+-
-++-+-++++------+---+------++++-+-++--+--+--+++-+-++++--++---+---++--++++-+-+++-++--+++--++-++-+----+++---+++----+-++-++--+++++++-++++-+-+--++-+-+++++-+-++--+-+-++++-+----+
--++-+-++++------+---+------++++-+-++--+--++-+++-+-++++--++---+---++--++++-+-+++-++--+++--++-++-+----+++---+++----+-++-++--+++++++-++++-+-+--++-+-+++++-+-++--+-+-++++-+----
+--++-+-++++------+---+------++++-+-++--+--++-+++-+-++++--++---+---++--++++-+-+++-++--+++--++-++-+----+++---+++----+-++-++--+++++-+-++++-+-+--++-+-+++++-+-++--+-+-++++-+---
-+--++-+-++++------+---+------++++-+-++--+--++-+++-+-++++--++---+---++--++++-+-+++-++-++++--++-++-+----+++---+++----+-++-++--++++--+-++++-+-+--++-+-+++++-+-++--+-+-++++-+--
--+--++-+-++++------+---+------++++-+-++--+--++-+++-+-++++--++---+---++--++++-+-+++-+++++++--++-++-+----+++---+++----+-++-++--+++---+-++++-+-+--++-+-+++++-+-++--+-+-++++-+-
+--+--++-+-++++------+---+------++++-+-++--+--++-+++-+-++++--++---+---++--++++-+-+++-+++++++--++-++-+----+++---+++----+-++-++--++----+-++++-+-+--++-+-+++++-+-++--+-+-++++-+
-+--+--++-+-++++------+---+------++++-+-++-++--++-+++-+-++++--++---+---++--++++-+-+++-+++++++--++-++-+----+++---+++----+-++-++--++----+-++++-+-+--++-+-+++++-+-++--+-+-++++-
--+--+--++-+-++++------+---+------++++-+-++-++--++-+++-+-++++--++---+---++--++++-+-+++++++++++--++-++-+----+++---+++----+-++-++---+----+-++++-+-+--++-+-+++++-+-++--+-+-++++
+--+--+--++-+-++++------+---+------++++-+-++-++--++-+++-+-++++--++---+---++--++++-+-++-++++++++--++-++-+----+++---+++----+-++-++-+-+----+-++++-+-+--++-+-+++++-+-++--+-+-+++
++--+--+--++-+-++++------+---+------++++-+-++-++--++-+++-+-++++--++---+---++--++++-+-+--++++++++--++-++-+----+++---+++----+-++-++++-+----+-++++-+-+--++-+-+++++-+-++--+-+-++
-++--+--+--++-+-++++------+---+------++++-++++-++--++-+++-+-++++--++---+---++--++++-+-+--++++++++--++-++-+----+++---+++----+-++-++++-+----+-++++-+-+--++-+-+++++-+-++--+-+-+
+-++--+--+--++-+-++++------+---+------++++--+++-++--++-+++-+-++++--++---+---++--++++-+++--++++++++--++-++-+----+++---+++----+-++-++++-+----+-++++-+-+--++-+-+++++-+-++--+-+-
-+-++--+--+--++-+-++++------+---+------+++++-+++-++--++-+++-+-++++--++---+---++--++++--++--++++++++--++-++-+----+++---+++----+-++-++++-+----+-++++-+-+--++-+-+++++-+-++--+-+
+-+-++--+--+--++-+-++++------+---+------+++-+-+++-++--++-+++-+-++++--++---+---++--+++++-++--++++++++--++-++-+----+++---+++----+-++-++++-+----+-++++-+-+--++-+-+++++-+-++--+-
++-+-++--+--+--++-+-++++------+---+------+++-+-+++-++--++-+++-+-++++--++---+---++--+++++-++--++++++++--++-++-+----+++---+++----+--+-++++-+----+-++++-+-+--++-+-+++++-+-++--+
+++-+-++--+--+--++-+-++++------+---+------+++-+-+++-++--++-+++-+-++++--++---+---++--++-++-++--++++++++--++-++-+----+++---+++----++-+-++++-+----+-++++-+-+--++-+-+++++-+-++--
++++-+-++--+--+--++-+-++++------+---+------+++-+-+++-++--++-+++-+-++++--++---+---++--++-++-++--++++++++--++-++-+----+++---+++-----+-+-++++-+----+-++++-+-+--++-+-+++++-+-++-
-++++-+-++--+--+--++-+-++++------+---+-----++++-+-+++-++--++-+++-+-++++--++---+---++---+-++-++--++++++++--++-++-+----+++---+++-----+-+-++++-+----+-++++-+-+--++-+-+++++-+-++
--++++-+-++--+--+--++-+-++++------+---+-----++++-+-+++-++--++-+++-+-++++--++---+---++---+-++-++--++++++++--++-++-+----+++---+++--+--+-+-++++-+----+-++++-+-+--++-+-+++++-+-+
---++++-+-++--+--+--++-+-++++------+---+-----++++-+-+++-++--++-+++-+-++++--++---+---++---+-++-++--++++++++--++-++-+----+++---+++-++--+-+-++++-+----+-++++-+-+--++-+-+++++-+-
----++++-+-++--+--+--++-+-++++------+---+--+--++++-+-+++-++--++-+++-+-++++--++---+---+----+-++-++--++++++++--++-++-+----+++---+++-++--+-+-++++-+----+-++++-+-+--++-+-+++++-+
-----++++-+-++--+--+--++-+-++++------+---+-++--++++-+-+++-++--++-+++-+-++++--++---+---+----+-++-++--++++++++--++-++-+----+++---+++-++--+-+-++++-+----+-++++-+-+--++-+-+++++-
------++++-+-++--+--+--++-+-++++------+---+-++--++++-+-+++-++--++-+++-+-++++--++---+--++----+-++-++--++++++++--++-++-+----+++---+-+-++--+-+-++++-+----+-++++-+-+--++-+-+++++
+------++++-+-++--+--+--++-+-++++------+-----++--++++-+-+++-++--++-+++-+-++++--++---+-+++----+-++-++--++++++++--++-++-+----+++---+-+-++--+-+-++++-+----+-++++-+-+--++-+-++++
-+------++++-+-++--+--+--++-+-++++------+-----++--++++-+-+++-++--++-+++-+-++++--++---+-+++----+-++-++--++++++++--++-++-+----+++--++-+-++--+-+-++++-+----+-++++-+-+--++-+-+++
---+-+--++-+-+----+-++++-+----+-+-++--+-+--++---++++-+--+--++--------++--+--+-++++---++---++--++++-+-+++-++--++-+++-+-++++--++-----+------++++-+-++--+--+--++-+-++++------+-
----+-+--++-+-+----+-++++-+----+-+-++--+-+-+++---++++-+--+--++--------++--+--+-++++----+---++--++++-+-+++-++--++-+++-+-++++--++-----+------++++-+-++--+--+--++-+-++++------+
-----+-+--++-+-+----+-++++-+----+-+-++--+-+-+++---++++-+--+--++--------++--+--+-++++----+---++--++++-+-+++-++--++-+++-+-++++--++-+---+------++++-+-++--+--+--++-+-++++------
+-----+-+--++-+-+----+-++++-+----+-+-++--+---+++---++++-+--+--++--------++--+--+-++++----+---++--++++-+-+++-++--++-+++-+-++++--++-+---+------++++-+-++--+--+--++-+-++++-----
-+-----+-+--++-+-+----+-++++-+----+-+-++--+---+++---++++-+--+--++--------++--+--+-+++++---+---++--++++-+-+++-++--++-+++-+-++++--+--+---+------++++-+-++--+--+--++-+-++++----
+-+-----+-+--++-+-+----+-++++-+----+-+-++--+---+++---++++-+--+--++--------++--+--+-+++++---+---++--++++-+-+++-++--++-+++-+-++++-----+---+------++++-+-++--+--+--++-+-++++---
-+-+-----+-+--++-+-+----+-++++-+----+-+-++-++---+++---++++-+--+--++--------++--+--+-++-++---+---++--++++-+-+++-++--++-+++-+-++++-----+---+------++++-+-++--+--+--++-+-++++--
--+-+-----+-+--++-+-+----+-++++-+----+-+-+++++---+++---++++-+--+--++--------++--+--+-+--++---+---++--++++-+-+++-++--++-+++-+-++++-----+---+------++++-+-++--+--+--++-+-++++-
+--+-+-----+-+--++-+-+----+-++++-+----+-+-+++++---+++---++++-+--+--++--------++--+--+-+--++---+---++--++++-+-+++-++--++-+++-+-+++------+---+------++++-+-++--+--+--++-+-++++
++--+-+-----+-+--++-+-+----+-++++-+----+-+--++++---+++---++++-+--+--++--------++--+--+++--++---+---++--++++-+-+++-++--++-+++-+-+++------+---+------++++-+-++--+--+--++-+-+++
-++--+-+-----+-+--++-+-+----+-++++-+----+-++-++++---+++---++++-+--+--++--------++--+--+++--++---+---++--++++-+-+++-++--++-+++-+-+++------+---+------++++-+-++--+--+--++-+-++
+-++--+-+-----+-+--++-+-+----+-++++-+----+--+-++++---+++---++++-+--+--++--------++--+-++++--++---+---++--++++-+-+++-++--++-+++-+-+++------+---+------++++-+-++--+--+--++-+-+
-+-++--+-+-----+-+--++-+-+----+-++++-+----+--+-++++---+++---++++-+--+--++--------++--+-++++--++---+---++--++++-+-+++-++--++-+++-+++++------+---+------++++-+-++--+--+--++-+-
+-+-++--+-+-----+-+--++-+-+----+-++++-+----+--+-++++---+++---++++-+--+--++--------++--+-++++--++---+---++--++++-+-+++-++--++-+++--++++------+---+------++++-+-++--+--+--++-+
-+-+-++--+-+-----+-+--++-+-+----+-++++-+----+--+-++++---+++---++++-+--+--++--------++--+-++++--++---+---++--++++-+-+++-++--++-++++-++++------+---+------++++-+-++--+--+--++-
--+-+-++--+-+-----+-+--++-+-+----+-++++-+----+--+-++++---+++---++++-+--+--++--------+++-+-++++--++---+---++--++++-+-+++-++--++-++-+-++++------+---+------++++-+-++--+--+--++
---+-+-++--+-+-----+-+--++-+-+----+-++++-+-+--+--+-++++---+++---++++-+--+--++--------+++-+-++++--++---+---++--++++-+-+++-++--++-++-+-++++------+---+------++++-+-++--+--+--+
----+-+-++--+-+-----+-+--++-+-+----+-++++-+++--+--+-++++---+++---++++-+--+--++--------+++-+-++++--++---+---++--++++-+-+++-++--++-++-+-++++------+---+------++++-+-++--+--+--
+----+-+-++--+-+-----+-+--++-+-+----+-++++--++--+--+-++++---+++---++++-+--+--++--------+++-+-++++--++---+---++--++++-+-+++-++--++-++-+-++++------+---+------++++-+-++--+--+-
-+----+-+-++--+-+-----+-+--++-+-+----+-++++--++--+--+-++++---+++---++++-+--+--++------+-+++-+-++++--++---+---++--++++-+-+++-++--+--++-+-++++------+---+------++++-+-++--+--+
+-+----+-+-++--+-+-----+-+--++-+-+----+-+++---++--+--+-++++---+++---++++-+--+--++-----++-+++-+-++++--++---+---++--++++-+-+++-++--+--++-+-++++------+---+------++++-+-++--+--
++-+----+-+-++--+-+-----+-+--++-+-+----+-++----++--+--+-++++---+++---++++-+--+--++-----++-+++-+-++++--++---+---++--++++-+-+++-++--+--++-+-++++------+---+------++++-+-++--+-
+++-+----+-+-++--+-+-----+-+--++-+-+----+-+-----++--+--+-++++---+++---++++-+--+--++-----++-+++-+-++++--++---+---++--++++-+-+++-++--+--++-+-++++------+---+------++++-+-++--+
++++-+----+-+-++--+-+-----+-+--++-+-+----+-------++--+--+-++++---+++---++++-+--+--++--+--++-+++-+-++++--++---+---++--++++-+-+++-++--+--++-+-++++------+---+------++++-+-++--
-++++-+----+-+-++--+-+-----+-+--++-+-+----+-------++--+--+-++++---+++---++++-+--+--++-++--++-+++-+-++++--++---+---++--++++-+-+++--+--+--++-+-++++------+---+------++++-+-++-
+-++++-+----+-+-++--+-+-----+-+--++-+-+------------++--+--+-++++---+++---++++-+--+--++-++--++-+++-+-++++--++---+---++--++++-+-+++--+--+--++-+-++++------+---+------++++-+-++
-+-++++-+----+-+-++--+-+-----+-+--++-+-+---+--------++--+--+-++++---+++---++++-+--+--++-++--++-+++-+-++++--++---+---++--++++-+-+++--+--+--++-+-++++------+---+------++++-+-+
--+-++++-+----+-+-++--+-+-----+-+--++-+-+--++--------++--+--+-++++---+++---++++-+--+--++-++--++-+++-+-++++--++---+---++--++++-+-+++--+--+--++-+-++++------+---+------++++-+-
---+-++++-+----+-+-++--+-+-----+-+--++-+-+--++--------++--+--+-++++---+++---++++-+--+-+++-++--++-+++-+-++++--++---+---++--++++-+--++--+--+--++-+-++++------+---+------++++-+
----+-++++-+----+-+-++--+-+-----+-+--++-+-+--++--------++--+--+-++++---+++---++++-+--+-+++-++--++-+++-+-++++--++---+---++--++++-++-++--+--+--++-+-++++------+---+------++++-
+----+-++++-+----+-+-++--+-+-----+-+--++-+-+--++--------++--+--+-++++---+++---++++-+--+-+++-++--++-+++-+-++++--++---+---++--++++--+-++--+--+--++-+-++++------+---+------++++
-+----+-++++-+----+-+-++--+-+-----+-+--++-+-+--++--------++--+--+-++++---+++---++++-+--+-+++-++--++-+++-+-++++--++---+---++--+++++-+-++--+--+--++-+-++++------+---+------+++
+-+----+-++++-+----+-+-++--+-+-----+-+--++---+--++--------++--+--+-++++---+++---++++-++-+-+++-++--++-+++-+-++++--++---+---++--+++++-+-++--+--+--++-+-++++------+---+------++
-+-+----+-++++-+----+-+-++--+-+-----+-+--+++--+--++--------++--+--+-++++---+++---++++-++-+-+++-++--++-+++-+-++++--++---+---++--+++++-+-++--+--+--++-+-++++------+---+------+
+-+-+----+-++++-+----+-+-++--+-+-----+-+--+-+--+--++--------++--+--+-++++---+++---+++++++-+-+++-++--++-+++-+-++++--++---+---++--+++++-+-++--+--+--++-+-++++------+---+------
++-+-+----+-++++-+----+-+-++--+-+-----+-+--+-+--+--++--------++--+--+-++++---+++---+++++++-+-+++-++--++-+++-+-++++--++---+---++---++++-+-++--+--+--++-+-++++------+---+-----
-++-+-+----+-++++-+----+-+-++--+-+-----+-+-++-+--+--++--------++--+--+-++++---+++---++-++++-+-+++-++--++-+++-+-++++--++---+---++---++++-+-++--+--+--++-+-++++------+---+----
--++-+-+----+-++++-+----+-+-++--+-+-----+-++++-+--+--++--------++--+--+-++++---+++---+--++++-+-+++-++--++-+++-+-++++--++---+---++---++++-+-++--+--+--++-+-++++------+---+---
+--++-+-+----+-++++-+----+-+-++--+-+-----+-++++-+--+--++--------++--+--+-++++---+++---+--++++-+-+++-++--++-+++-+-++++--++---+---+----++++-+-++--+--+--++-+-++++------+---+--
-+--++-+-+----+-++++-+----+-+-++--+-+-----+-++++-+--+--++--------++--+--+-++++---+++--++--++++-+-+++-++--++-+++-+-++++--++---+--------++++-+-++--+--+--++-+-++++------+---+-
+-+--++-+-+----+-++++-+----+-+-++--+-+-------++++-+--+--++--------++--+--+-++++---+++--++--++++-+-+++-++--++-+++-+-++++--++---+--------++++-+-++--+--+--++-+-++++------+---+
-+-+--++-+-+----+-++++-+----+-+-++--+-+-------++++-+--+--++--------++--+--+-++++---+++--++--++++-+-+++-++--++-+++-+-++++--++---+-+------++++-+-++--+--+--++-+-++++------+---
--+-+--++-+-+----+-++++-+----+-+-++--+-+---+---++++-+--+--++--------++--+--+-++++---++---++--++++-+-+++-++--++-+++-+-++++--++---+-+------++++-+-++--+--+--++-+-++++------+--
--+++----+-++-++--++++++++--++-++-+----+++----+-+--++-+-+----+-++++-+----+-+-++--+-+--++-++++++----+-+--++-++-++--+-+----++++++-++---++--++++-+-+++-++--++-+++-+-++++--++---
---+++----+-++-++--++++++++--++-++-+----+++----+-+--++-+-+----+-++++-+----+-+-++--+-+-+++-++++++----+-+--++-++-++--+-+----++++++--+---++--++++-+-+++-++--++-+++-+-++++--++--
+---+++----+-++-++--++++++++--++-++-+----++-----+-+--++-+-+----+-++++-+----+-+-++--+-+-+++-++++++----+-+--++-++-++--+-+----++++++--+---++--++++-+-+++-++--++-+++-+-++++--++-
++---+++----+-++-++--++++++++--++-++-+----++-----+-+--++-+-+----+-++++-+----+-+-++--+-+-+++-++++++----+-+--++-++-++--+-+----+++++---+---++--++++-+-+++-++--++-+++-+-++++--++
+++---+++----+-++-++--++++++++--++-++-+-----+-----+-+--++-+-+----+-++++-+----+-+-++--+++-+++-++++++----+-+--++-++-++--+-+----+++++---+---++--++++-+-+++-++--++-+++-+-++++--+
-+++---+++----+-++-++--++++++++--++-++-+---+-+-----+-+--++-+-+----+-++++-+----+-+-++--+++-+++-++++++----+-+--++-++-++--+-+----+++++---+---++--++++-+-+++-++--++-+++-+-++++--
--+++---+++----+-++-++--++++++++--++-++-+---+-+-----+-+--++-+-+----+-++++-+----+-+-++-++++-+++-++++++----+-+--++-++-++--+-+----++-++---+---++--++++-+-+++-++--++-+++-+-++++-
---+++---+++----+-++-++--++++++++--++-++-+---+-+-----+-+--++-+-+----+-++++-+----+-+-+++++++-+++-++++++----+-+--++-++-++--+-+----+--++---+---++--++++-+-+++-++--++-+++-+-++++
----+++---+++----+-++-++--++++++++--++-++-++--+-+-----+-+--++-+-+----+-++++-+----+-+-+++++++-+++-++++++----+-+--++-++-++--+-+----+--++---+---++--++++-+-+++-++--++-+++-+-+++
+----+++---+++----+-++-++--++++++++--++-++-++--+-+-----+-+--++-+-+----+-++++-+----+-+--++++++-+++-++++++----+-+--++-++-++--+-+---++--++---+---++--++++-+-+++-++--++-+++-+-++
-+----+++---+++----+-++-++--++++++++--++-++-++--+-+-----+-+--++-+-+----+-++++-+----+-+--++++++-+++-++++++----+-+--++-++-++--+-+--+++--++---+---++--++++-+-+++-++--++-+++-+-+
+-+----+++---+++----+-++-++--++++++++--++-++-++--+-+-----+-+--++-+-+----+-++++-+----+----++++++-+++-++++++----+-+--++-++-++--+-+-++++--++---+---++--++++-+-+++-++--++-+++-+-
++-+----+++---+++----+-++-++--++++++++--++--+-++--+-+-----+-+--++-+-+----+-++++-+----+----++++++-+++-++++++----+-+--++-++-++--+-+-++++--++---+---++--++++-+-+++-++--++-+++-+
-++-+----+++---+++----+-++-++--++++++++--+++-+-++--+-+-----+-+--++-+-+----+-++++-+----+----++++++-+++-++++++----+-+--++-++-++--+-+-++++--++---+---++--++++-+-+++-++--++-+++-
+-++-+----+++---+++----+-++-++--++++++++--+-+-+-++--+-+-----+-+--++-+-+----+-++++-+----+----++++++-+++-++++++----+-+--++-++-++--+-+-++++--++---+---++--++++-+-+++-++--++-+++
++-++-+----+++---+++----+-++-++--++++++++----+-+-++--+-+-----+-+--++-+-+----+-++++-+--+-+----++++++-+++-++++++----+-+--++-++-++--+-+-++++--++---+---++--++++-+-+++-++--++-++
-++-++-+----+++---+++----+-++-++--++++++++----+-+-++--+-+-----+-+--++-+-+----+-++++-+--+-+----++++++-+++-++++++----+-+--++-++-++-++-+-++++--++---+---++--++++-+-+++-++--++-+
--++-++-+----+++---+++----+-++-++--++++++++----+-+-++--+-+-----+-+--++-+-+----+-++++-+--+-+----++++++-+++-++++++----+-+--++-++-+++++-+-++++--++---+---++--++++-+-+++-++--++-
+--++-++-+----+++---+++----+-++-++--++++++++----+-+-++--+-+-----+-+--++-+-+----+-++++-+--+-+----++++++-+++-++++++----+-+--++-++-+-+++-+-++++--++---+---++--++++-+-+++-++--++
++--++-++-+----+++---+++----+-++-++--++++++-+----+-+-++--+-+-----+-+--++-+-+----+-++++++--+-+----++++++-+++-++++++----+-+--++-++-+-+++-+-++++--++---+---++--++++-+-+++-++--+
+++--++-++-+----+++---+++----+-++-++--++++++-+----+-+-++--+-+-----+-+--++-+-+----+-+++-++--+-+----++++++-+++-++++++----+-+--++-++++-+++-+-++++--++---+---++--++++-+-+++-++--
++++--++-++-+----+++---+++----+-++-++--++++++-+----+-+-++--+-+-----+-+--++-+-+----+-+++-++--+-+----++++++-+++-++++++----+-+--++-+-++-+++-+-++++--++---+---++--++++-+-+++-++-
+++++--++-++-+----+++---+++----+-++-++--++++++-+----+-+-++--+-+-----+-+--++-+-+----+-+++-++--+-+----++++++-+++-++++++----+-+--++---++-+++-+-++++--++---+---++--++++-+-+++-++
++++++--++-++-+----+++---+++----+-++-++--++++++-+----+-+-++--+-+-----+-+--++-+-+----+--++-++--+-+----++++++-+++-++++++----+-+--+++--++-+++-+-++++--++---+---++--++++-+-+++-+
+++++++--++-++-+----+++---+++----+-++-++--+-++++-+----+-+-++--+-+-----+-+--++-+-+----++-++-++--+-+----++++++-+++-++++++----+-+--+++--++-+++-+-++++--++---+---++--++++-+-+++-
++++++++--++-++-+----+++---+++----+-++-++--+-++++-+----+-+-++--+-+-----+-+--++-+-+----++-++-++--+-+----++++++-+++-++++++----+-+---++--++-+++-+-++++--++---+---++--++++-+-+++
-++++++++--++-++-+----+++---+++----+-++-++--+-++++-+----+-+-++--+-+-----+-+--++-+-+----++-++-++--+-+----++++++-+++-++++++----+-+-+-++--++-+++-+-++++--++---+---++--++++-+-++
--++++++++--++-++-+----+++---+++----+-++-++--+-++++-+----+-+-++--+-+-----+-+--++-+-+----++-++-++--+-+----++++++-+++-++++++----+-+++-++--++-+++-+-++++--++---+---++--++++-+-+
+--++++++++--++-++-+----+++---+++----+-++-+---+-++++-+----+-+-++--+-+-----+-+--++-+-+-+--++-++-++--+-+----++++++-+++-++++++----+-+++-++--++-+++-+-++++--++---+---++--++++-+-
++--++++++++--++-++-+----+++---+++----+-++-----+-++++-+----+-+-++--+-+-----+-+--++-+-+-+--++-++-++--+-+----++++++-+++-++++++----+-+++-++--++-+++-+-++++--++---+---++--++++-+
-++--++++++++--++-++-+----+++---+++----+-+++----+-++++-+----+-+-++--+-+-----+-+--++-+-+-+--++-++-++--+-+----++++++-+++-++++++----+-+++-++--++-+++-+-++++--++---+---++--++++-
+-++--++++++++--++-++-+----+++---+++----+-+-+----+-++++-+----+-+-++--+-+-----+-+--++-+-+-+--++-++-++--+-+----++++++-+++-++++++----+-+++-++--++-+++-+-++++--++---+---++--++++
++-++--++++++++--++-++-+----+++---+++----+-+-+----+-++++-+----+-+-++--+-+-----+-+--++---+-+--++-++-++--+-+----++++++-+++-++++++--+-+-+++-++--++-+++-+-++++--++---+---++--+++
-++-++--++++++++--++-++-+----+++---+++----+-+-+----+-++++-+----+-+-++--+-+-----+-+--++---+-+--++-++-++--+-+----++++++-+++-++++++-++-+-+++-++--++-+++-+-++++--++---+---++--++
+-++-++--++++++++--++-++-+----+++---+++----+-+-+----+-++++-+----+-+-++--+-+-----+-+--+----+-+--++-++-++--+-+----++++++-+++-+++++++++-+-+++-++--++-+++-+-++++--++---+---++--+
-+-++-++--++++++++--++-++-+----+++---+++---++-+-+----+-++++-+----+-+-++--+-+-----+-+--+----+-+--++-++-++--+-+----++++++-+++-+++++++++-+-+++-++--++-+++-+-++++--++---+---++--
--+-++-++--++++++++--++-++-+----+++---+++---++-+-+----+-++++-+----+-+-++--+-+-----+-+-++----+-+--++-++-++--+-+----++++++-+++-++++-++++-+-+++-++--++-+++-+-++++--++---+---++-
---+-++-++--++++++++--++-++-+----+++---+++---++-+-+----+-++++-+----+-+-++--+-+-----+-++++----+-+--++-++-++--+-+----++++++-+++-+++--++++-+-+++-++--++-+++-+-++++--++---+---++
----+-++-++--++++++++--++-++-+----+++---++++--++-+-+----+-++++-+----+-+-++--+-+-----+-++++----+-+--++-++-++--+-+----++++++-+++-+++--++++-+-+++-++--++-+++-+-++++--++---+---+
+----+-++-++--++++++++--++-++-+----+++---++-+--++-+-+----+-++++-+----+-+-++--+-+-----++++++----+-+--++-++-++--+-+----++++++-+++-+++--++++-+-+++-++--++-+++-+-++++--++---+---
++----+-++-++--++++++++--++-++-+----+++---++-+--++-+-+----+-++++-+----+-+-++--+-+-----++++++----+-+--++-++-++--+-+----++++++-+++--++--++++-+-+++-++--++-+++-+-++++--++---+--
+++----+-++-++--++++++++--++-++-+----+++----+-+--++-+-+----+-++++-+----+-+-++--+-+-----++++++----+-+--++-++-++--+-+----++++++-+++--++--++++-+-+++-++--++-+++-+-++++--++---+-
-+++----+-++-++--++++++++--++-++-+----+++----+-+--++-+-+----+-++++-+----+-+-++--+-+---+-++++++----+-+--++-++-++--+-+----++++++-++---++--++++-+-+++-++--++-+++-+-++++--++---+
</content>

<content full_path="exllamav3/util/hadamard_data/primes.txt">
2
3
5
7
11
13
17
19
23
29
31
37
41
43
47
53
59
61
67
71
73
79
83
89
97
101
103
107
109
113
127
131
137
139
149
151
157
163
167
173
179
181
191
193
197
199
211
223
227
229
233
239
241
251
257
263
269
271
277
281
283
293
307
311
313
317
331
337
347
349
353
359
367
373
379
383
389
397
401
409
419
421
431
433
439
443
449
457
461
463
467
479
487
491
499
503
509
521
523
541
547
557
563
569
571
577
587
593
599
601
607
613
617
619
631
641
643
647
653
659
661
673
677
683
691
701
709
719
727
733
739
743
751
757
761
769
773
787
797
809
811
821
823
827
829
839
853
857
859
863
877
881
883
887
907
911
919
929
937
941
947
953
967
971
977
983
991
997
1009
1013
1019
1021
1031
1033
1039
1049
1051
1061
1063
1069
1087
1091
1093
1097
1103
1109
1117
1123
1129
1151
1153
1163
1171
1181
1187
1193
1201
1213
1217
1223
1229
1231
1237
1249
1259
1277
1279
1283
1289
1291
1297
1301
1303
1307
1319
1321
1327
1361
1367
1373
1381
1399
1409
1423
1427
1429
1433
1439
1447
1451
1453
1459
1471
1481
1483
1487
1489
1493
1499
1511
1523
1531
1543
1549
1553
1559
1567
1571
1579
1583
1597
1601
1607
1609
1613
1619
1621
1627
1637
1657
1663
1667
1669
1693
1697
1699
1709
1721
1723
1733
1741
1747
1753
1759
1777
1783
1787
1789
1801
1811
1823
1831
1847
1861
1867
1871
1873
1877
1879
1889
1901
1907
1913
1931
1933
1949
1951
1973
1979
1987
1993
1997
1999
2003
2011
2017
2027
2029
2039
2053
2063
2069
2081
2083
2087
2089
2099
2111
2113
2129
2131
2137
2141
2143
2153
2161
2179
2203
2207
2213
2221
2237
2239
2243
2251
2267
2269
2273
2281
2287
2293
2297
2309
2311
2333
2339
2341
2347
2351
2357
2371
2377
2381
2383
2389
2393
2399
2411
2417
2423
2437
2441
2447
2459
2467
2473
2477
2503
2521
2531
2539
2543
2549
2551
2557
2579
2591
2593
2609
2617
2621
2633
2647
2657
2659
2663
2671
2677
2683
2687
2689
2693
2699
2707
2711
2713
2719
2729
2731
2741
2749
2753
2767
2777
2789
2791
2797
2801
2803
2819
2833
2837
2843
2851
2857
2861
2879
2887
2897
2903
2909
2917
2927
2939
2953
2957
2963
2969
2971
2999
3001
3011
3019
3023
3037
3041
3049
3061
3067
3079
3083
3089
3109
3119
3121
3137
3163
3167
3169
3181
3187
3191
3203
3209
3217
3221
3229
3251
3253
3257
3259
3271
3299
3301
3307
3313
3319
3323
3329
3331
3343
3347
3359
3361
3371
3373
3389
3391
3407
3413
3433
3449
3457
3461
3463
3467
3469
3491
3499
3511
3517
3527
3529
3533
3539
3541
3547
3557
3559
3571
3581
3583
3593
3607
3613
3617
3623
3631
3637
3643
3659
3671
3673
3677
3691
3697
3701
3709
3719
3727
3733
3739
3761
3767
3769
3779
3793
3797
3803
3821
3823
3833
3847
3851
3853
3863
3877
3881
3889
3907
3911
3917
3919
3923
3929
3931
3943
3947
3967
3989
4001
4003
4007
4013
4019
4021
4027
4049
4051
4057
4073
4079
4091
4093
4099
4111
4127
4129
4133
4139
4153
4157
4159
4177
4201
4211
4217
4219
4229
4231
4241
4243
4253
4259
4261
4271
4273
4283
4289
4297
4327
4337
4339
4349
4357
4363
4373
4391
4397
4409
4421
4423
4441
4447
4451
4457
4463
4481
4483
4493
4507
4513
4517
4519
4523
4547
4549
4561
4567
4583
4591
4597
4603
4621
4637
4639
4643
4649
4651
4657
4663
4673
4679
4691
4703
4721
4723
4729
4733
4751
4759
4783
4787
4789
4793
4799
4801
4813
4817
4831
4861
4871
4877
4889
4903
4909
4919
4931
4933
4937
4943
4951
4957
4967
4969
4973
4987
4993
4999
5003
5009
5011
5021
5023
5039
5051
5059
5077
5081
5087
5099
5101
5107
5113
5119
5147
5153
5167
5171
5179
5189
5197
5209
5227
5231
5233
5237
5261
5273
5279
5281
5297
5303
5309
5323
5333
5347
5351
5381
5387
5393
5399
5407
5413
5417
5419
5431
5437
5441
5443
5449
5471
5477
5479
5483
5501
5503
5507
5519
5521
5527
5531
5557
5563
5569
5573
5581
5591
5623
5639
5641
5647
5651
5653
5657
5659
5669
5683
5689
5693
5701
5711
5717
5737
5741
5743
5749
5779
5783
5791
5801
5807
5813
5821
5827
5839
5843
5849
5851
5857
5861
5867
5869
5879
5881
5897
5903
5923
5927
5939
5953
5981
5987
6007
6011
6029
6037
6043
6047
6053
6067
6073
6079
6089
6091
6101
6113
6121
6131
6133
6143
6151
6163
6173
6197
6199
6203
6211
6217
6221
6229
6247
6257
6263
6269
6271
6277
6287
6299
6301
6311
6317
6323
6329
6337
6343
6353
6359
6361
6367
6373
6379
6389
6397
6421
6427
6449
6451
6469
6473
6481
6491
6521
6529
6547
6551
6553
6563
6569
6571
6577
6581
6599
6607
6619
6637
6653
6659
6661
6673
6679
6689
6691
6701
6703
6709
6719
6733
6737
6761
6763
6779
6781
6791
6793
6803
6823
6827
6829
6833
6841
6857
6863
6869
6871
6883
6899
6907
6911
6917
6947
6949
6959
6961
6967
6971
6977
6983
6991
6997
7001
7013
7019
7027
7039
7043
7057
7069
7079
7103
7109
7121
7127
7129
7151
7159
7177
7187
7193
7207
7211
7213
7219
7229
7237
7243
7247
7253
7283
7297
7307
7309
7321
7331
7333
7349
7351
7369
7393
7411
7417
7433
7451
7457
7459
7477
7481
7487
7489
7499
7507
7517
7523
7529
7537
7541
7547
7549
7559
7561
7573
7577
7583
7589
7591
7603
7607
7621
7639
7643
7649
7669
7673
7681
7687
7691
7699
7703
7717
7723
7727
7741
7753
7757
7759
7789
7793
7817
7823
7829
7841
7853
7867
7873
7877
7879
7883
7901
7907
7919
7927
7933
7937
7949
7951
7963
7993
8009
8011
8017
8039
8053
8059
8069
8081
8087
8089
8093
8101
8111
8117
8123
8147
8161
8167
8171
8179
8191
8209
8219
8221
8231
8233
8237
8243
8263
8269
8273
8287
8291
8293
8297
8311
8317
8329
8353
8363
8369
8377
8387
8389
8419
8423
8429
8431
8443
8447
8461
8467
8501
8513
8521
8527
8537
8539
8543
8563
8573
8581
8597
8599
8609
8623
8627
8629
8641
8647
8663
8669
8677
8681
8689
8693
8699
8707
8713
8719
8731
8737
8741
8747
8753
8761
8779
8783
8803
8807
8819
8821
8831
8837
8839
8849
8861
8863
8867
8887
8893
8923
8929
8933
8941
8951
8963
8969
8971
8999
9001
9007
9011
9013
9029
9041
9043
9049
9059
9067
9091
9103
9109
9127
9133
9137
9151
9157
9161
9173
9181
9187
9199
9203
9209
9221
9227
9239
9241
9257
9277
9281
9283
9293
9311
9319
9323
9337
9341
9343
9349
9371
9377
9391
9397
9403
9413
9419
9421
9431
9433
9437
9439
9461
9463
9467
9473
9479
9491
9497
9511
9521
9533
9539
9547
9551
9587
9601
9613
9619
9623
9629
9631
9643
9649
9661
9677
9679
9689
9697
9719
9721
9733
9739
9743
9749
9767
9769
9781
9787
9791
9803
9811
9817
9829
9833
9839
9851
9857
9859
9871
9883
9887
9901
9907
9923
9929
9931
9941
9949
9967
9973
10007
10009
10037
10039
10061
10067
10069
10079
10091
10093
10099
10103
10111
10133
10139
10141
10151
10159
10163
10169
10177
10181
10193
10211
10223
10243
10247
10253
10259
10267
10271
10273
10289
10301
10303
10313
10321
10331
10333
10337
10343
10357
10369
10391
10399
10427
10429
10433
10453
10457
10459
10463
10477
10487
10499
10501
10513
10529
10531
10559
10567
10589
10597
10601
10607
10613
10627
10631
10639
10651
10657
10663
10667
10687
10691
10709
10711
10723
10729
10733
10739
10753
10771
10781
10789
10799
10831
10837
10847
10853
10859
10861
10867
10883
10889
10891
10903
10909
10937
10939
10949
10957
10973
10979
10987
10993
11003
11027
11047
11057
11059
11069
11071
11083
11087
11093
11113
11117
11119
11131
11149
11159
11161
11171
11173
11177
11197
11213
11239
11243
11251
11257
11261
11273
11279
11287
11299
11311
11317
11321
11329
11351
11353
11369
11383
11393
11399
11411
11423
11437
11443
11447
11467
11471
11483
11489
11491
11497
11503
11519
11527
11549
11551
11579
11587
11593
11597
11617
11621
11633
11657
11677
11681
11689
11699
11701
11717
11719
11731
11743
11777
11779
11783
11789
11801
11807
11813
11821
11827
11831
11833
11839
11863
11867
11887
11897
11903
11909
11923
11927
11933
11939
11941
11953
11959
11969
11971
11981
11987
12007
12011
12037
12041
12043
12049
12071
12073
12097
12101
12107
12109
12113
12119
12143
12149
12157
12161
12163
12197
12203
12211
12227
12239
12241
12251
12253
12263
12269
12277
12281
12289
12301
12323
12329
12343
12347
12373
12377
12379
12391
12401
12409
12413
12421
12433
12437
12451
12457
12473
12479
12487
12491
12497
12503
12511
12517
12527
12539
12541
12547
12553
12569
12577
12583
12589
12601
12611
12613
12619
12637
12641
12647
12653
12659
12671
12689
12697
12703
12713
12721
12739
12743
12757
12763
12781
12791
12799
12809
12821
12823
12829
12841
12853
12889
12893
12899
12907
12911
12917
12919
12923
12941
12953
12959
12967
12973
12979
12983
13001
13003
13007
13009
13033
13037
13043
13049
13063
13093
13099
13103
13109
13121
13127
13147
13151
13159
13163
13171
13177
13183
13187
13217
13219
13229
13241
13249
13259
13267
13291
13297
13309
13313
13327
13331
13337
13339
13367
13381
13397
13399
13411
13417
13421
13441
13451
13457
13463
13469
13477
13487
13499
13513
13523
13537
13553
13567
13577
13591
13597
13613
13619
13627
13633
13649
13669
13679
13681
13687
13691
13693
13697
13709
13711
13721
13723
13729
13751
13757
13759
13763
13781
13789
13799
13807
13829
13831
13841
13859
13873
13877
13879
13883
13901
13903
13907
13913
13921
13931
13933
13963
13967
13997
13999
14009
14011
14029
14033
14051
14057
14071
14081
14083
14087
14107
14143
14149
14153
14159
14173
14177
14197
14207
14221
14243
14249
14251
14281
14293
14303
14321
14323
14327
14341
14347
14369
14387
14389
14401
14407
14411
14419
14423
14431
14437
14447
14449
14461
14479
14489
14503
14519
14533
14537
14543
14549
14551
14557
14561
14563
14591
14593
14621
14627
14629
14633
14639
14653
14657
14669
14683
14699
14713
14717
14723
14731
14737
14741
14747
14753
14759
14767
14771
14779
14783
14797
14813
14821
14827
14831
14843
14851
14867
14869
14879
14887
14891
14897
14923
14929
14939
14947
14951
14957
14969
14983
15013
15017
15031
15053
15061
15073
15077
15083
15091
15101
15107
15121
15131
15137
15139
15149
15161
15173
15187
15193
15199
15217
15227
15233
15241
15259
15263
15269
15271
15277
15287
15289
15299
15307
15313
15319
15329
15331
15349
15359
15361
15373
15377
15383
15391
15401
15413
15427
15439
15443
15451
15461
15467
15473
15493
15497
15511
15527
15541
15551
15559
15569
15581
15583
15601
15607
15619
15629
15641
15643
15647
15649
15661
15667
15671
15679
15683
15727
15731
15733
15737
15739
15749
15761
15767
15773
15787
15791
15797
15803
15809
15817
15823
15859
15877
15881
15887
15889
15901
15907
15913
15919
15923
15937
15959
15971
15973
15991
16001
16007
16033
16057
16061
16063
16067
16069
16073
16087
16091
16097
16103
16111
16127
16139
16141
16183
16187
16189
16193
16217
16223
16229
16231
16249
16253
16267
16273
16301
16319
16333
16339
16349
16361
16363
16369
16381
16411
16417
16421
16427
16433
16447
16451
16453
16477
16481
16487
16493
16519
16529
16547
16553
16561
16567
16573
16603
16607
16619
16631
16633
16649
16651
16657
16661
16673
16691
16693
16699
16703
16729
16741
16747
16759
16763
16787
16811
16823
16829
16831
16843
16871
16879
16883
16889
16901
16903
16921
16927
16931
16937
16943
16963
16979
16981
16987
16993
17011
17021
17027
17029
17033
17041
17047
17053
17077
17093
17099
17107
17117
17123
17137
17159
17167
17183
17189
17191
17203
17207
17209
17231
17239
17257
17291
17293
17299
17317
17321
17327
17333
17341
17351
17359
17377
17383
17387
17389
17393
17401
17417
17419
17431
17443
17449
17467
17471
17477
17483
17489
17491
17497
17509
17519
17539
17551
17569
17573
17579
17581
17597
17599
17609
17623
17627
17657
17659
17669
17681
17683
17707
17713
17729
17737
17747
17749
17761
17783
17789
17791
17807
17827
17837
17839
17851
17863
17881
17891
17903
17909
17911
17921
17923
17929
17939
17957
17959
17971
17977
17981
17987
17989
18013
18041
18043
18047
18049
18059
18061
18077
18089
18097
18119
18121
18127
18131
18133
18143
18149
18169
18181
18191
18199
18211
18217
18223
18229
18233
18251
18253
18257
18269
18287
18289
18301
18307
18311
18313
18329
18341
18353
18367
18371
18379
18397
18401
18413
18427
18433
18439
18443
18451
18457
18461
18481
18493
18503
18517
18521
18523
18539
18541
18553
18583
18587
18593
18617
18637
18661
18671
18679
18691
18701
18713
18719
18731
18743
18749
18757
18773
18787
18793
18797
18803
18839
18859
18869
18899
18911
18913
18917
18919
18947
18959
18973
18979
19001
19009
19013
19031
19037
19051
19069
19073
19079
19081
19087
19121
19139
19141
19157
19163
19181
19183
19207
19211
19213
19219
19231
19237
19249
19259
19267
19273
19289
19301
19309
19319
19333
19373
19379
19381
19387
19391
19403
19417
19421
19423
19427
19429
19433
19441
19447
19457
19463
19469
19471
19477
19483
19489
19501
19507
19531
19541
19543
19553
19559
19571
19577
19583
19597
19603
19609
19661
19681
19687
19697
19699
19709
19717
19727
19739
19751
19753
19759
19763
19777
19793
19801
19813
19819
19841
19843
19853
19861
19867
19889
19891
19913
19919
19927
19937
19949
19961
19963
19973
19979
19991
19993
19997
20011
20021
20023
20029
20047
20051
20063
20071
20089
20101
20107
20113
20117
20123
20129
20143
20147
20149
20161
20173
20177
20183
20201
20219
20231
20233
20249
20261
20269
20287
20297
20323
20327
20333
20341
20347
20353
20357
20359
20369
20389
20393
20399
20407
20411
20431
20441
20443
20477
20479
20483
20507
20509
20521
20533
20543
20549
20551
20563
20593
20599
20611
20627
20639
20641
20663
20681
20693
20707
20717
20719
20731
20743
20747
20749
20753
20759
20771
20773
20789
20807
20809
20849
20857
20873
20879
20887
20897
20899
20903
20921
20929
20939
20947
20959
20963
20981
20983
21001
21011
21013
21017
21019
21023
21031
21059
21061
21067
21089
21101
21107
21121
21139
21143
21149
21157
21163
21169
21179
21187
21191
21193
21211
21221
21227
21247
21269
21277
21283
21313
21317
21319
21323
21341
21347
21377
21379
21383
21391
21397
21401
21407
21419
21433
21467
21481
21487
21491
21493
21499
21503
21517
21521
21523
21529
21557
21559
21563
21569
21577
21587
21589
21599
21601
21611
21613
21617
21647
21649
21661
21673
21683
21701
21713
21727
21737
21739
21751
21757
21767
21773
21787
21799
21803
21817
21821
21839
21841
21851
21859
21863
21871
21881
21893
21911
21929
21937
21943
21961
21977
21991
21997
22003
22013
22027
22031
22037
22039
22051
22063
22067
22073
22079
22091
22093
22109
22111
22123
22129
22133
22147
22153
22157
22159
22171
22189
22193
22229
22247
22259
22271
22273
22277
22279
22283
22291
22303
22307
22343
22349
22367
22369
22381
22391
22397
22409
22433
22441
22447
22453
22469
22481
22483
22501
22511
22531
22541
22543
22549
22567
22571
22573
22613
22619
22621
22637
22639
22643
22651
22669
22679
22691
22697
22699
22709
22717
22721
22727
22739
22741
22751
22769
22777
22783
22787
22807
22811
22817
22853
22859
22861
22871
22877
22901
22907
22921
22937
22943
22961
22963
22973
22993
23003
23011
23017
23021
23027
23029
23039
23041
23053
23057
23059
23063
23071
23081
23087
23099
23117
23131
23143
23159
23167
23173
23189
23197
23201
23203
23209
23227
23251
23269
23279
23291
23293
23297
23311
23321
23327
23333
23339
23357
23369
23371
23399
23417
23431
23447
23459
23473
23497
23509
23531
23537
23539
23549
23557
23561
23563
23567
23581
23593
23599
23603
23609
23623
23627
23629
23633
23663
23669
23671
23677
23687
23689
23719
23741
23743
23747
23753
23761
23767
23773
23789
23801
23813
23819
23827
23831
23833
23857
23869
23873
23879
23887
23893
23899
23909
23911
23917
23929
23957
23971
23977
23981
23993
24001
24007
24019
24023
24029
24043
24049
24061
24071
24077
24083
24091
24097
24103
24107
24109
24113
24121
24133
24137
24151
24169
24179
24181
24197
24203
24223
24229
24239
24247
24251
24281
24317
24329
24337
24359
24371
24373
24379
24391
24407
24413
24419
24421
24439
24443
24469
24473
24481
24499
24509
24517
24527
24533
24547
24551
24571
24593
24611
24623
24631
24659
24671
24677
24683
24691
24697
24709
24733
24749
24763
24767
24781
24793
24799
24809
24821
24841
24847
24851
24859
24877
24889
24907
24917
24919
24923
24943
24953
24967
24971
24977
24979
24989
25013
25031
25033
25037
25057
25073
25087
25097
25111
25117
25121
25127
25147
25153
25163
25169
25171
25183
25189
25219
25229
25237
25243
25247
25253
25261
25301
25303
25307
25309
25321
25339
25343
25349
25357
25367
25373
25391
25409
25411
25423
25439
25447
25453
25457
25463
25469
25471
25523
25537
25541
25561
25577
25579
25583
25589
25601
25603
25609
25621
25633
25639
25643
25657
25667
25673
25679
25693
25703
25717
25733
25741
25747
25759
25763
25771
25793
25799
25801
25819
25841
25847
25849
25867
25873
25889
25903
25913
25919
25931
25933
25939
25943
25951
25969
25981
25997
25999
26003
26017
26021
26029
26041
26053
26083
26099
26107
26111
26113
26119
26141
26153
26161
26171
26177
26183
26189
26203
26209
26227
26237
26249
26251
26261
26263
26267
26293
26297
26309
26317
26321
26339
26347
26357
26371
26387
26393
26399
26407
26417
26423
26431
26437
26449
26459
26479
26489
26497
26501
26513
26539
26557
26561
26573
26591
26597
26627
26633
26641
26647
26669
26681
26683
26687
26693
26699
26701
26711
26713
26717
26723
26729
26731
26737
26759
26777
26783
26801
26813
26821
26833
26839
26849
26861
26863
26879
26881
26891
26893
26903
26921
26927
26947
26951
26953
26959
26981
26987
26993
27011
27017
27031
27043
27059
27061
27067
27073
27077
27091
27103
27107
27109
27127
27143
27179
27191
27197
27211
27239
27241
27253
27259
27271
27277
27281
27283
27299
27329
27337
27361
27367
27397
27407
27409
27427
27431
27437
27449
27457
27479
27481
27487
27509
27527
27529
27539
27541
27551
27581
27583
27611
27617
27631
27647
27653
27673
27689
27691
27697
27701
27733
27737
27739
27743
27749
27751
27763
27767
27773
27779
27791
27793
27799
27803
27809
27817
27823
27827
27847
27851
27883
27893
27901
27917
27919
27941
27943
27947
27953
27961
27967
27983
27997
28001
28019
28027
28031
28051
28057
28069
28081
28087
28097
28099
28109
28111
28123
28151
28163
28181
28183
28201
28211
28219
28229
28277
28279
28283
28289
28297
28307
28309
28319
28349
28351
28387
28393
28403
28409
28411
28429
28433
28439
28447
28463
28477
28493
28499
28513
28517
28537
28541
28547
28549
28559
28571
28573
28579
28591
28597
28603
28607
28619
28621
28627
28631
28643
28649
28657
28661
28663
28669
28687
28697
28703
28711
28723
28729
28751
28753
28759
28771
28789
28793
28807
28813
28817
28837
28843
28859
28867
28871
28879
28901
28909
28921
28927
28933
28949
28961
28979
29009
29017
29021
29023
29027
29033
29059
29063
29077
29101
29123
29129
29131
29137
29147
29153
29167
29173
29179
29191
29201
29207
29209
29221
29231
29243
29251
29269
29287
29297
29303
29311
29327
29333
29339
29347
29363
29383
29387
29389
29399
29401
29411
29423
29429
29437
29443
29453
29473
29483
29501
29527
29531
29537
29567
29569
29573
29581
29587
29599
29611
29629
29633
29641
29663
29669
29671
29683
29717
29723
29741
29753
29759
29761
29789
29803
29819
29833
29837
29851
29863
29867
29873
29879
29881
29917
29921
29927
29947
29959
29983
29989
30011
30013
30029
30047
30059
30071
30089
30091
30097
30103
30109
30113
30119
30133
30137
30139
30161
30169
30181
30187
30197
30203
30211
30223
30241
30253
30259
30269
30271
30293
30307
30313
30319
30323
30341
30347
30367
30389
30391
30403
30427
30431
30449
30467
30469
30491
30493
30497
30509
30517
30529
30539
30553
30557
30559
30577
30593
30631
30637
30643
30649
30661
30671
30677
30689
30697
30703
30707
30713
30727
30757
30763
30773
30781
30803
30809
30817
30829
30839
30841
30851
30853
30859
30869
30871
30881
30893
30911
30931
30937
30941
30949
30971
30977
30983
31013
31019
31033
31039
31051
31063
31069
31079
31081
31091
31121
31123
31139
31147
31151
31153
31159
31177
31181
31183
31189
31193
31219
31223
31231
31237
31247
31249
31253
31259
31267
31271
31277
31307
31319
31321
31327
31333
31337
31357
31379
31387
31391
31393
31397
31469
31477
31481
31489
31511
31513
31517
31531
31541
31543
31547
31567
31573
31583
31601
31607
31627
31643
31649
31657
31663
31667
31687
31699
31721
31723
31727
31729
31741
31751
31769
31771
31793
31799
31817
31847
31849
31859
31873
31883
31891
31907
31957
31963
31973
31981
31991
32003
32009
32027
32029
32051
32057
32059
32063
32069
32077
32083
32089
32099
32117
32119
32141
32143
32159
32173
32183
32189
32191
32203
32213
32233
32237
32251
32257
32261
32297
32299
32303
32309
32321
32323
32327
32341
32353
32359
32363
32369
32371
32377
32381
32401
32411
32413
32423
32429
32441
32443
32467
32479
32491
32497
32503
32507
32531
32533
32537
32561
32563
32569
32573
32579
32587
32603
32609
32611
32621
32633
32647
32653
32687
32693
32707
32713
32717
32719
32749
32771
32779
32783
32789
32797
32801
32803
32831
32833
32839
32843
32869
32887
32909
32911
32917
32933
32939
32941
32957
32969
32971
32983
32987
32993
32999
33013
33023
33029
33037
33049
33053
33071
33073
33083
33091
33107
33113
33119
33149
33151
33161
33179
33181
33191
33199
33203
33211
33223
33247
33287
33289
33301
33311
33317
33329
33331
33343
33347
33349
33353
33359
33377
33391
33403
33409
33413
33427
33457
33461
33469
33479
33487
33493
33503
33521
33529
33533
33547
33563
33569
33577
33581
33587
33589
33599
33601
33613
33617
33619
33623
33629
33637
33641
33647
33679
33703
33713
33721
33739
33749
33751
33757
33767
33769
33773
33791
33797
33809
33811
33827
33829
33851
33857
33863
33871
33889
33893
33911
33923
33931
33937
33941
33961
33967
33997
34019
34031
34033
34039
34057
34061
34123
34127
34129
34141
34147
34157
34159
34171
34183
34211
34213
34217
34231
34253
34259
34261
34267
34273
34283
34297
34301
34303
34313
34319
34327
34337
34351
34361
34367
34369
34381
34403
34421
34429
34439
34457
34469
34471
34483
34487
34499
34501
34511
34513
34519
34537
34543
34549
34583
34589
34591
34603
34607
34613
34631
34649
34651
34667
34673
34679
34687
34693
34703
34721
34729
34739
34747
34757
34759
34763
34781
34807
34819
34841
34843
34847
34849
34871
34877
34883
34897
34913
34919
34939
34949
34961
34963
34981
35023
35027
35051
35053
35059
35069
35081
35083
35089
35099
35107
35111
35117
35129
35141
35149
35153
35159
35171
35201
35221
35227
35251
35257
35267
35279
35281
35291
35311
35317
35323
35327
35339
35353
35363
35381
35393
35401
35407
35419
35423
35437
35447
35449
35461
35491
35507
35509
35521
35527
35531
35533
35537
35543
35569
35573
35591
35593
35597
35603
35617
35671
35677
35729
35731
35747
35753
35759
35771
35797
35801
35803
35809
35831
35837
35839
35851
35863
35869
35879
35897
35899
35911
35923
35933
35951
35963
35969
35977
35983
35993
35999
36007
36011
36013
36017
36037
36061
36067
36073
36083
36097
36107
36109
36131
36137
36151
36161
36187
36191
36209
36217
36229
36241
36251
36263
36269
36277
36293
36299
36307
36313
36319
36341
36343
36353
36373
36383
36389
36433
36451
36457
36467
36469
36473
36479
36493
36497
36523
36527
36529
36541
36551
36559
36563
36571
36583
36587
36599
36607
36629
36637
36643
36653
36671
36677
36683
36691
36697
36709
36713
36721
36739
36749
36761
36767
36779
36781
36787
36791
36793
36809
36821
36833
36847
36857
36871
36877
36887
36899
36901
36913
36919
36923
36929
36931
36943
36947
36973
36979
36997
37003
37013
37019
37021
37039
37049
37057
37061
37087
37097
37117
37123
37139
37159
37171
37181
37189
37199
37201
37217
37223
37243
37253
37273
37277
37307
37309
37313
37321
37337
37339
37357
37361
37363
37369
37379
37397
37409
37423
37441
37447
37463
37483
37489
37493
37501
37507
37511
37517
37529
37537
37547
37549
37561
37567
37571
37573
37579
37589
37591
37607
37619
37633
37643
37649
37657
37663
37691
37693
37699
37717
37747
37781
37783
37799
37811
37813
37831
37847
37853
37861
37871
37879
37889
37897
37907
37951
37957
37963
37967
37987
37991
37993
37997
38011
38039
38047
38053
38069
38083
38113
38119
38149
38153
38167
38177
38183
38189
38197
38201
38219
38231
38237
38239
38261
38273
38281
38287
38299
38303
38317
38321
38327
38329
38333
38351
38371
38377
38393
38431
38447
38449
38453
38459
38461
38501
38543
38557
38561
38567
38569
38593
38603
38609
38611
38629
38639
38651
38653
38669
38671
38677
38693
38699
38707
38711
38713
38723
38729
38737
38747
38749
38767
38783
38791
38803
38821
38833
38839
38851
38861
38867
38873
38891
38903
38917
38921
38923
38933
38953
38959
38971
38977
38993
39019
39023
39041
39043
39047
39079
39089
39097
39103
39107
39113
39119
39133
39139
39157
39161
39163
39181
39191
39199
39209
39217
39227
39229
39233
39239
39241
39251
39293
39301
39313
39317
39323
39341
39343
39359
39367
39371
39373
39383
39397
39409
39419
39439
39443
39451
39461
39499
39503
39509
39511
39521
39541
39551
39563
39569
39581
39607
39619
39623
39631
39659
39667
39671
39679
39703
39709
39719
39727
39733
39749
39761
39769
39779
39791
39799
39821
39827
39829
39839
39841
39847
39857
39863
39869
39877
39883
39887
39901
39929
39937
39953
39971
39979
39983
39989
40009
40013
40031
40037
40039
40063
40087
40093
40099
40111
40123
40127
40129
40151
40153
40163
40169
40177
40189
40193
40213
40231
40237
40241
40253
40277
40283
40289
40343
40351
40357
40361
40387
40423
40427
40429
40433
40459
40471
40483
40487
40493
40499
40507
40519
40529
40531
40543
40559
40577
40583
40591
40597
40609
40627
40637
40639
40693
40697
40699
40709
40739
40751
40759
40763
40771
40787
40801
40813
40819
40823
40829
40841
40847
40849
40853
40867
40879
40883
40897
40903
40927
40933
40939
40949
40961
40973
40993
41011
41017
41023
41039
41047
41051
41057
41077
41081
41113
41117
41131
41141
41143
41149
41161
41177
41179
41183
41189
41201
41203
41213
41221
41227
41231
41233
41243
41257
41263
41269
41281
41299
41333
41341
41351
41357
41381
41387
41389
41399
41411
41413
41443
41453
41467
41479
41491
41507
41513
41519
41521
41539
41543
41549
41579
41593
41597
41603
41609
41611
41617
41621
41627
41641
41647
41651
41659
41669
41681
41687
41719
41729
41737
41759
41761
41771
41777
41801
41809
41813
41843
41849
41851
41863
41879
41887
41893
41897
41903
41911
41927
41941
41947
41953
41957
41959
41969
41981
41983
41999
42013
42017
42019
42023
42043
42061
42071
42073
42083
42089
42101
42131
42139
42157
42169
42179
42181
42187
42193
42197
42209
42221
42223
42227
42239
42257
42281
42283
42293
42299
42307
42323
42331
42337
42349
42359
42373
42379
42391
42397
42403
42407
42409
42433
42437
42443
42451
42457
42461
42463
42467
42473
42487
42491
42499
42509
42533
42557
42569
42571
42577
42589
42611
42641
42643
42649
42667
42677
42683
42689
42697
42701
42703
42709
42719
42727
42737
42743
42751
42767
42773
42787
42793
42797
42821
42829
42839
42841
42853
42859
42863
42899
42901
42923
42929
42937
42943
42953
42961
42967
42979
42989
43003
43013
43019
43037
43049
43051
43063
43067
43093
43103
43117
43133
43151
43159
43177
43189
43201
43207
43223
43237
43261
43271
43283
43291
43313
43319
43321
43331
43391
43397
43399
43403
43411
43427
43441
43451
43457
43481
43487
43499
43517
43541
43543
43573
43577
43579
43591
43597
43607
43609
43613
43627
43633
43649
43651
43661
43669
43691
43711
43717
43721
43753
43759
43777
43781
43783
43787
43789
43793
43801
43853
43867
43889
43891
43913
43933
43943
43951
43961
43963
43969
43973
43987
43991
43997
44017
44021
44027
44029
44041
44053
44059
44071
44087
44089
44101
44111
44119
44123
44129
44131
44159
44171
44179
44189
44201
44203
44207
44221
44249
44257
44263
44267
44269
44273
44279
44281
44293
44351
44357
44371
44381
44383
44389
44417
44449
44453
44483
44491
44497
44501
44507
44519
44531
44533
44537
44543
44549
44563
44579
44587
44617
44621
44623
44633
44641
44647
44651
44657
44683
44687
44699
44701
44711
44729
44741
44753
44771
44773
44777
44789
44797
44809
44819
44839
44843
44851
44867
44879
44887
44893
44909
44917
44927
44939
44953
44959
44963
44971
44983
44987
45007
45013
45053
45061
45077
45083
45119
45121
45127
45131
45137
45139
45161
45179
45181
45191
45197
45233
45247
45259
45263
45281
45289
45293
45307
45317
45319
45329
45337
45341
45343
45361
45377
45389
45403
45413
45427
45433
45439
45481
45491
45497
45503
45523
45533
45541
45553
45557
45569
45587
45589
45599
45613
45631
45641
45659
45667
45673
45677
45691
45697
45707
45737
45751
45757
45763
45767
45779
45817
45821
45823
45827
45833
45841
45853
45863
45869
45887
45893
45943
45949
45953
45959
45971
45979
45989
46021
46027
46049
46051
46061
46073
46091
46093
46099
46103
46133
46141
46147
46153
46171
46181
46183
46187
46199
46219
46229
46237
46261
46271
46273
46279
46301
46307
46309
46327
46337
46349
46351
46381
46399
46411
46439
46441
46447
46451
46457
46471
46477
46489
46499
46507
46511
46523
46549
46559
46567
46573
46589
46591
46601
46619
46633
46639
46643
46649
46663
46679
46681
46687
46691
46703
46723
46727
46747
46751
46757
46769
46771
46807
46811
46817
46819
46829
46831
46853
46861
46867
46877
46889
46901
46919
46933
46957
46993
46997
47017
47041
47051
47057
47059
47087
47093
47111
47119
47123
47129
47137
47143
47147
47149
47161
47189
47207
47221
47237
47251
47269
47279
47287
47293
47297
47303
47309
47317
47339
47351
47353
47363
47381
47387
47389
47407
47417
47419
47431
47441
47459
47491
47497
47501
47507
47513
47521
47527
47533
47543
47563
47569
47581
47591
47599
47609
47623
47629
47639
47653
47657
47659
47681
47699
47701
47711
47713
47717
47737
47741
47743
47777
47779
47791
47797
47807
47809
47819
47837
47843
47857
47869
47881
47903
47911
47917
47933
47939
47947
47951
47963
47969
47977
47981
48017
48023
48029
48049
48073
48079
48091
48109
48119
48121
48131
48157
48163
48179
48187
48193
48197
48221
48239
48247
48259
48271
48281
48299
48311
48313
48337
48341
48353
48371
48383
48397
48407
48409
48413
48437
48449
48463
48473
48479
48481
48487
48491
48497
48523
48527
48533
48539
48541
48563
48571
48589
48593
48611
48619
48623
48647
48649
48661
48673
48677
48679
48731
48733
48751
48757
48761
48767
48779
48781
48787
48799
48809
48817
48821
48823
48847
48857
48859
48869
48871
48883
48889
48907
48947
48953
48973
48989
48991
49003
49009
49019
49031
49033
49037
49043
49057
49069
49081
49103
49109
49117
49121
49123
49139
49157
49169
49171
49177
49193
49199
49201
49207
49211
49223
49253
49261
49277
49279
49297
49307
49331
49333
49339
49363
49367
49369
49391
49393
49409
49411
49417
49429
49433
49451
49459
49463
49477
49481
49499
49523
49529
49531
49537
49547
49549
49559
49597
49603
49613
49627
49633
49639
49663
49667
49669
49681
49697
49711
49727
49739
49741
49747
49757
49783
49787
49789
49801
49807
49811
49823
49831
49843
49853
49871
49877
49891
49919
49921
49927
49937
49939
49943
49957
49991
49993
49999
50021
50023
50033
50047
50051
50053
50069
50077
50087
50093
50101
50111
50119
50123
50129
50131
50147
50153
50159
50177
50207
50221
50227
50231
50261
50263
50273
50287
50291
50311
50321
50329
50333
50341
50359
50363
50377
50383
50387
50411
50417
50423
50441
50459
50461
50497
50503
50513
50527
50539
50543
50549
50551
50581
50587
50591
50593
50599
50627
50647
50651
50671
50683
50707
50723
50741
50753
50767
50773
50777
50789
50821
50833
50839
50849
50857
50867
50873
50891
50893
50909
50923
50929
50951
50957
50969
50971
50989
50993
51001
51031
51043
51047
51059
51061
51071
51109
51131
51133
51137
51151
51157
51169
51193
51197
51199
51203
51217
51229
51239
51241
51257
51263
51283
51287
51307
51329
51341
51343
51347
51349
51361
51383
51407
51413
51419
51421
51427
51431
51437
51439
51449
51461
51473
51479
51481
51487
51503
51511
51517
51521
51539
51551
51563
51577
51581
51593
51599
51607
51613
51631
51637
51647
51659
51673
51679
51683
51691
51713
51719
51721
51749
51767
51769
51787
51797
51803
51817
51827
51829
51839
51853
51859
51869
51871
51893
51899
51907
51913
51929
51941
51949
51971
51973
51977
51991
52009
52021
52027
52051
52057
52067
52069
52081
52103
52121
52127
52147
52153
52163
52177
52181
52183
52189
52201
52223
52237
52249
52253
52259
52267
52289
52291
52301
52313
52321
52361
52363
52369
52379
52387
52391
52433
52453
52457
52489
52501
52511
52517
52529
52541
52543
52553
52561
52567
52571
52579
52583
52609
52627
52631
52639
52667
52673
52691
52697
52709
52711
52721
52727
52733
52747
52757
52769
52783
52807
52813
52817
52837
52859
52861
52879
52883
52889
52901
52903
52919
52937
52951
52957
52963
52967
52973
52981
52999
53003
53017
53047
53051
53069
53077
53087
53089
53093
53101
53113
53117
53129
53147
53149
53161
53171
53173
53189
53197
53201
53231
53233
53239
53267
53269
53279
53281
53299
53309
53323
53327
53353
53359
53377
53381
53401
53407
53411
53419
53437
53441
53453
53479
53503
53507
53527
53549
53551
53569
53591
53593
53597
53609
53611
53617
53623
53629
53633
53639
53653
53657
53681
53693
53699
53717
53719
53731
53759
53773
53777
53783
53791
53813
53819
53831
53849
53857
53861
53881
53887
53891
53897
53899
53917
53923
53927
53939
53951
53959
53987
53993
54001
54011
54013
54037
54049
54059
54083
54091
54101
54121
54133
54139
54151
54163
54167
54181
54193
54217
54251
54269
54277
54287
54293
54311
54319
54323
54331
54347
54361
54367
54371
54377
54401
54403
54409
54413
54419
54421
54437
54443
54449
54469
54493
54497
54499
54503
54517
54521
54539
54541
54547
54559
54563
54577
54581
54583
54601
54617
54623
54629
54631
54647
54667
54673
54679
54709
54713
54721
54727
54751
54767
54773
54779
54787
54799
54829
54833
54851
54869
54877
54881
54907
54917
54919
54941
54949
54959
54973
54979
54983
55001
55009
55021
55049
55051
55057
55061
55073
55079
55103
55109
55117
55127
55147
55163
55171
55201
55207
55213
55217
55219
55229
55243
55249
55259
55291
55313
55331
55333
55337
55339
55343
55351
55373
55381
55399
55411
55439
55441
55457
55469
55487
55501
55511
55529
55541
55547
55579
55589
55603
55609
55619
55621
55631
55633
55639
55661
55663
55667
55673
55681
55691
55697
55711
55717
55721
55733
55763
55787
55793
55799
55807
55813
55817
55819
55823
55829
55837
55843
55849
55871
55889
55897
55901
55903
55921
55927
55931
55933
55949
55967
55987
55997
56003
56009
56039
56041
56053
56081
56087
56093
56099
56101
56113
56123
56131
56149
56167
56171
56179
56197
56207
56209
56237
56239
56249
56263
56267
56269
56299
56311
56333
56359
56369
56377
56383
56393
56401
56417
56431
56437
56443
56453
56467
56473
56477
56479
56489
56501
56503
56509
56519
56527
56531
56533
56543
56569
56591
56597
56599
56611
56629
56633
56659
56663
56671
56681
56687
56701
56711
56713
56731
56737
56747
56767
56773
56779
56783
56807
56809
56813
56821
56827
56843
56857
56873
56891
56893
56897
56909
56911
56921
56923
56929
56941
56951
56957
56963
56983
56989
56993
56999
57037
57041
57047
57059
57073
57077
57089
57097
57107
57119
57131
57139
57143
57149
57163
57173
57179
57191
57193
57203
57221
57223
57241
57251
57259
57269
57271
57283
57287
57301
57329
57331
57347
57349
57367
57373
57383
57389
57397
57413
57427
57457
57467
57487
57493
57503
57527
57529
57557
57559
57571
57587
57593
57601
57637
57641
57649
57653
57667
57679
57689
57697
57709
57713
57719
57727
57731
57737
57751
57773
57781
57787
57791
57793
57803
57809
57829
57839
57847
57853
57859
57881
57899
57901
57917
57923
57943
57947
57973
57977
57991
58013
58027
58031
58043
58049
58057
58061
58067
58073
58099
58109
58111
58129
58147
58151
58153
58169
58171
58189
58193
58199
58207
58211
58217
58229
58231
58237
58243
58271
58309
58313
58321
58337
58363
58367
58369
58379
58391
58393
58403
58411
58417
58427
58439
58441
58451
58453
58477
58481
58511
58537
58543
58549
58567
58573
58579
58601
58603
58613
58631
58657
58661
58679
58687
58693
58699
58711
58727
58733
58741
58757
58763
58771
58787
58789
58831
58889
58897
58901
58907
58909
58913
58921
58937
58943
58963
58967
58979
58991
58997
59009
59011
59021
59023
59029
59051
59053
59063
59069
59077
59083
59093
59107
59113
59119
59123
59141
59149
59159
59167
59183
59197
59207
59209
59219
59221
59233
59239
59243
59263
59273
59281
59333
59341
59351
59357
59359
59369
59377
59387
59393
59399
59407
59417
59419
59441
59443
59447
59453
59467
59471
59473
59497
59509
59513
59539
59557
59561
59567
59581
59611
59617
59621
59627
59629
59651
59659
59663
59669
59671
59693
59699
59707
59723
59729
59743
59747
59753
59771
59779
59791
59797
59809
59833
59863
59879
59887
59921
59929
59951
59957
59971
59981
59999
60013
60017
60029
60037
60041
60077
60083
60089
60091
60101
60103
60107
60127
60133
60139
60149
60161
60167
60169
60209
60217
60223
60251
60257
60259
60271
60289
60293
60317
60331
60337
60343
60353
60373
60383
60397
60413
60427
60443
60449
60457
60493
60497
60509
60521
60527
60539
60589
60601
60607
60611
60617
60623
60631
60637
60647
60649
60659
60661
60679
60689
60703
60719
60727
60733
60737
60757
60761
60763
60773
60779
60793
60811
60821
60859
60869
60887
60889
60899
60901
60913
60917
60919
60923
60937
60943
60953
60961
61001
61007
61027
61031
61043
61051
61057
61091
61099
61121
61129
61141
61151
61153
61169
61211
61223
61231
61253
61261
61283
61291
61297
61331
61333
61339
61343
61357
61363
61379
61381
61403
61409
61417
61441
61463
61469
61471
61483
61487
61493
61507
61511
61519
61543
61547
61553
61559
61561
61583
61603
61609
61613
61627
61631
61637
61643
61651
61657
61667
61673
61681
61687
61703
61717
61723
61729
61751
61757
61781
61813
61819
61837
61843
61861
61871
61879
61909
61927
61933
61949
61961
61967
61979
61981
61987
61991
62003
62011
62017
62039
62047
62053
62057
62071
62081
62099
62119
62129
62131
62137
62141
62143
62171
62189
62191
62201
62207
62213
62219
62233
62273
62297
62299
62303
62311
62323
62327
62347
62351
62383
62401
62417
62423
62459
62467
62473
62477
62483
62497
62501
62507
62533
62539
62549
62563
62581
62591
62597
62603
62617
62627
62633
62639
62653
62659
62683
62687
62701
62723
62731
62743
62753
62761
62773
62791
62801
62819
62827
62851
62861
62869
62873
62897
62903
62921
62927
62929
62939
62969
62971
62981
62983
62987
62989
63029
63031
63059
63067
63073
63079
63097
63103
63113
63127
63131
63149
63179
63197
63199
63211
63241
63247
63277
63281
63299
63311
63313
63317
63331
63337
63347
63353
63361
63367
63377
63389
63391
63397
63409
63419
63421
63439
63443
63463
63467
63473
63487
63493
63499
63521
63527
63533
63541
63559
63577
63587
63589
63599
63601
63607
63611
63617
63629
63647
63649
63659
63667
63671
63689
63691
63697
63703
63709
63719
63727
63737
63743
63761
63773
63781
63793
63799
63803
63809
63823
63839
63841
63853
63857
63863
63901
63907
63913
63929
63949
63977
63997
64007
64013
64019
64033
64037
64063
64067
64081
64091
64109
64123
64151
64153
64157
64171
64187
64189
64217
64223
64231
64237
64271
64279
64283
64301
64303
64319
64327
64333
64373
64381
64399
64403
64433
64439
64451
64453
64483
64489
64499
64513
64553
64567
64577
64579
64591
64601
64609
64613
64621
64627
64633
64661
64663
64667
64679
64693
64709
64717
64747
64763
64781
64783
64793
64811
64817
64849
64853
64871
64877
64879
64891
64901
64919
64921
64927
64937
64951
64969
64997
65003
65011
65027
65029
65033
65053
65063
65071
65089
65099
65101
65111
65119
65123
65129
65141
65147
65167
65171
65173
65179
65183
65203
65213
65239
65257
65267
65269
65287
65293
65309
65323
65327
65353
65357
65371
65381
65393
65407
65413
65419
65423
65437
65447
65449
65479
65497
65519
65521
65537
65539
65543
65551
65557
65563
65579
65581
65587
65599
65609
65617
65629
65633
65647
65651
65657
65677
65687
65699
65701
65707
65713
65717
65719
65729
65731
65761
65777
65789
65809
65827
65831
65837
65839
65843
65851
65867
65881
65899
65921
65927
65929
65951
65957
65963
65981
65983
65993
66029
66037
66041
66047
66067
66071
66083
66089
66103
66107
66109
66137
66161
66169
66173
66179
66191
66221
66239
66271
66293
66301
66337
66343
66347
66359
66361
66373
66377
66383
66403
66413
66431
66449
66457
66463
66467
66491
66499
66509
66523
66529
66533
66541
66553
66569
66571
66587
66593
66601
66617
66629
66643
66653
66683
66697
66701
66713
66721
66733
66739
66749
66751
66763
66791
66797
66809
66821
66841
66851
66853
66863
66877
66883
66889
66919
66923
66931
66943
66947
66949
66959
66973
66977
67003
67021
67033
67043
67049
67057
67061
67073
67079
67103
67121
67129
67139
67141
67153
67157
67169
67181
67187
67189
67211
67213
67217
67219
67231
67247
67261
67271
67273
67289
67307
67339
67343
67349
67369
67391
67399
67409
67411
67421
67427
67429
67433
67447
67453
67477
67481
67489
67493
67499
67511
67523
67531
67537
67547
67559
67567
67577
67579
67589
67601
67607
67619
67631
67651
67679
67699
67709
67723
67733
67741
67751
67757
67759
67763
67777
67783
67789
67801
67807
67819
67829
67843
67853
67867
67883
67891
67901
67927
67931
67933
67939
67943
67957
67961
67967
67979
67987
67993
68023
68041
68053
68059
68071
68087
68099
68111
68113
68141
68147
68161
68171
68207
68209
68213
68219
68227
68239
68261
68279
68281
68311
68329
68351
68371
68389
68399
68437
68443
68447
68449
68473
68477
68483
68489
68491
68501
68507
68521
68531
68539
68543
68567
68581
68597
68611
68633
68639
68659
68669
68683
68687
68699
68711
68713
68729
68737
68743
68749
68767
68771
68777
68791
68813
68819
68821
68863
68879
68881
68891
68897
68899
68903
68909
68917
68927
68947
68963
68993
69001
69011
69019
69029
69031
69061
69067
69073
69109
69119
69127
69143
69149
69151
69163
69191
69193
69197
69203
69221
69233
69239
69247
69257
69259
69263
69313
69317
69337
69341
69371
69379
69383
69389
69401
69403
69427
69431
69439
69457
69463
69467
69473
69481
69491
69493
69497
69499
69539
69557
69593
69623
69653
69661
69677
69691
69697
69709
69737
69739
69761
69763
69767
69779
69809
69821
69827
69829
69833
69847
69857
69859
69877
69899
69911
69929
69931
69941
69959
69991
69997
70001
70003
70009
70019
70039
70051
70061
70067
70079
70099
70111
70117
70121
70123
70139
70141
70157
70163
70177
70181
70183
70199
70201
70207
70223
70229
70237
70241
70249
70271
70289
70297
70309
70313
70321
70327
70351
70373
70379
70381
70393
70423
70429
70439
70451
70457
70459
70481
70487
70489
70501
70507
70529
70537
70549
70571
70573
70583
70589
70607
70619
70621
70627
70639
70657
70663
70667
70687
70709
70717
70729
70753
70769
70783
70793
70823
70841
70843
70849
70853
70867
70877
70879
70891
70901
70913
70919
70921
70937
70949
70951
70957
70969
70979
70981
70991
70997
70999
71011
71023
71039
71059
71069
71081
71089
71119
71129
71143
71147
71153
71161
71167
71171
71191
71209
71233
71237
71249
71257
71261
71263
71287
71293
71317
71327
71329
71333
71339
71341
71347
71353
71359
71363
71387
71389
71399
71411
71413
71419
71429
71437
71443
71453
71471
71473
71479
71483
71503
71527
71537
71549
71551
71563
71569
71593
71597
71633
71647
71663
71671
71693
71699
71707
71711
71713
71719
71741
71761
71777
71789
71807
71809
71821
71837
71843
71849
71861
71867
71879
71881
71887
71899
71909
71917
71933
71941
71947
71963
71971
71983
71987
71993
71999
72019
72031
72043
72047
72053
72073
72077
72089
72091
72101
72103
72109
72139
72161
72167
72169
72173
72211
72221
72223
72227
72229
72251
72253
72269
72271
72277
72287
72307
72313
72337
72341
72353
72367
72379
72383
72421
72431
72461
72467
72469
72481
72493
72497
72503
72533
72547
72551
72559
72577
72613
72617
72623
72643
72647
72649
72661
72671
72673
72679
72689
72701
72707
72719
72727
72733
72739
72763
72767
72797
72817
72823
72859
72869
72871
72883
72889
72893
72901
72907
72911
72923
72931
72937
72949
72953
72959
72973
72977
72997
73009
73013
73019
73037
73039
73043
73061
73063
73079
73091
73121
73127
73133
73141
73181
73189
73237
73243
73259
73277
73291
73303
73309
73327
73331
73351
73361
73363
73369
73379
73387
73417
73421
73433
73453
73459
73471
73477
73483
73517
73523
73529
73547
73553
73561
73571
73583
73589
73597
73607
73609
73613
73637
73643
73651
73673
73679
73681
73693
73699
73709
73721
73727
73751
73757
73771
73783
73819
73823
73847
73849
73859
73867
73877
73883
73897
73907
73939
73943
73951
73961
73973
73999
74017
74021
74027
74047
74051
74071
74077
74093
74099
74101
74131
74143
74149
74159
74161
74167
74177
74189
74197
74201
74203
74209
74219
74231
74257
74279
74287
74293
74297
74311
74317
74323
74353
74357
74363
74377
74381
74383
74411
74413
74419
74441
74449
74453
74471
74489
74507
74509
74521
74527
74531
74551
74561
74567
74573
74587
74597
74609
74611
74623
74653
74687
74699
74707
74713
74717
74719
74729
74731
74747
74759
74761
74771
74779
74797
74821
74827
74831
74843
74857
74861
74869
74873
74887
74891
74897
74903
74923
74929
74933
74941
74959
75011
75013
75017
75029
75037
75041
75079
75083
75109
75133
75149
75161
75167
75169
75181
75193
75209
75211
75217
75223
75227
75239
75253
75269
75277
75289
75307
75323
75329
75337
75347
75353
75367
75377
75389
75391
75401
75403
75407
75431
75437
75479
75503
75511
75521
75527
75533
75539
75541
75553
75557
75571
75577
75583
75611
75617
75619
75629
75641
75653
75659
75679
75683
75689
75703
75707
75709
75721
75731
75743
75767
75773
75781
75787
75793
75797
75821
75833
75853
75869
75883
75913
75931
75937
75941
75967
75979
75983
75989
75991
75997
76001
76003
76031
76039
76079
76081
76091
76099
76103
76123
76129
76147
76157
76159
76163
76207
76213
76231
76243
76249
76253
76259
76261
76283
76289
76303
76333
76343
76367
76369
76379
76387
76403
76421
76423
76441
76463
76471
76481
76487
76493
76507
76511
76519
76537
76541
76543
76561
76579
76597
76603
76607
76631
76649
76651
76667
76673
76679
76697
76717
76733
76753
76757
76771
76777
76781
76801
76819
76829
76831
76837
76847
76871
76873
76883
76907
76913
76919
76943
76949
76961
76963
76991
77003
77017
77023
77029
77041
77047
77069
77081
77093
77101
77137
77141
77153
77167
77171
77191
77201
77213
77237
77239
77243
77249
77261
77263
77267
77269
77279
77291
77317
77323
77339
77347
77351
77359
77369
77377
77383
77417
77419
77431
77447
77471
77477
77479
77489
77491
77509
77513
77521
77527
77543
77549
77551
77557
77563
77569
77573
77587
77591
77611
77617
77621
77641
77647
77659
77681
77687
77689
77699
77711
77713
77719
77723
77731
77743
77747
77761
77773
77783
77797
77801
77813
77839
77849
77863
77867
77893
77899
77929
77933
77951
77969
77977
77983
77999
78007
78017
78031
78041
78049
78059
78079
78101
78121
78137
78139
78157
78163
78167
78173
78179
78191
78193
78203
78229
78233
78241
78259
78277
78283
78301
78307
78311
78317
78341
78347
78367
78401
78427
78437
78439
78467
78479
78487
78497
78509
78511
78517
78539
78541
78553
78569
78571
78577
78583
78593
78607
78623
78643
78649
78653
78691
78697
78707
78713
78721
78737
78779
78781
78787
78791
78797
78803
78809
78823
78839
78853
78857
78877
78887
78889
78893
78901
78919
78929
78941
78977
78979
78989
79031
79039
79043
79063
79087
79103
79111
79133
79139
79147
79151
79153
79159
79181
79187
79193
79201
79229
79231
79241
79259
79273
79279
79283
79301
79309
79319
79333
79337
79349
79357
79367
79379
79393
79397
79399
79411
79423
79427
79433
79451
79481
79493
79531
79537
79549
79559
79561
79579
79589
79601
79609
79613
79621
79627
79631
79633
79657
79669
79687
79691
79693
79697
79699
79757
79769
79777
79801
79811
79813
79817
79823
79829
79841
79843
79847
79861
79867
79873
79889
79901
79903
79907
79939
79943
79967
79973
79979
79987
79997
79999
80021
80039
80051
80071
80077
80107
80111
80141
80147
80149
80153
80167
80173
80177
80191
80207
80209
80221
80231
80233
80239
80251
80263
80273
80279
80287
80309
80317
80329
80341
80347
80363
80369
80387
80407
80429
80447
80449
80471
80473
80489
80491
80513
80527
80537
80557
80567
80599
80603
80611
80621
80627
80629
80651
80657
80669
80671
80677
80681
80683
80687
80701
80713
80737
80747
80749
80761
80777
80779
80783
80789
80803
80809
80819
80831
80833
80849
80863
80897
80909
80911
80917
80923
80929
80933
80953
80963
80989
81001
81013
81017
81019
81023
81031
81041
81043
81047
81049
81071
81077
81083
81097
81101
81119
81131
81157
81163
81173
81181
81197
81199
81203
81223
81233
81239
81281
81283
81293
81299
81307
81331
81343
81349
81353
81359
81371
81373
81401
81409
81421
81439
81457
81463
81509
81517
81527
81533
81547
81551
81553
81559
81563
81569
81611
81619
81629
81637
81647
81649
81667
81671
81677
81689
81701
81703
81707
81727
81737
81749
81761
81769
81773
81799
81817
81839
81847
81853
81869
81883
81899
81901
81919
81929
81931
81937
81943
81953
81967
81971
81973
82003
82007
82009
82013
82021
82031
82037
82039
82051
82067
82073
82129
82139
82141
82153
82163
82171
82183
82189
82193
82207
82217
82219
82223
82231
82237
82241
82261
82267
82279
82301
82307
82339
82349
82351
82361
82373
82387
82393
82421
82457
82463
82469
82471
82483
82487
82493
82499
82507
82529
82531
82549
82559
82561
82567
82571
82591
82601
82609
82613
82619
82633
82651
82657
82699
82721
82723
82727
82729
82757
82759
82763
82781
82787
82793
82799
82811
82813
82837
82847
82883
82889
82891
82903
82913
82939
82963
82981
82997
83003
83009
83023
83047
83059
83063
83071
83077
83089
83093
83101
83117
83137
83177
83203
83207
83219
83221
83227
83231
83233
83243
83257
83267
83269
83273
83299
83311
83339
83341
83357
83383
83389
83399
83401
83407
83417
83423
83431
83437
83443
83449
83459
83471
83477
83497
83537
83557
83561
83563
83579
83591
83597
83609
83617
83621
83639
83641
83653
83663
83689
83701
83717
83719
83737
83761
83773
83777
83791
83813
83833
83843
83857
83869
83873
83891
83903
83911
83921
83933
83939
83969
83983
83987
84011
84017
84047
84053
84059
84061
84067
84089
84121
84127
84131
84137
84143
84163
84179
84181
84191
84199
84211
84221
84223
84229
84239
84247
84263
84299
84307
84313
84317
84319
84347
84349
84377
84389
84391
84401
84407
84421
84431
84437
84443
84449
84457
84463
84467
84481
84499
84503
84509
84521
84523
84533
84551
84559
84589
84629
84631
84649
84653
84659
84673
84691
84697
84701
84713
84719
84731
84737
84751
84761
84787
84793
84809
84811
84827
84857
84859
84869
84871
84913
84919
84947
84961
84967
84977
84979
84991
85009
85021
85027
85037
85049
85061
85081
85087
85091
85093
85103
85109
85121
85133
85147
85159
85193
85199
85201
85213
85223
85229
85237
85243
85247
85259
85297
85303
85313
85331
85333
85361
85363
85369
85381
85411
85427
85429
85439
85447
85451
85453
85469
85487
85513
85517
85523
85531
85549
85571
85577
85597
85601
85607
85619
85621
85627
85639
85643
85661
85667
85669
85691
85703
85711
85717
85733
85751
85781
85793
85817
85819
85829
85831
85837
85843
85847
85853
85889
85903
85909
85931
85933
85991
85999
86011
86017
86027
86029
86069
86077
86083
86111
86113
86117
86131
86137
86143
86161
86171
86179
86183
86197
86201
86209
86239
86243
86249
86257
86263
86269
86287
86291
86293
86297
86311
86323
86341
86351
86353
86357
86369
86371
86381
86389
86399
86413
86423
86441
86453
86461
86467
86477
86491
86501
86509
86531
86533
86539
86561
86573
86579
86587
86599
86627
86629
86677
86689
86693
86711
86719
86729
86743
86753
86767
86771
86783
86813
86837
86843
86851
86857
86861
86869
86923
86927
86929
86939
86951
86959
86969
86981
86993
87011
87013
87037
87041
87049
87071
87083
87103
87107
87119
87121
87133
87149
87151
87179
87181
87187
87211
87221
87223
87251
87253
87257
87277
87281
87293
87299
87313
87317
87323
87337
87359
87383
87403
87407
87421
87427
87433
87443
87473
87481
87491
87509
87511
87517
87523
87539
87541
87547
87553
87557
87559
87583
87587
87589
87613
87623
87629
87631
87641
87643
87649
87671
87679
87683
87691
87697
87701
87719
87721
87739
87743
87751
87767
87793
87797
87803
87811
87833
87853
87869
87877
87881
87887
87911
87917
87931
87943
87959
87961
87973
87977
87991
88001
88003
88007
88019
88037
88069
88079
88093
88117
88129
88169
88177
88211
88223
88237
88241
88259
88261
88289
88301
88321
88327
88337
88339
88379
88397
88411
88423
88427
88463
88469
88471
88493
88499
88513
88523
88547
88589
88591
88607
88609
88643
88651
88657
88661
88663
88667
88681
88721
88729
88741
88747
88771
88789
88793
88799
88801
88807
88811
88813
88817
88819
88843
88853
88861
88867
88873
88883
88897
88903
88919
88937
88951
88969
88993
88997
89003
89009
89017
89021
89041
89051
89057
89069
89071
89083
89087
89101
89107
89113
89119
89123
89137
89153
89189
89203
89209
89213
89227
89231
89237
89261
89269
89273
89293
89303
89317
89329
89363
89371
89381
89387
89393
89399
89413
89417
89431
89443
89449
89459
89477
89491
89501
89513
89519
89521
89527
89533
89561
89563
89567
89591
89597
89599
89603
89611
89627
89633
89653
89657
89659
89669
89671
89681
89689
89753
89759
89767
89779
89783
89797
89809
89819
89821
89833
89839
89849
89867
89891
89897
89899
89909
89917
89923
89939
89959
89963
89977
89983
89989
90001
90007
90011
90017
90019
90023
90031
90053
90059
90067
90071
90073
90089
90107
90121
90127
90149
90163
90173
90187
90191
90197
90199
90203
90217
90227
90239
90247
90263
90271
90281
90289
90313
90353
90359
90371
90373
90379
90397
90401
90403
90407
90437
90439
90469
90473
90481
90499
90511
90523
90527
90529
90533
90547
90583
90599
90617
90619
90631
90641
90647
90659
90677
90679
90697
90703
90709
90731
90749
90787
90793
90803
90821
90823
90833
90841
90847
90863
90887
90901
90907
90911
90917
90931
90947
90971
90977
90989
90997
91009
91019
91033
91079
91081
91097
91099
91121
91127
91129
91139
91141
91151
91153
91159
91163
91183
91193
91199
91229
91237
91243
91249
91253
91283
91291
91297
91303
91309
91331
91367
91369
91373
91381
91387
91393
91397
91411
91423
91433
91453
91457
91459
91463
91493
91499
91513
91529
91541
91571
91573
91577
91583
91591
91621
91631
91639
91673
91691
91703
91711
91733
91753
91757
91771
91781
91801
91807
91811
91813
91823
91837
91841
91867
91873
91909
91921
91939
91943
91951
91957
91961
91967
91969
91997
92003
92009
92033
92041
92051
92077
92083
92107
92111
92119
92143
92153
92173
92177
92179
92189
92203
92219
92221
92227
92233
92237
92243
92251
92269
92297
92311
92317
92333
92347
92353
92357
92363
92369
92377
92381
92383
92387
92399
92401
92413
92419
92431
92459
92461
92467
92479
92489
92503
92507
92551
92557
92567
92569
92581
92593
92623
92627
92639
92641
92647
92657
92669
92671
92681
92683
92693
92699
92707
92717
92723
92737
92753
92761
92767
92779
92789
92791
92801
92809
92821
92831
92849
92857
92861
92863
92867
92893
92899
92921
92927
92941
92951
92957
92959
92987
92993
93001
93047
93053
93059
93077
93083
93089
93097
93103
93113
93131
93133
93139
93151
93169
93179
93187
93199
93229
93239
93241
93251
93253
93257
93263
93281
93283
93287
93307
93319
93323
93329
93337
93371
93377
93383
93407
93419
93427
93463
93479
93481
93487
93491
93493
93497
93503
93523
93529
93553
93557
93559
93563
93581
93601
93607
93629
93637
93683
93701
93703
93719
93739
93761
93763
93787
93809
93811
93827
93851
93871
93887
93889
93893
93901
93911
93913
93923
93937
93941
93949
93967
93971
93979
93983
93997
94007
94009
94033
94049
94057
94063
94079
94099
94109
94111
94117
94121
94151
94153
94169
94201
94207
94219
94229
94253
94261
94273
94291
94307
94309
94321
94327
94331
94343
94349
94351
94379
94397
94399
94421
94427
94433
94439
94441
94447
94463
94477
94483
94513
94529
94531
94541
94543
94547
94559
94561
94573
94583
94597
94603
94613
94621
94649
94651
94687
94693
94709
94723
94727
94747
94771
94777
94781
94789
94793
94811
94819
94823
94837
94841
94847
94849
94873
94889
94903
94907
94933
94949
94951
94961
94993
94999
95003
95009
95021
95027
95063
95071
95083
95087
95089
95093
95101
95107
95111
95131
95143
95153
95177
95189
95191
95203
95213
95219
95231
95233
95239
95257
95261
95267
95273
95279
95287
95311
95317
95327
95339
95369
95383
95393
95401
95413
95419
95429
95441
95443
95461
95467
95471
95479
95483
95507
95527
95531
95539
95549
95561
95569
95581
95597
95603
95617
95621
95629
95633
95651
95701
95707
95713
95717
95723
95731
95737
95747
95773
95783
95789
95791
95801
95803
95813
95819
95857
95869
95873
95881
95891
95911
95917
95923
95929
95947
95957
95959
95971
95987
95989
96001
96013
96017
96043
96053
96059
96079
96097
96137
96149
96157
96167
96179
96181
96199
96211
96221
96223
96233
96259
96263
96269
96281
96289
96293
96323
96329
96331
96337
96353
96377
96401
96419
96431
96443
96451
96457
96461
96469
96479
96487
96493
96497
96517
96527
96553
96557
96581
96587
96589
96601
96643
96661
96667
96671
96697
96703
96731
96737
96739
96749
96757
96763
96769
96779
96787
96797
96799
96821
96823
96827
96847
96851
96857
96893
96907
96911
96931
96953
96959
96973
96979
96989
96997
97001
97003
97007
97021
97039
97073
97081
97103
97117
97127
97151
97157
97159
97169
97171
97177
97187
97213
97231
97241
97259
97283
97301
97303
97327
97367
97369
97373
97379
97381
97387
97397
97423
97429
97441
97453
97459
97463
97499
97501
97511
97523
97547
97549
97553
97561
97571
97577
97579
97583
97607
97609
97613
97649
97651
97673
97687
97711
97729
97771
97777
97787
97789
97813
97829
97841
97843
97847
97849
97859
97861
97871
97879
97883
97919
97927
97931
97943
97961
97967
97973
97987
98009
98011
98017
98041
98047
98057
98081
98101
98123
98129
98143
98179
98207
98213
98221
98227
98251
98257
98269
98297
98299
98317
98321
98323
98327
98347
98369
98377
98387
98389
98407
98411
98419
98429
98443
98453
98459
98467
98473
98479
98491
98507
98519
98533
98543
98561
98563
98573
98597
98621
98627
98639
98641
98663
98669
98689
98711
98713
98717
98729
98731
98737
98773
98779
98801
98807
98809
98837
98849
98867
98869
98873
98887
98893
98897
98899
98909
98911
98927
98929
98939
98947
98953
98963
98981
98993
98999
99013
99017
99023
99041
99053
99079
99083
99089
99103
99109
99119
99131
99133
99137
99139
99149
99173
99181
99191
99223
99233
99241
99251
99257
99259
99277
99289
99317
99347
99349
99367
99371
99377
99391
99397
99401
99409
99431
99439
99469
99487
99497
99523
99527
99529
99551
99559
99563
99571
99577
99581
99607
99611
99623
99643
99661
99667
99679
99689
99707
99709
99713
99719
99721
99733
99761
99767
99787
99793
99809
99817
99823
99829
99833
99839
99859
99871
99877
99881
99901
99907
99923
99929
99961
99971
99989
99991
100003
100019
100043
100049
100057
100069
100103
100109
100129
100151
100153
100169
100183
100189
100193
100207
100213
100237
100267
100271
100279
100291
100297
100313
100333
100343
100357
100361
100363
100379
100391
100393
100403
100411
100417
100447
100459
100469
100483
100493
100501
100511
100517
100519
100523
100537
100547
100549
100559
100591
100609
100613
100621
100649
100669
100673
100693
100699
100703
100733
100741
100747
100769
100787
100799
100801
100811
100823
100829
100847
100853
100907
100913
100927
100931
100937
100943
100957
100981
100987
100999
101009
101021
101027
101051
101063
101081
101089
101107
101111
101113
101117
101119
101141
101149
101159
101161
101173
101183
101197
101203
101207
101209
101221
101267
101273
101279
101281
101287
101293
101323
101333
101341
101347
101359
101363
101377
101383
101399
101411
101419
101429
101449
101467
101477
101483
101489
101501
101503
101513
101527
101531
101533
101537
101561
101573
101581
101599
101603
101611
101627
101641
101653
101663
101681
101693
101701
101719
101723
101737
101741
101747
101749
101771
101789
101797
101807
101833
101837
101839
101863
101869
101873
101879
101891
101917
101921
101929
101939
101957
101963
101977
101987
101999
102001
102013
102019
102023
102031
102043
102059
102061
102071
102077
102079
102101
102103
102107
102121
102139
102149
102161
102181
102191
102197
102199
102203
102217
102229
102233
102241
102251
102253
102259
102293
102299
102301
102317
102329
102337
102359
102367
102397
102407
102409
102433
102437
102451
102461
102481
102497
102499
102503
102523
102533
102539
102547
102551
102559
102563
102587
102593
102607
102611
102643
102647
102653
102667
102673
102677
102679
102701
102761
102763
102769
102793
102797
102811
102829
102841
102859
102871
102877
102881
102911
102913
102929
102931
102953
102967
102983
103001
103007
103043
103049
103067
103069
103079
103087
103091
103093
103099
103123
103141
103171
103177
103183
103217
103231
103237
103289
103291
103307
103319
103333
103349
103357
103387
103391
103393
103399
103409
103421
103423
103451
103457
103471
103483
103511
103529
103549
103553
103561
103567
103573
103577
103583
103591
103613
103619
103643
103651
103657
103669
103681
103687
103699
103703
103723
103769
103787
103801
103811
103813
103837
103841
103843
103867
103889
103903
103913
103919
103951
103963
103967
103969
103979
103981
103991
103993
103997
104003
104009
104021
104033
104047
104053
104059
104087
104089
104107
104113
104119
104123
104147
104149
104161
104173
104179
104183
104207
104231
104233
104239
104243
104281
104287
104297
104309
104311
104323
104327
104347
104369
104381
104383
104393
104399
104417
104459
104471
104473
104479
104491
104513
104527
104537
104543
104549
104551
104561
104579
104593
104597
104623
104639
104651
104659
104677
104681
104683
104693
104701
104707
104711
104717
104723
104729

</content>

<content full_path="exllamav3/util/hadamard_data/hadamard_428.txt">
+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-
-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+----+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+
+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+-+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+-
-+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++------++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+-+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--
--+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+--++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+
---+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++--++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+---+----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--++
----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+--------+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++
+----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----+---+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++-
++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++--+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++--
+++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----+++-+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++---
++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++--++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----+++++----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----
-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+
+-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-+---++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+-
++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++--++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+--
-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----+++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-+++++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++--++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+---
+-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----+++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-+++++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-+++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----
++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++-++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++-++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-+++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----+
+++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----+++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--+-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-+++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++
++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----+--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--+-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++-
-++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+------+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--
--++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+----+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++--+-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--+
---++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++----++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++
----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+---+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+-+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++----++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-
-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+-+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-+
+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+---+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++
-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--++-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-
+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+--++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+
-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-
+-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----+-+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+-+----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-+
++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+------+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++
-++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++---++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+---+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-
--++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++--+++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--++--+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++--
---++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++-++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++-+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++---
----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++----
+----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----++-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----
++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----++-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+
+++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----++++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-
++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++--++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-+++----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++--++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+
-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-
+-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-+---+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+-+++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-+
++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++--+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+--++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++
+++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-+++-+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+---+-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-+++
++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++--+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-+++++---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+-----+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++
-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-++++++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++----+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----++++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-
+-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-++++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+--+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+-++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+
++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-++-+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+--+-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-++
+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+--+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-++++-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+----+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++
-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-++-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++--+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---++-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-
+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+--+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-++-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+--+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+
-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--++--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+--++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-++--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-
+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++----+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+---+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+
-+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+-+---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-+-+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+-
--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-++++++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+-----+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-+++++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--
+--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-++++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+--+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++-++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+
++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--++-+-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++--+-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--++
+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+--+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--++++-++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++----+--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++
-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--++--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++--++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---++--++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-
+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++----++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+++--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+---++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+
-+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+-+--+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-+-++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+-
--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+----+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++++-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--
+--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++-----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--+-+++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++-+-----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--+
++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+----------+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--+++++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++-------+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++
-++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+--------+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-++++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+----+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-
--++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+------+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++--+++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--++---+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++--
---++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+----+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++---++-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++--+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++---
----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+--+-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++----+-+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--++++-+-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++----
-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-++-+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++------+----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--++++++-+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----
+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++--+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----++----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++--+++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+
-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---+++++++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-----+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-++++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-
+-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---+++++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+---+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+-++-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+
++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---+++-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-++--+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+--+-+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-++
+++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---+-+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++
++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---+++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-
-++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+--++-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+
--++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+-+-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-++
---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+-++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++
+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-
-+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+-+-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-+
--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++
+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-
-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++---++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+
+-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--++--++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+-
++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+-++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+--
+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---
-+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++-+--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---+
--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++
+--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-+-+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++-
++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-+---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--
-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++---+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+
+-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-++--+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+-
++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+-+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+--
+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---
-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++----++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+
+-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--+++---++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+-
++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++--++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+--
+++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--+-++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+---
++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----
-++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+-+-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----+
--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++
+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-
-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+
+-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-++++----+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-
++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++---+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+--
+++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-++--+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+---
++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+-+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+----
+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----
-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+--++---++++-++++--++--+--+-+--+++++-++-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+--+----++++-++-++++-----+-+-++----+++-+----+---+-+-++---+-++--+++++-+----+++-++-+---++--+---+----++-+-----+-+--+++----+----++--++-++-+-++-----+-+-++++-+++-+-+--+++-+--++-----+-+++-+++-++-+---++--+---+----++-+-----+
+--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+--+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-++-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--
--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----++++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+------+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+
-+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++-+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-
+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++---+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++---+++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-++---+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+--
++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+--+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++--++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++--+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+---
+----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--++---+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++-+-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-++++-+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+----
----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-++++++-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----
---+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++-+----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++--++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+
--+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++--++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-
-+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++---+++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-+-++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+-+----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-+
+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+------+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++
----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-+++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--+---+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++-
---++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+-+-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-+++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++--+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++--
--++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+--++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-+-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--+++-+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++---
-++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--+++++---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----
++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----+-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++----+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+
+--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----+++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+--+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+-
--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----+++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-++-+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+--
-++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++-++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+------++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-++++--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---
++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++---++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+----++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++---++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+
+-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--+--++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+---+--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-+-++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+-
-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++---++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+----+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--
++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++-+---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--+
+-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-++++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++-----+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++
-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-+++-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+--+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++-
+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++--+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--++-+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++--
-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-++-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++--+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--++++-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---
++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+--+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----+++--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++--++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+
+-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-
-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++-----+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+-+-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-+
----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++--++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++---+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+---+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++
---+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++-----+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--++++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-
--+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++------++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+-++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+
-+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++--------++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++-+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+--+----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-++
+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+-------+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++
--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++-
-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+-+++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+--+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++--
+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++--+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++---
----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-+++++-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----
---+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+-+-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-+++-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+
--+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+--++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-+++++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-
-+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+---+++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+
+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-+++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-++
---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++
--+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+-+-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-++++
-+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+--++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++
+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++-
-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--
+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--+
-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++
++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-
+---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-+-+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++----++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+
---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+-
--+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++-+--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-++-++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+--
-+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++--++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---
+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---+
-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++
++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-
+--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-+-+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+
--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----+++---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-
-+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++-+--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+
+++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-------+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+-
++++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+-++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+--
+++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--++--++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+---+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---
++-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++---++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+------+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+
+-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--++++----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+----+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+-
-+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+--+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+--
+---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++--+-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+---
---+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---+++++-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----
--+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+-+-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---+++-+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+
-+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+--++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---+++-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-
+---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---+-+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+
---+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---+++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-++
--+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+--++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+--++++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+----++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++
-+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+----++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+-+++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-++---++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++-
+--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+------++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+++--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++--++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++--
--+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---++---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+--+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-++++-++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++---
-+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+---+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++++-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----
+-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+----+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+-+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++-+-+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----+
-+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--++--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++---+-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++
+++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+--+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--++-+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-
++--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+--+-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+
+--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-++++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+--+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-++-----++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-
--++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-++++++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++---+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+------++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+
-++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++-+--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+------++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-
++-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++----+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+---++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+--
+-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-+-++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+---++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+---
-+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+---++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+----
+++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++--++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-++++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--+++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----
++-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-++--++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--+++++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----+
+-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-++++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+-++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++-++-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++
-++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-++++++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--+-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----+++
++++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++--+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--+++++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--+-++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++
+++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--+++++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-
++--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--+++-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-+
+--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--+-++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++
--+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-
-+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+-+++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-+
+-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++----++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++
-+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--++--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-+++
+++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+--+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++---++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++
++++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-++++--++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++-
+++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++-++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++--
++-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-++++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-++++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++----+--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++---
+-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-++++++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-++-+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----
-++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-++++++++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+--+-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+
++--+++----+----++--++-++-+-++-----+--+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++--+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-+-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++----+-+++++--++-+---++-+-+---+----+-+++----++-+-+-----++++-++-++++----+-
+-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+--+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++
-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----++++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-
++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+
+++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-+---+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+---+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++---+++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-++
++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++--+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++--++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++
+----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-+++-+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-------+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++-+-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-++++
----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-+++++-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+---------+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++
---+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++--++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-
--+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++--++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+
-+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++---+----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-+-++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+-
+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++--------+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--
--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+---+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++--++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-+++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--+
-+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+---+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++--+-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-+++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++
+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+---+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++---++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-+-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--+++
----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--++---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++-----++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++
---+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-
--+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+-++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+
-+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+--+++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-++
+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+------++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++
++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+-++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+----++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-
+++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----++-++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+---++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+---+--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-+
++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+----+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++
+-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--+----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++-
-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-++++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--
+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+
-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--++
+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++--+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++
--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----+++--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-
-++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-+-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+
++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--+-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-+++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++-----+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+-
+++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--+-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++---+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--
++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--+++++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++---++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++-----+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+
+---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--+++++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+---++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+-
---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--+++++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++-+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+--
--+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++--+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---
-+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++--+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+
+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+--+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+-
-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-++++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++--+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+--
++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+--+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-+++++-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---
+++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-+++-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+
++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-+++++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-
+-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-+++----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-+++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+
-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++---++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+--++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-+++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-++
+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++---++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+----++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++
++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+-++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+----+-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-++++
+-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-++++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++
-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-++++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++-
+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++---+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--
-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--+
+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++---+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++
--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--++-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-
-+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++----++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+
+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+-
++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-++-++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+--
+-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---
-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---+
+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++---+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++
--++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--++-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-
-++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+
++-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+---+++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----+++---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-
+-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+
-----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++++-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-------+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+-
----+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-+-++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-++-++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+--
---+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++---++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++--++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+---+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---
--+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++---++++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+------+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+
-+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++----+++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-+----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+----+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+-
+-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+--+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+--
-++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----++-+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++--+--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+---
++++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+--+-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---+++++--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----
+++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---+++--+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+
++---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++-+-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++-+-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+-
+---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++-----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--
---+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+--------++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+
--+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+--++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+------++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-
-+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+----++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+----++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+--
+--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++-----++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+------++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+--++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+---
--+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+-++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+----+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+---++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+----
-+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+-++-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+------+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+-++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----
+-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-++-+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----+
-+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----+++--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++--+-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++
+++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++--+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--++++-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-
++--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+-++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--++-++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+
+--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-++++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-
--++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-++++-++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-+
-++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++
++-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++---+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-
+-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-+
-+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++---++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++
+++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++--++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++-
++-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--+-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-++++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--
+-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-++----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--+
-++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--+++++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-----+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++
++++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++----+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++--+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++---+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++-
+++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-+--+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++--+-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--+++--+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++--
++--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++-+----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++---++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++-+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++---
+--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----
--+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+------+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+
-+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+----+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+-
+-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++----+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+--
-+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+---+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++--+++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+---
+++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+-----+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-++++++++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----
++++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-++++++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+
+++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-++++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----++
++-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++--+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----++++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-++--++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++
+-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-++++-+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++-++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+-++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++-
-++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-++++++-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--
++-++++----+--+----+++++-+-+--++++---+-++++-+++-+-+--+++-+--++-----+-++++---+--+-+++--++-+++-++++--+-+++++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++-+-++-++--++----+----+++--+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-++-+++++-+--++++-+++-++--+++-+--+---+---+-+++++--++-+---++-+-+---+----+--+-----++-+-++-++--++----+----+++--+
-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-+++-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-
++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++------+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+
+---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----
---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-++---+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+---+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++---
--++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++-++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++--+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++--
-++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++--+-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-++++-+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-------+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++-
++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++----+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-++++++-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+---------+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++
+++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++--++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-+++
++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++
+-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---+++-++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+-+----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-+
-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+------+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-
++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-+++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--+---+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++--++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++
+++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-+++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++--+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++--+-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-+
++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-+++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--+++-+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++---++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-
+--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-+++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--+++++---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++-----++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++
--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-+++++++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++----+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----+++
-++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++-++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+--+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+-++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++
++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--+-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-++-+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+--+++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----+
+--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--+-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-++++--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----
--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++---++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+-++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+----
-+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++-+--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-+-++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+---++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+---
+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++----+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+--
--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+-+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++-+---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--+----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-
-+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+-+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++-----+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+
+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+--+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-
-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--++-+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+
+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+--+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--++++-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-
--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-++--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++--++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++
-+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+---+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-+-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----+
+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+---+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+-+-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-+++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----
++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--++---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+---+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++---
+++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--++---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--++++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++---++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++--
++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++--++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+-++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+---++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++-
+-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--++++-++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+--+-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++
-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+----+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+++
+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++
-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+
+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-++++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-
----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++
---+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-+++
--+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+-------++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++
-+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+-------++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-+++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-+
+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+-------++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+--++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-
---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+--++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+----++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++
--+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+--++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+----+-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-++
-+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+--++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+
+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-
-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+
+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-
-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-++-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++---+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+
++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+--+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--++-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--
+---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++-
---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++
--+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++-+--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-++
-+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++----+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+
+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++----+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-
-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---++-+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++---+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+
++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+--+-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--++-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--
+--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++-
--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++
-+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++-+++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----+
+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----
++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-++-++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+----
+++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--++-++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++--++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+---
++-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+--
+-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-+----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+-
-+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++++-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-+
+---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++-
---+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++++
--+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+-+---++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---+++
-+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+-----++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---++
+---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+-----++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+--+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-+++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---+
---+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+-++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+--+++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+---
--+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+-++++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+-++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+--
-+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+--+++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-++--++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+-
+--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--+
--+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---++--+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+--
-+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+---+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++-+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+-
+-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+---+-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++---+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-+
-+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--++-+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++-
+++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+--+-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+++
++--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-++-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--++
+--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-++-+++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--+
--++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-++++++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+--+--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+-+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++--
-++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++-++++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-++--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+---+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++-
++-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--+++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-++--++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-++
+-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--+++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++-++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+-+--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-+
-+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--+++----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-++++++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++-
+++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-----+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-++++++--++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--+-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+++
++-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+---+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++---++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-++
+-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-++--+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++---++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++-++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-+
-++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++---++++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++-
++++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--+-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++++
+++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-+--+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+++-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--+++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--+++
++--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++-+----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+-+-++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--+++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--++
+--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+---++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--+++++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--+
--+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+++++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+--
-+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+-+++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-+-++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+-
+-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++----++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+--++---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-+
-+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+---+---++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++-
+++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+-------++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++++
++++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----+--++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++-+-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-++++
+++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++-++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++--++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+++
++-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----+++++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---+++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-++
+-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-++++-++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----+++++--+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-+
-+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++++-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++---+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---+++++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+-
+-++---++++-++++--++--+--+-+--+++++-+-+----+---+-+-++---+-++--+++++-+---+---+--+-+++--++-+++-++++--+-+++++-+-+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-+---++++--+-+-+++++----+--+----++++-+-+-----+-++----+---+--++---+-++-+++-+++-+-----++--+-+++--+-+-+++-++++-++-+++++--+-+--+--++--++++-++++---++--+++++-+--++++-+++-++--+++-+--+---++++-+-----++--+-+++--+-+-+++-++++-++++----++-+-+-----++++-++-++++----+-+

</content>

<content full_path="exllamav3/util/hadamard_data/hadamard_188.txt">
+-+----+--+----++---++++---+-+----++++++--+---++---+--++++++----+-+---+---+++--++++-++-++++-+-+--+--+-+++-----+---+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-
++-+----+--+----++---++++---+-+----++++++--+------+--++++++----+-+---+---+++--++++-++-++++-+-+--+--+-+++-----+---+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+
-++-+----+--+----++---++++---+-+----++++++--+----+--++++++----+-+---+---+++--++++-++-++++-+-+--+--+-+++-----+---+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+-
--++-+----+--+----++---++++---+-+----++++++--+--+--++++++----+-+---+---+++--++++-++-++++-+-+--+--+-+++-----+---+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--
---++-+----+--+----++---++++---+-+----++++++--++--++++++----+-+---+---+++--++++-++-++++-+-+-----+-+++-----+---+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+
+---++-+----+--+----++---++++---+-+----++++++----++++++----+-+---+---+++--++++-++-++++-+-+---+-+-+++-----+---+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+-
-+---++-+----+--+----++---++++---+-+----++++++--++++++----+-+---+---+++--++++-++-++++-+-+---+-+-+++-----+---+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--
--+---++-+----+--+----++---++++---+-+----++++++++++++----+-+---+---+++--++++-++-++++-+-+---+---+++-----+---+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--+
+--+---++-+----+--+----++---++++---+-+----++++++++++----+-+---+---+++--++++-++-++++-+-+---+--++++-----+---+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--+-
++--+---++-+----+--+----++---++++---+-+----++++++++----+-+---+---+++--++++-++-++++-+-+---+--++++-----+---+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--+-+
+++--+---++-+----+--+----++---++++---+-+----++++++----+-+---+---+++--++++-++-++++-+-+---+--++++-----+---+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--+-++
++++--+---++-+----+--+----++---++++---+-+----++++----+-+---+---+++--++++-++-++++-+-+---+--++++-----+---+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--+-+++
+++++--+---++-+----+--+----++---++++---+-+----++----+-+---+---+++--++++-++-++++-+-+---+--+++++----+---+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--+-+++-
++++++--+---++-+----+--+----++---++++---+-+--------+-+---+---+++--++++-++-++++-+-+---+--++++++---+---+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--+-+++--
-++++++--+---++-+----+--+----++---++++---+-+------+-+---+---+++--++++-++-++++-+-+---+--++++++---+---+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--+-+++---
--++++++--+---++-+----+--+----++---++++---+-+----+-+---+---+++--++++-++-++++-+-+---+--++++++---+---+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--+-+++----
---++++++--+---++-+----+--+----++---++++---+-+--+-+---+---+++--++++-++-++++-+-+---+--++++++---+---+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--+-+++-----
----++++++--+---++-+----+--+----++---++++---+-++-+---+---+++--++++-++-++++-+-+---+--++++++-------+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--+-+++-----+
+----++++++--+---++-+----+--+----++---++++---+--+---+---+++--++++-++-++++-+-+---+--++++++----+--+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--+-+++-----+-
-+----++++++--+---++-+----+--+----++---++++---++---+---+++--++++-++-++++-+-+---+--++++++----+--+---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--+-+++-----+--
+-+----++++++--+---++-+----+--+----++---++++------+---+++--++++-++-++++-+-+---+--++++++----+-++---++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--+-+++-----+---
-+-+----++++++--+---++-+----+--+----++---++++----+---+++--++++-++-++++-+-+---+--++++++----+-+----++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--+-+++-----+---+
--+-+----++++++--+---++-+----+--+----++---++++--+---+++--++++-++-++++-+-+---+--++++++----+-+----++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--+-+++-----+---+-
---+-+----++++++--+---++-+----+--+----++---+++++---+++--++++-++-++++-+-+---+--++++++----+-+----++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--+-+++-----+---+--
+---+-+----++++++--+---++-+----+--+----++---+++---+++--++++-++-++++-+-+---+--++++++----+-+---+++--+-++-+++-+-+--+---++--+--+-+++-----+---+-----++-+--+---+-+-++-+++-+--+--+-+++-----+---+---
++---+-+----++++++--+---++-+----+--+----++---++--+++--++++-++-++++-+-+---+--++++++----+-+---+-+--+-++-+++-+-+--+---++--+--+-+++-----+---+---+-++-+--+---+-+-++-+++-+--+--+-+++-----+---+----
+++---+-+----++++++--+---++-+----+--+----++---+-+++--++++-++-++++-+-+---+--++++++----+-+---+----+-++-+++-+-+--+---++--+--+-+++-----+---+---++++-+--+---+-+-++-+++-+--+--+-+++-----+---+-----
++++---+-+----++++++--+---++-+----+--+----++---+++--++++-++-++++-+-+---+--++++++----+-+---+----+-++-+++-+-+--+---++--+--+-+++-----+---+---++-+-+--+---+-+-++-+++-+--+--+-+++-----+---+-----+
-++++---+-+----++++++--+---++-+----+--+----++--++--++++-++-++++-+-+---+--++++++----+-+---+---++-++-+++-+-+--+---++--+--+-+++-----+---+---++---+--+---+-+-++-+++-+--+--+-+++-----+---+-----++
--++++---+-+----++++++--+---++-+----+--+----++-+--++++-++-++++-+-+---+--++++++----+-+---+---++-++-+++-+-+--+---++--+--+-+++-----+---+---++--++--+---+-+-++-+++-+--+--+-+++-----+---+-----++-
---++++---+-+----++++++--+---++-+----+--+----++--++++-++-++++-+-+---+--++++++----+-+---+---+++++-+++-+-+--+---++--+--+-+++-----+---+---++--+---+---+-+-++-+++-+--+--+-+++-----+---+-----++-+
+---++++---+-+----++++++--+---++-+----+--+----+-++++-++-++++-+-+---+--++++++----+-+---+---+++-+-+++-+-+--+---++--+--+-+++-----+---+---++--+-+-+---+-+-++-+++-+--+--+-+++-----+---+-----++-+-
++---++++---+-+----++++++--+---++-+----+--+----++++-++-++++-+-+---+--++++++----+-+---+---+++---+++-+-+--+---++--+--+-+++-----+---+---++--+-+++---+-+-++-+++-+--+--+-+++-----+---+-----++-+--
-++---++++---+-+----++++++--+---++-+----+--+---+++-++-++++-+-+---+--++++++----+-+---+---+++--++++-+-+--+---++--+--+-+++-----+---+---++--+-++----+-+-++-+++-+--+--+-+++-----+---+-----++-+--+
--++---++++---+-+----++++++--+---++-+----+--+--++-++-++++-+-+---+--++++++----+-+---+---+++--++++-+-+--+---++--+--+-+++-----+---+---++--+-++-+--+-+-++-+++-+--+--+-+++-----+---+-----++-+--+-
---++---++++---+-+----++++++--+---++-+----+--+-+-++-++++-+-+---+--++++++----+-+---+---+++--++++-+-+--+---++--+--+-+++-----+---+---++--+-++-++-+-+-++-+++-+--+--+-+++-----+---+-----++-+--+--
----++---++++---+-+----++++++--+---++-+----+--+-++-++++-+-+---+--++++++----+-+---+---+++--++++-+-+--+---++--+--+-+++-----+---+---++--+-++-++++-+-++-+++-+--+--+-+++-----+---+-----++-+--+---
+----++---++++---+-+----++++++--+---++-+----+--++-++++-+-+---+--++++++----+-+---+---+++--++++-+-+--+---++--+--+-+++-----+---+---++--+-++-+++--+-++-+++-+--+--+-+++-----+---+-----++-+--+---+
-+----++---++++---+-+----++++++--+---++-+----+-+-++++-+-+---+--++++++----+-+---+---+++--++++-+-+--+---++--+--+-+++-----+---+---++--+-++-+++-++-++-+++-+--+--+-+++-----+---+-----++-+--+---+-
--+----++---++++---+-+----++++++--+---++-+----+-++++-+-+---+--++++++----+-+---+---+++--++++-+++--+---++--+--+-+++-----+---+---++--+-++-+++-+--++-+++-+--+--+-+++-----+---+-----++-+--+---+-+
+--+----++---++++---+-+----++++++--+---++-+----++++-+-+---+--++++++----+-+---+---+++--++++-++---+---++--+--+-+++-----+---+---++--+-++-+++-+-+++-+++-+--+--+-+++-----+---+-----++-+--+---+-+-
-+--+----++---++++---+-+----++++++--+---++-+---+++-+-+---+--++++++----+-+---+---+++--++++-++-+-+---++--+--+-+++-----+---+---++--+-++-+++-+-+-+-+++-+--+--+-+++-----+---+-----++-+--+---+-+-+
--+--+----++---++++---+-+----++++++--+---++-+--++-+-+---+--++++++----+-+---+---+++--++++-++-+++---++--+--+-+++-----+---+---++--+-++-+++-+-+---+++-+--+--+-+++-----+---+-----++-+--+---+-+-++
---+--+----++---++++---+-+----++++++--+---++-+-+-+-+---+--++++++----+-+---+---+++--++++-++-+++---++--+--+-+++-----+---+---++--+-++-+++-+-+--++++-+--+--+-+++-----+---+-----++-+--+---+-+-++-
----+--+----++---++++---+-+----++++++--+---++-+-+-+---+--++++++----+-+---+---+++--++++-++-++++--++--+--+-+++-----+---+---++--+-++-+++-+-+--+-++-+--+--+-+++-----+---+-----++-+--+---+-+-++-+
+----+--+----++---++++---+-+----++++++--+---++-+-+---+--++++++----+-+---+---+++--++++-++-++++--++--+--+-+++-----+---+---++--+-++-+++-+-+--+--+-+--+--+-+++-----+---+-----++-+--+---+-+-++-++
-+----+--+----++---++++---+-+----++++++--+---++-+---+--++++++----+-+---+---+++--++++-++-++++-+++--+--+-+++-----+---+---++--+-++-+++-+-+--+----+--+--+-+++-----+---+-----++-+--+---+-+-++-+++
-+++-++------++++-+-+++-+++---++----+--+----+-++-+----+--+----++---++++---+-+----++++++--+---+---+--+-+-+++-++-+--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++
+++-++------++++-+-+++-+++---++----+--+----+-+-++-+----+--+----++---++++---+-+----++++++--+-----+--+-+-+++-++-+--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++-
++-++------++++-+-+++-+++---++----+--+----+-+-+-++-+----+--+----++---++++---+-+----++++++--+---+--+-+-+++-++-+--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++--
+-++------++++-+-+++-+++---++----+--+----+-+-++--++-+----+--+----++---++++---+-+----++++++--+-+--+-+-+++-++-+--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---
-++------++++-+-+++-+++---++----+--+----+-+-+++---++-+----+--+----++---++++---+-+----++++++--+--+-+-+++-++-+--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+
++------++++-+-+++-+++---++----+--+----+-+-+++-+---++-+----+--+----++---++++---+-+----++++++---+-+-+++-++-+--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+-
+------++++-+-+++-+++---++----+--+----+-+-+++-+-+---++-+----+--+----++---++++---+-+----++++++-+-+-+++-++-+--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+--
------++++-+-+++-+++---++----+--+----+-+-+++-++--+---++-+----+--+----++---++++---+-+----++++++-+-+++-++-+--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+--+
-----++++-+-+++-+++---++----+--+----+-+-+++-++-+--+---++-+----+--+----++---++++---+-+----++++++-+++-++-+--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+--+-
----++++-+-+++-+++---++----+--+----+-+-+++-++--++--+---++-+----+--+----++---++++---+-+----++++-+++-++-+--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+--+-+
---++++-+-+++-+++---++----+--+----+-+-+++-++---+++--+---++-+----+--+----++---++++---+-+----++++++-++-+--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+--+-+-
--++++-+-+++-+++---++----+--+----+-+-+++-++----++++--+---++-+----+--+----++---++++---+-+----++++-++-+--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+--+-+-+
-++++-+-+++-+++---++----+--+----+-+-+++-++-----+++++--+---++-+----+--+----++---++++---+-+----++-++-+--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+--+-+-++
++++-+-+++-+++---++----+--+----+-+-+++-++------++++++--+---++-+----+--+----++---++++---+-+-----++-+--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+--+-+-+++
+++-+-+++-+++---++----+--+----+-+-+++-++------+-++++++--+---++-+----+--+----++---++++---+-+---++-+--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+--+-+-+++-
++-+-+++-+++---++----+--+----+-+-+++-++------++--++++++--+---++-+----+--+----++---++++---+-+--+-+--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+--+-+-+++-+
+-+-+++-+++---++----+--+----+-+-+++-++------+++---++++++--+---++-+----+--+----++---++++---+-+--+--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+--+-+-+++-++
-+-+++-+++---++----+--+----+-+-+++-++------++++----++++++--+---++-+----+--+----++---++++---+-++--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+--+-+-+++-++-
+-+++-+++---++----+--+----+-+-+++-++------++++-+----++++++--+---++-+----+--+----++---++++---+---+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+--+-+-+++-++-+
-+++-+++---++----+--+----+-+-+++-++------++++-+-+----++++++--+---++-+----+--+----++---++++---+-+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+--+-+-+++-++-+-
+++-+++---++----+--+----+-+-+++-++------++++-+-+-+----++++++--+---++-+----+--+----++---++++---+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+--+-+-+++-++-+--
++-+++---++----+--+----+-+-+++-++------++++-+-+-+-+----++++++--+---++-+----+--+----++---++++--++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+--+-+-+++-++-+--+
+-+++---++----+--+----+-+-+++-++------++++-+-++--+-+----++++++--+---++-+----+--+----++---++++-+++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--++---+--+-+-+++-++-+--++
-+++---++----+--+----+-+-+++-++------++++-+-+++---+-+----++++++--+---++-+----+--+----++---++++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--+++--+---+-----+++-+--+--++---+--+-+-+++-++-+--++-
+++---++----+--+----+-+-+++-++------++++-+-+++-+---+-+----++++++--+---++-+----+--+----++---++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++++-+---+-----+++-+--+--++---+--+-+-+++-++-+--++--
++---++----+--+----+-+-+++-++------++++-+-+++-+++---+-+----++++++--+---++-+----+--+----++---++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++++++---+-----+++-+--+--++---+--+-+-+++-++-+--++---
+---++----+--+----+-+-+++-++------++++-+-+++-+++++---+-+----++++++--+---++-+----+--+----++---++++-+++++---+-++-++-+---+--+-+-+++-++-+--+++++----+-----+++-+--+--++---+--+-+-+++-++-+--++---+
---++----+--+----+-+-+++-++------++++-+-+++-+++++++---+-+----++++++--+---++-+----+--+----++---++-+++++---+-++-++-+---+--+-+-+++-++-+--+++++-+--+-----+++-+--+--++---+--+-+-+++-++-+--++---+-
--++----+--+----+-+-+++-++------++++-+-+++-+++--++++---+-+----++++++--+---++-+----+--+----++--+-+++++---+-++-++-+---+--+-+-+++-++-+--+++++-++-+-----+++-+--+--++---+--+-+-+++-++-+--++---+--
-++----+--+----+-+-+++-++------++++-+-+++-+++----++++---+-+----++++++--+---++-+----+--+----++--+++++---+-++-++-+---+--+-+-+++-++-+--+++++-++++-----+++-+--+--++---+--+-+-+++-++-+--++---+---
++----+--+----+-+-+++-++------++++-+-+++-+++------++++---+-+----++++++--+---++-+----+--+----+++++++---+-++-++-+---+--+-+-+++-++-+--+++++-+++------+++-+--+--++---+--+-+-+++-++-+--++---+---+
+----+--+----+-+-+++-++------++++-+-+++-+++---++---++++---+-+----++++++--+---++-+----+--+----+++++---+-++-++-+---+--+-+-+++-++-+--+++++-+++-+----+++-+--+--++---+--+-+-+++-++-+--++---+---+-
----+--+----+-+-+++-++------++++-+-+++-+++---++++---++++---+-+----++++++--+---++-+----+--+----+++---+-++-++-+---+--+-+-+++-++-+--+++++-+++-++---+++-+--+--++---+--+-+-+++-++-+--++---+---+--
---+--+----+-+-+++-++------++++-+-+++-+++---++--++---++++---+-+----++++++--+---++-+----+--+---++---+-++-++-+---+--+-+-+++-++-+--+++++-+++-+++--+++-+--+--++---+--+-+-+++-++-+--++---+---+---
--+--+----+-+-+++-++------++++-+-+++-+++---++----++---++++---+-+----++++++--+---++-+----+--+--+---+-++-++-+---+--+-+-+++-++-+--+++++-+++-++++-+++-+--+--++---+--+-+-+++-++-+--++---+---+----
-+--+----+-+-+++-++------++++-+-+++-+++---++------++---++++---+-+----++++++--+---++-+----+--+----+-++-++-+---+--+-+-+++-++-+--+++++-+++-++++++++-+--+--++---+--+-+-+++-++-+--++---+---+-----
+--+----+-+-+++-++------++++-+-+++-+++---++--------++---++++---+-+----++++++--+---++-+----+--+--+-++-++-+---+--+-+-+++-++-+--+++++-+++-+++++-++-+--+--++---+--+-+-+++-++-+--++---+---+-----+
--+----+-+-+++-++------++++-+-+++-+++---++----++----++---++++---+-+----++++++--+---++-+----+---+-++-++-+---+--+-+-+++-++-+--+++++-+++-+++++--+-+--+--++---+--+-+-+++-++-+--++---+---+-----++
-+----+-+-+++-++------++++-+-+++-+++---++----+--+----++---++++---+-+----++++++--+---++-+----+-+-++-++-+---+--+-+-+++-++-+--+++++-+++-+++++----+--+--++---+--+-+-+++-++-+--++---+---+-----+++
+----+-+-+++-++------++++-+-+++-+++---++----+----+----++---++++---+-+----++++++--+---++-+----+-++-++-+---+--+-+-+++-++-+--+++++-+++-+++++---++--+--++---+--+-+-+++-++-+--++---+---+-----+++-
----+-+-+++-++------++++-+-+++-+++---++----+--++--+----++---++++---+-+----++++++--+---++-+----++-++-+---+--+-+-+++-++-+--+++++-+++-+++++---+---+--++---+--+-+-+++-++-+--++---+---+-----+++-+
---+-+-+++-++------++++-+-+++-+++---++----+--+--+--+----++---++++---+-+----++++++--+---++-+---+-++-+---+--+-+-+++-++-+--+++++-+++-+++++---+-+-+--++---+--+-+-+++-++-+--++---+---+-----+++-+-
--+-+-+++-++------++++-+-+++-+++---++----+--+----+--+----++---++++---+-+----++++++--+---++-+---++-+---+--+-+-+++-++-+--+++++-+++-+++++---+-+++--++---+--+-+-+++-++-+--++---+---+-----+++-+--
-+-+-+++-++------++++-+-+++-+++---++----+--+------+--+----++---++++---+-+----++++++--+---++-+-++-+---+--+-+-+++-++-+--+++++-+++-+++++---+-++---++---+--+-+-+++-++-+--++---+---+-----+++-+--+
+-+-+++-++------++++-+-+++-+++---++----+--+--------+--+----++---++++---+-+----++++++--+---++-++-+---+--+-+-+++-++-+--+++++-+++-+++++---+-++-+-++---+--+-+-+++-++-+--++---+---+-----+++-+--+-
-+-+++-++------++++-+-+++-+++---++----+--+----++----+--+----++---++++---+-+----++++++--+---++--+---+--+-+-+++-++-+--+++++-+++-+++++---+-++-++++---+--+-+-+++-++-+--++---+---+-----+++-+--+--
+-+++-++------++++-+-+++-+++---++----+--+----+--+----+--+----++---++++---+-+----++++++--+---+++---+--+-+-+++-++-+--+++++-+++-+++++---+-++-++-+---+--+-+-+++-++-+--++---+---+-----+++-+--+--+
-++-++-+---+++++-+++-+++--++-+--+---+-+-++-+++-+++-++-+-+---+--+-++-----+---+-----+++-+--+--+-+-+----+--+----++---++++---+-+----++++++--+---+-+----+--+----++---+++-+++-+-++++------++-+++-+
++-++-+---+++++-+++-+++--++-+--+---+-+-++-+++--++-++-+-+---+--+-++-----+---+-----+++-+--+--+-+++-+----+--+----++---++++---+-+----++++++--+---+----+--+----++---+++-+++-+-++++------++-+++-+-
+-++-+---+++++-+++-+++--++-+--+---+-+-++-+++--++-++-+-+---+--+-++-----+---+-----+++-+--+--+-++-++-+----+--+----++---++++---+-+----++++++--+------+--+----++---+++-+++-+-++++------++-+++-+-+
-++-+---+++++-+++-+++--++-+--+---+-+-++-+++--++-++-+-+---+--+-++-----+---+-----+++-+--+--+-+++--++-+----+--+----++---++++---+-+----++++++--+----+--+----++---+++-+++-+-++++------++-+++-+-+-
++-+---+++++-+++-+++--++-+--+---+-+-++-+++--++-++-+-+---+--+-++-----+---+-----+++-+--+--+-+++----++-+----+--+----++---++++---+-+----++++++--+--+--+----++---+++-+++-+-++++------++-+++-+-+--
+-+---+++++-+++-+++--++-+--+---+-+-++-+++--++-++-+-+---+--+-++-----+---+-----+++-+--+--+-+++-++---++-+----+--+----++---++++---+-+----++++++---+--+----++---+++-+++-+-++++------++-+++-+-+---
-+---+++++-+++-+++--++-+--+---+-+-++-+++--++-++-+-+---+--+-++-----+---+-----+++-+--+--+-+++-++-+---++-+----+--+----++---++++---+-+----++++++-+--+----++---+++-+++-+-++++------++-+++-+-+----
+---+++++-+++-+++--++-+--+---+-+-++-+++--++-++-+-+---+--+-++-----+---+-----+++-+--+--+-+++-++---+---++-+----+--+----++---++++---+-+----++++++--+----++---+++-+++-+-++++------++-+++-+-+----+
---+++++-+++-+++--++-+--+---+-+-++-+++--++-++-+-+---+--+-++-----+---+-----+++-+--+--+-+++-++-++--+---++-+----+--+----++---++++---+-+----+++++-+----++---+++-+++-+-++++------++-+++-+-+----+-
--+++++-+++-+++--++-+--+---+-+-++-+++--++-++-+-+---+--+-++-----+---+-----+++-+--+--+-+++-++-+-++--+---++-+----+--+----++---++++---+-+----+++++----++---+++-+++-+-++++------++-+++-+-+----+--
-+++++-+++-+++--++-+--+---+-+-++-+++--++-++-+-----+--+-++-----+---+-----+++-+--+--+-+++-++-+-++++--+---++-+----+--+----++---++++---+-+----+++----++---+++-+++-+-++++------++-+++-+-+----+--+
+++++-+++-+++--++-+--+---+-+-++-+++--++-++-+-----+--+-++-----+---+-----+++-+--+--+-+++-++-+-+-++++--+---++-+----+--+----++---++++---+-+----++---++---+++-+++-+-++++------++-+++-+-+----+--+-
++++-+++-+++--++-+--+---+-+-++-+++--++-++-+---+-+--+-++-----+---+-----+++-+--+--+-+++-++-+-+--+++++--+---++-+----+--+----++---++++---+-+----+--++---+++-+++-+-++++------++-+++-+-+----+--+--
+++-+++-+++--++-+--+---+-+-++-+++--++-++-+---+++--+-++-----+---+-----+++-+--+--+-+++-++-+-+---++++++--+---++-+----+--+----++---++++---+-+-----++---+++-+++-+-++++------++-+++-+-+----+--+---
++-+++-+++--++-+--+---+-+-++-+++--++-++-+---+++--+-++-----+---+-----+++-+--+--+-+++-++-+-+---+-++++++--+---++-+----+--+----++---++++---+-+---++---+++-+++-+-++++------++-+++-+-+----+--+----
+-+++-+++--++-+--+---+-+-++-+++--++-++-+---++++-+-++-----+---+-----+++-+--+--+-+++-++-+-+---+---++++++--+---++-+----+--+----++---++++---+-+--+---+++-+++-+-++++------++-+++-+-+----+--+----+
-+++-+++--++-+--+---+-+-++-+++--++-++-+---++++++-++-----+---+-----+++-+--+--+-+++-++-+-+---+-----++++++--+---++-+----+--+----++---++++---+-+----+++-+++-+-++++------++-+++-+-+----+--+----++
+++-+++--++-+--+---+-+-++-+++--++-++-+---+++++--++-----+---+-----+++-+--+--+-+++-++-+-+---+--+----++++++--+---++-+----+--+----++---++++---+-+--+++-+++-+-++++------++-+++-+-+----+--+----++-
++-+++--++-+--+---+-+-++-+++--++-++-+---+++++-+++-----+---+-----+++-+--+--+-+++-++-+-+---+--+-+----++++++--+---++-+----+--+----++---++++---+--+++-+++-+-++++------++-+++-+-+----+--+----++--
+-+++--++-+--+---+-+-++-+++--++-++-+---+++++-+++-----+---+-----+++-+--+--+-+++-++-+-+---+--+-+-+----++++++--+---++-+----+--+----++---++++---++++-+++-+-++++------++-+++-+-+----+--+----++---
-+++--++-+--+---+-+-++-+++--++-++-+---+++++-+++-----+---+-----+++-+--+--+-+++-++-+-+---+--+-+++-+----++++++--+---++-+----+--+----++---++++---++-+++-+-++++------++-+++-+-+----+--+----++---+
+++--++-+--+---+-+-++-+++--++-++-+---+++++-+++-----+---+-----+++-+--+--+-+++-++-+-+---+--+-++--+-+----++++++--+---++-+----+--+----++---++++--+-+++-+-++++------++-+++-+-+----+--+----++---++
++--++-+--+---+-+-++-+++--++-++-+---+++++-+++-+---+---+-----+++-+--+--+-+++-++-+-+---+--+-++----+-+----++++++--+---++-+----+--+----++---++++--+++-+-++++------++-+++-+-+----+--+----++---+++
+--++-+--+---+-+-++-+++--++-++-+---+++++-+++-++--+---+-----+++-+--+--+-+++-++-+-+---+--+-++------+-+----++++++--+---++-+----+--+----++---+++++++-+-++++------++-+++-+-+----+--+----++---+++-
--++-+--+---+-+-++-+++--++-++-+---+++++-+++-+++-+---+-----+++-+--+--+-+++-++-+-+---+--+-++----+---+-+----++++++--+---++-+----+--+----++---+++++-+-++++------++-+++-+-+----+--+----++---+++-+
-++-+--+---+-+-++-+++--++-++-+---+++++-+++-+++-+---+-----+++-+--+--+-+++-++-+-+---+--+-++-----++---+-+----++++++--+---++-+----+--+----++---+++-+-++++------++-+++-+-+----+--+----++---+++-++
++-+--+---+-+-++-+++--++-++-+---+++++-+++-+++-----+-----+++-+--+--+-+++-++-+-+---+--+-++-----++++---+-+----++++++--+---++-+----+--+----++---+-+-++++------++-+++-+-+----+--+----++---+++-+++
+-+--+---+-+-++-+++--++-++-+---+++++-+++-+++--+--+-----+++-+--+--+-+++-++-+-+---+--+-++-----+-++++---+-+----++++++--+---++-+----+--+----++---+-++++------++-+++-+-+----+--+----++---+++-+++-
-+--+---+-+-++-+++--++-++-+---+++++-+++-+++--++-+-----+++-+--+--+-+++-++-+-+---+--+-++-----+---++++---+-+----++++++--+---++-+----+--+----++---++++------++-+++-+-+----+--+----++---+++-+++-+
+--+---+-+-++-+++--++-++-+---+++++-+++-+++--++-+-----+++-+--+--+-+++-++-+-+---+--+-++-----+-----++++---+-+----++++++--+---++-+----+--+----++-++++------++-+++-+-+----+--+----++---+++-+++-+-
--+---+-+-++-+++--++-++-+---+++++-+++-+++--++-+-----+++-+--+--+-+++-++-+-+---+--+-++-----+---+---++++---+-+----++++++--+---++-+----+--+----+++++------++-+++-+-+----+--+----++---+++-+++-+-+
-+---+-+-++-+++--++-++-+---+++++-+++-+++--++-+-----+++-+--+--+-+++-++-+-+---+--+-++-----+---+-+---++++---+-+----++++++--+---++-+----+--+----+++------++-+++-+-+----+--+----++---+++-+++-+-++
+---+-+-++-+++--++-++-+---+++++-+++-+++--++-+-----+++-+--+--+-+++-++-+-+---+--+-++-----+---+--++---++++---+-+----++++++--+---++-+----+--+----+------++-+++-+-+----+--+----++---+++-+++-+-+++
---+-+-++-+++--++-++-+---+++++-+++-+++--++-+--+--+++-+--+--+-+++-++-+-+---+--+-++-----+---+----++---++++---+-+----++++++--+---++-+----+--+---------++-+++-+-+----+--+----++---+++-+++-+-++++
--+-+-++-+++--++-++-+---+++++-+++-+++--++-+--+--+++-+--+--+-+++-++-+-+---+--+-++-----+---+------++---++++---+-+----++++++--+---++-+----+--+-------++-+++-+-+----+--+----++---+++-+++-+-++++-
-+-+-++-+++--++-++-+---+++++-+++-+++--++-+--+--+++-+--+--+-+++-++-+-+---+--+-++-----+---+--------++---++++---+-+----++++++--+---++-+----+--+-----++-+++-+-+----+--+----++---+++-+++-+-++++--
+-+-++-+++--++-++-+---+++++-+++-+++--++-+--+---++-+--+--+-+++-++-+-+---+--+-++-----+---+-----+----++---++++---+-+----++++++--+---++-+----+--+---++-+++-+-+----+--+----++---+++-+++-+-++++---
-+-++-+++--++-++-+---+++++-+++-+++--++-+--+---++-+--+--+-+++-++-+-+---+--+-++-----+---+-----+++----++---++++---+-+----++++++--+---++-+----+----++-+++-+-+----+--+----++---+++-+++-+-++++----
+-++-+++--++-++-+---+++++-+++-+++--++-+--+---+--+--+--+-+++-++-+-+---+--+-++-----+---+-----+++-+----++---++++---+-+----++++++--+---++-+----+--++-+++-+-+----+--+----++---+++-+++-+-++++-----
-++-+++--++-++-+---+++++-+++-+++--++-+--+---+-++--+--+-+++-++-+-+---+--+-++-----+---+-----+++---+----++---++++---+-+----++++++--+---++-+----+++-+++-+-+----+--+----++---+++-+++-+-++++------
++-+++--++-++-+---+++++-+++-+++--++-+--+---+-+---+--+-+++-++-+-+---+--+-++-----+---+-----+++-++--+----++---++++---+-+----++++++--+---++-+----+-+++-+-+----+--+----++---+++-+++-+-++++------+
+-+++--++-++-+---+++++-+++-+++--++-+--+---+-+-+-+--+-+++-++-+-+---+--+-++-----+---+-----+++-+--+--+----++---++++---+-+----++++++--+---++-+----+++-+-+----+--+----++---+++-+++-+-++++------++
-+++--++-++-+---+++++-+++-+++--++-+--+---+-+-+++--+-+++-++-+-+---+--+-++-----+---+-----+++-+----+--+----++---++++---+-+----++++++--+---++-+--+++-+-+----+--+----++---+++-+++-+-++++------++-
+++--++-++-+---+++++-+++-+++--++-+--+---+-+-++---+-+++-++-+-+---+--+-++-----+---+-----+++-+--+---+--+----++---++++---+-+----++++++--+---++-+-++-+-+----+--+----++---+++-+++-+-++++------++-+
++--++-++-+---+++++-+++-+++--++-+--+---+-+-++-+-+-+++-++-+-+---+--+-++-----+---+-----+++-+--+-----+--+----++---++++---+-+----++++++--+---++-++-+-+----+--+----++---+++-+++-+-++++------++-++
+--++-++-+---+++++-+++-+++--++-+--+---+-+-++-+++-+++-++-+-+---+--+-++-----+---+-----+++-+--+--+----+--+----++---++++---+-+----++++++--+---++--+-+----+--+----++---+++-+++-+-++++------++-+++
--++-++-+---+++++-+++-+++--++-+--+---+-+-++-+++-+++-++-+-+---+--+-++-----+---+-----+++-+--+--+-+----+--+----++---++++---+-+----++++++--+---+++-+----+--+----++---+++-+++-+-++++------++-+++-
-++-++-+---+++++-+++-+++++--+-++-+++-+-+--+---++++-++-+-+---+--+-++--+++-+++-+++++---+-++-++--+-++++-++-++++--+++---+---+-+----++++++--+---+-+-+----+--+----++---++++---+-+----++++++--+---+
++-++-+---+++++-+++-+++++--+-++-+++-+-+--+---+-++-++-+-+---+--+-++--+++-+++-+++++---+-++-++--+-++++-++-++++--+++---+---+-+----++++++--+---+-+++-+----+--+----++---++++---+-+----++++++--+---
+-++-+---+++++-+++-+++++--+-++-+++-+-+--+---+-++-++-+-+---+--+-++--+++-+++-+++++---+-++-++--++++++-++-++++--+++---+---+-+----++++++--+---+-+--++-+----+--+----++---++++---+-+----++++++--+--
-++-+---+++++-+++-+++++--+-++-+++-+-+--+---+-++-++-+-+---+--+-++--+++-+++-+++++---+-++-++--++++++-++-++++--+++---+---+-+----++++++--+---+-+-+--++-+----+--+----++---++++---+-+----++++++--+-
++-+---+++++-+++-+++++--+-++-+++-+-+--+---+-++-++-+-+---+--+-++--+++-+++-+++++---+-++-++--+++-++-++-++++--+++---+---+-+----++++++--+---+-+-++---++-+----+--+----++---++++---+-+----++++++--+
+-+---+++++-+++-+++++--+-++-+++-+-+--+---+-++-++-+-+---+--+-++--+++-+++-+++++---+-++-++--+++-++-++-++++--+++---+---+-+----++++++--+---+-+-++++---++-+----+--+----++---++++---+-+----++++++--
-+---+++++-+++-+++++--+-++-+++-+-+--+---+-++-++-+-+---+--+-++--+++-+++-+++++---+-++-++--+++-++-++-++++--+++---+---+-+----++++++--+---+-+-++++-+---++-+----+--+----++---++++---+-+----++++++-
+---+++++-+++-+++++--+-++-+++-+-+--+---+-++-++-+-+---+--+-++--+++-+++-+++++---+-++-++--+++-++-++-++++--+++---+---+-+----++++++--+---+-+-++++---+---++-+----+--+----++---++++---+-+----++++++
---+++++-+++-+++++--+-++-+++-+-+--+---+-++-++-+-+---+--+-++--+++-+++-+++++---+-++-++--+++-++-++-++++--+++---+---+-+----++++++--+---+-+-++++-++--+---++-+----+--+----++---++++---+-+----+++++
--+++++-+++-+++++--+-++-+++-+-+--+---+-++-++-+-+---+--+-++--+++-+++-+++++---+-++-++--+++-++-+--++++--+++---+---+-+----++++++--+---+-+-++++-++++--+---++-+----+--+----++---++++---+-+----++++
-+++++-+++-+++++--+-++-+++-+-+--+---+-++-++-+-----+--+-++--+++-+++-+++++---+-++-++--+++-++-+-+++++--+++---+---+-+----++++++--+---+-+-++++-++-+++--+---++-+----+--+----++---++++---+-+----+++
+++++-+++-+++++--+-++-+++-+-+--+---+-++-++-+-----+--+-++--+++-+++-+++++---+-++-++--+++-++-+-+-+++--+++---+---+-+----++++++--+---+-+-++++-++-+++++--+---++-+----+--+----++---++++---+-+----++
++++-+++-+++++--+-++-+++-+-+--+---+-++-++-+---+-+--+-++--+++-+++-+++++---+-++-++--+++-++-+-+--++--+++---+---+-+----++++++--+---+-+-++++-++-+++++++--+---++-+----+--+----++---++++---+-+----+
+++-+++-+++++--+-++-+++-+-+--+---+-++-++-+---+++--+-++--+++-+++-+++++---+-++-++--+++-++-+-+---+--+++---+---+-+----++++++--+---+-+-++++-++-+++++++++--+---++-+----+--+----++---++++---+-+----
++-+++-+++++--+-++-+++-+-+--+---+-++-++-+---+++--+-++--+++-+++-+++++---+-++-++--+++-++-+-+---+--+++---+---+-+----++++++--+---+-+-++++-++-++++-++++++--+---++-+----+--+----++---++++---+-+---
+-+++-+++++--+-++-+++-+-+--+---+-++-++-+---++++-+-++--+++-+++-+++++---+-++-++--+++-++-+-+---+--+++---+---+-+----++++++--+---+-+-++++-++-++++---++++++--+---++-+----+--+----++---++++---+-+--
-+++-+++++--+-++-+++-+-+--+---+-++-++-+---++++++-++--+++-+++-+++++---+-++-++--+++-++-+-+---+--+++---+---+-+----++++++--+---+-+-++++-++-++++-----++++++--+---++-+----+--+----++---++++---+-+-
+++-+++++--+-++-+++-+-+--+---+-++-++-+---+++++--++--+++-+++-+++++---+-++-++--+++-++-+-+---+--+++---+---+-+----++++++--+---+-+-++++-++-++++--+----++++++--+---++-+----+--+----++---++++---+-+
++-+++++--+-++-+++-+-+--+---+-++-++-+---+++++-+++--+++-+++-+++++---+-++-++--+++-++-+-+---+--+-+---+---+-+----++++++--+---+-+-++++-++-++++--+++----++++++--+---++-+----+--+----++---++++---+-
+-+++++--+-++-+++-+-+--+---+-++-++-+---+++++-+++--+++-+++-+++++---+-++-++--+++-++-+-+---+--+-+---+---+-+----++++++--+---+-+-++++-++-++++--+++-+----++++++--+---++-+----+--+----++---++++---+
-+++++--+-++-+++-+-+--+---+-++-++-+---+++++-+++--+++-+++-+++++---+-++-++--+++-++-+-+---+--+-++--+---+-+----++++++--+---+-+-++++-++-++++--+++-+-+----++++++--+---++-+----+--+----++---++++---
+++++--+-++-+++-+-+--+---+-++-++-+---+++++-+++--+++-+++-+++++---+-++-++--+++-++-+-+---+--+-++--+---+-+----++++++--+---+-+-++++-++-++++--+++---+-+----++++++--+---++-+----+--+----++---++++--
++++--+-++-+++-+-+--+---+-++-++-+---+++++-+++-++++-+++-+++++---+-++-++--+++-++-+-+---+--+-++--+---+-+----++++++--+---+-+-++++-++-++++--+++-----+-+----++++++--+---++-+----+--+----++---++++-
+++--+-++-+++-+-+--+---+-++-++-+---+++++-+++-++++-+++-+++++---+-++-++--+++-++-+-+---+--+-++--+---+-+----++++++--+---+-+-++++-++-++++--+++---+---+-+----++++++--+---++-+----+--+----++---++++
++--+-++-+++-+-+--+---+-++-++-+---+++++-+++-++++-+++-+++++---+-++-++--+++-++-+-+---+--+-++--++--+-+----++++++--+---+-+-++++-++-++++--+++---+-+---+-+----++++++--+---++-+----+--+----++---+++
+--+-++-+++-+-+--+---+-++-++-+---+++++-+++-++++-+++-+++++---+-++-++--+++-++-+-+---+--+-++--+++-+-+----++++++--+---+-+-++++-++-++++--+++---+--++---+-+----++++++--+---++-+----+--+----++---++
--+-++-+++-+-+--+---+-++-++-+---+++++-+++-++++++++-+++++---+-++-++--+++-++-+-+---+--+-++--+++-+-+----++++++--+---+-+-++++-++-++++--+++---+---+++---+-+----++++++--+---++-+----+--+----++---+
-+-++-+++-+-+--+---+-++-++-+---+++++-+++-+++++-++-+++++---+-++-++--+++-++-+-+---+--+-++--+++-+-+----++++++--+---+-+-++++-++-++++--+++---+---+++++---+-+----++++++--+---++-+----+--+----++---
+-++-+++-+-+--+---+-++-++-+---+++++-+++-+++++--+-+++++---+-++-++--+++-++-+-+---+--+-++--+++-+++----++++++--+---+-+-++++-++-++++--+++---+---+--++++---+-+----++++++--+---++-+----+--+----++--
-++-+++-+-+--+---+-++-++-+---+++++-+++-+++++--+-+++++---+-++-++--+++-++-+-+---+--+-++--+++-+++----++++++--+---+-+-++++-++-++++--+++---+---+-+--++++---+-+----++++++--+---++-+----+--+----++-
++-+++-+-+--+---+-++-++-+---+++++-+++-+++++--+-+++++---+-++-++--+++-++-+-+---+--+-++--+++-+++----++++++--+---+-+-++++-++-++++--+++---+---+-+----++++---+-+----++++++--+---++-+----+--+----++
+-+++-+-+--+---+-++-++-+---+++++-+++-+++++--+-+++++---+-++-++--+++-++-+-+---+--+-++--+++-+++-+--++++++--+---+-+-++++-++-++++--+++---+---+-+--+---++++---+-+----++++++--+---++-+----+--+----+
-+++-+-+--+---+-++-++-+---+++++-+++-+++++--+-+++++---+-++-++--+++-++-+-+---+--+-++--+++-+++-++-++++++--+---+-+-++++-++-++++--+++---+---+-+---++---++++---+-+----++++++--+---++-+----+--+----
+++-+-+--+---+-++-++-+---+++++-+++-+++++--+-++-++---+-++-++--+++-++-+-+---+--+-++--+++-+++-+++++++++--+---+-+-++++-++-++++--+++---+---+-+-----++---++++---+-+----++++++--+---++-+----+--+---
++-+-+--+---+-++-++-+---+++++-+++-+++++--+-++-++---+-++-++--+++-++-+-+---+--+-++--+++-+++-+++++++++--+---+-+-++++-++-++++--+++---+---+-+----+--++---++++---+-+----++++++--+---++-+----+--+--
+-+-+--+---+-++-++-+---+++++-+++-+++++--+-++-++---+-++-++--+++-++-+-+---+--+-++--+++-+++-+++++++++--+---+-+-++++-++-++++--+++---+---+-+----++---++---++++---+-+----++++++--+---++-+----+--+-
-+-+--+---+-++-++-+---+++++-+++-+++++--+-++-+++--+-++-++--+++-++-+-+---+--+-++--+++-+++-+++++-+++--+---+-+-++++-++-++++--+++---+---+-+----+++----++---++++---+-+----++++++--+---++-+----+--+
+-+--+---+-++-++-+---+++++-+++-+++++--+-++-+++--+-++-++--+++-++-+-+---+--+-++--+++-+++-+++++--++--+---+-+-++++-++-++++--+++---+---+-+----+++++----++---++++---+-+----++++++--+---++-+----+--
-+--+---+-++-++-+---+++++-+++-+++++--+-++-+++-++-++-++--+++-++-+-+---+--+-++--+++-+++-+++++---+--+---+-+-++++-++-++++--+++---+---+-+----+++++-+----++---++++---+-+----++++++--+---++-+----+-
+--+---+-++-++-+---+++++-+++-+++++--+-++-+++-+--++-++--+++-++-+-+---+--+-++--+++-+++-+++++---+--+---+-+-++++-++-++++--+++---+---+-+----++++++--+----++---++++---+-+----++++++--+---++-+----+
--+---+-++-++-+---+++++-+++-+++++--+-++-+++-+-+++-++--+++-++-+-+---+--+-++--+++-+++-+++++---+--+---+-+-++++-++-++++--+++---+---+-+----++++++-+--+----++---++++---+-+----++++++--+---++-+----
-+---+-++-++-+---+++++-+++-+++++--+-++-+++-+-+-+-++--+++-++-+-+---+--+-++--+++-+++-+++++---+-++---+-+-++++-++-++++--+++---+---+-+----++++++---+--+----++---++++---+-+----++++++--+---++-+---
+---+-++-++-+---+++++-+++-+++++--+-++-+++-+-+---++--+++-++-+-+---+--+-++--+++-+++-+++++---+-++---+-+-++++-++-++++--+++---+---+-+----++++++--+--+--+----++---++++---+-+----++++++--+---++-+--
---+-++-++-+---+++++-+++-+++++--+-++-+++-+-+--+++--+++-++-+-+---+--+-++--+++-+++-+++++---+-++---+-+-++++-++-++++--+++---+---+-+----++++++--+----+--+----++---++++---+-+----++++++--+---++-+-
--+-++-++-+---+++++-+++-+++++--+-++-+++-+-+--+-+--+++-++-+-+---+--+-++--+++-+++-+++++---+-++-+-+-+-++++-++-++++--+++---+---+-+----++++++--+------+--+----++---++++---+-+----++++++--+---++-+
-+-++-++-+---+++++-+++-+++++--+-++-+++-+-+--+----+++-++-+-+---+--+-++--+++-+++-+++++---+-++-+++-+-++++-++-++++--+++---+---+-+----++++++--+---+----+--+----++---++++---+-+----++++++--+---++-
+-++-++-+---+++++-+++-+++++--+-++-+++-+-+--+----+++-++-+-+---+--+-++--+++-+++-+++++---+-++-++--+-++++-++-++++--+++---+---+-+----++++++--+---+-+----+--+----++---++++---+-+----++++++--+---++
</content>

<content full_path="exllamav3/util/hadamard_data/hadamard_116.txt">
++--+--+-+++-++++-+++-+--+--+++++-++-+---++++++---+-++-++++-+---++--+-++++++-+--++---+-+++---++--+-+----+-+--++---++
+++--+--+-+++-++++-+++-+--+--+++++-++-+---++++++---+-++-++-+-+---++--+-++++++-+--++---+++++---++--+-+----+-+--++---+
-+++--+--+-+++-++++-+++-+--+-++++++-++-+---++++++---+-++-++-+-+---++--+-++++++-+--++---+++++---++--+-+----+-+--++---
--+++--+--+-+++-++++-+++-+--++++++++-++-+---++++++---+-++--+-+-+---++--+-++++++-+--++---+++++---++--+-+----+-+--++--
+--+++--+--+-+++-++++-+++-+---+++++++-++-+---++++++---+-++--+-+-+---++--+-++++++-+--++---+++++---++--+-+----+-+--++-
-+--+++--+--+-+++-++++-+++-+-+-+++++++-++-+---++++++---+-+---+-+-+---++--+-++++++-+--++---+++++---++--+-+----+-+--++
--+--+++--+--+-+++-++++-+++-+++-+++++++-++-+---++++++---+-+---+-+-+---++--+-++++++-+--++---+++++---++--+-+----+-+--+
+--+--+++--+--+-+++-++++-+++--++-+++++++-++-+---++++++---+++---+-+-+---++--+-++++++-+--++---+++++---++--+-+----+-+--
-+--+--+++--+--+-+++-++++-++++-++-+++++++-++-+---++++++----++---+-+-+---++--+-++++++-+--++---+++++---++--+-+----+-+-
+-+--+--+++--+--+-+++-++++-++-+-++-+++++++-++-+---++++++----++---+-+-+---++--+-++++++-+--++---+++++---++--+-+----+-+
++-+--+--+++--+--+-+++-++++-+--+-++-+++++++-++-+---++++++-+--++---+-+-+---++--+-++++++-+--++---+++++---++--+-+----+-
+++-+--+--+++--+--+-+++-++++----+-++-+++++++-++-+---++++++-+--++---+-+-+---++--+-++++++-+--++---+++++---++--+-+----+
-+++-+--+--+++--+--+-+++-+++++---+-++-+++++++-++-+---++++++-+--++---+-+-+---++--+-++++++-+--++---+++++---++--+-+----
+-+++-+--+--+++--+--+-+++-+++++---+-++-+++++++-++-+---++++++-+--++---+-+-+---++--+-++++-+-+--++---+++++---++--+-+---
++-+++-+--+--+++--+--+-+++-+++++---+-++-+++++++-++-+---++++++-+--++---+-+-+---++--+-+++--+-+--++---+++++---++--+-+--
+++-+++-+--+--+++--+--+-+++-+++++---+-++-+++++++-++-+---++++++-+--++---+-+-+---++--+-++---+-+--++---+++++---++--+-+-
++++-+++-+--+--+++--+--+-+++-+++++---+-++-+++++++-++-+---++++++-+--++---+-+-+---++--+-+----+-+--++---+++++---++--+-+
-++++-+++-+--+--+++--+--+-+++++++++---+-++-+++++++-++-+---++++++-+--++---+-+-+---++--+-+----+-+--++---+++++---++--+-
+-++++-+++-+--+--+++--+--+-++-++++++---+-++-+++++++-++-+---++++++-+--++---+-+-+---++--+-+----+-+--++---+++++---++--+
++-++++-+++-+--+--+++--+--+-+--++++++---+-++-+++++++-++-+-+-++++++-+--++---+-+-+---++--+-+----+-+--++---+++++---++--
+++-++++-+++-+--+--+++--+--+----++++++---+-++-+++++++-++-+-+-++++++-+--++---+-+-+---++--+-+----+-+--++---+++++---++-
-+++-++++-+++-+--+--+++--+--++---++++++---+-++-+++++++-++---+-++++++-+--++---+-+-+---++--+-+----+-+--++---+++++---++
+-+++-++++-+++-+--+--+++--+---+---++++++---+-++-+++++++-+++--+-++++++-+--++---+-+-+---++--+-+----+-+--++---+++++---+
-+-+++-++++-+++-+--+--+++--+-+-+---++++++---+-++-+++++++-+++--+-++++++-+--++---+-+-+---++--+-+----+-+--++---+++++---
--+-+++-++++-+++-+--+--+++--+++-+---++++++---+-++-+++++++--++--+-++++++-+--++---+-+-+---++--+-+----+-+--++---+++++--
+--+-+++-++++-+++-+--+--+++---++-+---++++++---+-++-+++++++--++--+-++++++-+--++---+-+-+---++--+-+----+-+--++---+++++-
-+--+-+++-++++-+++-+--+--+++-+-++-+---++++++---+-++-++++++---++--+-++++++-+--++---+-+-+---++--+-+----+-+--++---+++++
--+--+-+++-++++-+++-+--+--+++++-++-+---++++++---+-++-++++++---++--+-++++++-+--++---+-+-+---++--+-+----+-+--++---++++
+--+--+-+++-++++-+++-+--+--+++++-++-+---++++++---+-++-++++-+---++--+-++++++-+--++---+-+++---++--+-+----+-+--++---+++
----+--+-+++------+++-+--+---++--+--+-+++-++++-+++-+--+--+---+++--++-+-++++-+-++--+++--+-+---++--+-++++++-+--++---+-
-----+--+-+++------+++-+--+--+++--+--+-+++-++++-+++-+--+------+++--++-+-++++-+-++--+++--+-+---++--+-++++++-+--++---+
------+--+-+++------+++-+--+--+++--+--+-+++-++++-+++-+--+------+++--++-+-++++-+-++--++++-+-+---++--+-++++++-+--++---
-------+--+-+++------+++-+--+--+++--+--+-+++-++++-+++-+--++-----+++--++-+-++++-+-++--++-+-+-+---++--+-++++++-+--++--
+-------+--+-+++------+++-+--+--+++--+--+-+++-++++-+++-+--++-----+++--++-+-++++-+-++--+--+-+-+---++--+-++++++-+--++-
-+-------+--+-+++------+++-+--+--+++--+--+-+++-++++-+++-+-+++-----+++--++-+-++++-+-++-----+-+-+---++--+-++++++-+--++
--+-------+--+-+++------+++-+--+--+++--+--+-+++-++++-+++-+-+++-----+++--++-+-++++-+-++-+---+-+-+---++--+-++++++-+--+
+--+-------+--+-+++------+++-+--+--+++--+--+-+++-++++-+++---+++-----+++--++-+-++++-+-++++---+-+-+---++--+-++++++-+--
-+--+-------+--+-+++------+++-+--+--+++--+--+-+++-++++-++++--+++-----+++--++-+-++++-+-+-++---+-+-+---++--+-++++++-+-
+-+--+-------+--+-+++------+++-+--+--+++--+--+-+++-++++-++++--+++-----+++--++-+-++++-+---++---+-+-+---++--+-++++++-+
++-+--+-------+--+-+++------+++-+--+--+++--+--+-+++-++++-+-++--+++-----+++--++-+-++++-++--++---+-+-+---++--+-++++++-
+++-+--+-------+--+-+++------+++-+--+--+++--+--+-+++-++++-+-++--+++-----+++--++-+-++++--+--++---+-+-+---++--+-++++++
-+++-+--+-------+--+-+++------+++-+--+--+++--+--+-+++-++++-+-++--+++-----+++--++-+-+++++-+--++---+-+-+---++--+-+++++
--+++-+--+-------+--+-+++----+-+++-+--+--+++--+--+-+++-++++-+-++--+++-----+++--++-+-+++++-+--++---+-+-+---++--+-++++
---+++-+--+-------+--+-+++---++-+++-+--+--+++--+--+-+++-++++-+-++--+++-----+++--++-+-+++++-+--++---+-+-+---++--+-+++
----+++-+--+-------+--+-+++--+++-+++-+--+--+++--+--+-+++-++++-+-++--+++-----+++--++-+-+++++-+--++---+-+-+---++--+-++
-----+++-+--+-------+--+-+++-++++-+++-+--+--+++--+--+-+++-++++-+-++--+++-----+++--++-+-+++++-+--++---+-+-+---++--+-+
------+++-+--+-------+--+-+++-++++-+++-+--+--+++--+--+-+++-++++-+-++--+++-----+++--++-+++++++-+--++---+-+-+---++--+-
+------+++-+--+-------+--+-+++-++++-+++-+--+--+++--+--+-+++-++++-+-++--+++-----+++--++--++++++-+--++---+-+-+---++--+
++------+++-+--+-------+--+-+++-++++-+++-+--+--+++--+--+-+-+-++++-+-++--+++-----+++--+++-++++++-+--++---+-+-+---++--
+++------+++-+--+-------+--+-+++-++++-+++-+--+--+++--+--+-+-+-++++-+-++--+++-----+++--+-+-++++++-+--++---+-+-+---++-
-+++------+++-+--+-------+--+-+++-++++-+++-+--+--+++--+--+++-+-++++-+-++--+++-----+++----+-++++++-+--++---+-+-+---++
+-+++------+++-+--+-------+--+-+++-++++-+++-+--+--+++--+---++-+-++++-+-++--+++-----+++-+--+-++++++-+--++---+-+-+---+
-+-+++------+++-+--+-------+--+-+++-++++-+++-+--+--+++--+---++-+-++++-+-++--+++-----+++++--+-++++++-+--++---+-+-+---
--+-+++------+++-+--+-------+--+-+++-++++-+++-+--+--+++--++--++-+-++++-+-++--+++-----++-++--+-++++++-+--++---+-+-+--
+--+-+++------+++-+--+-------+--+-+++-++++-+++-+--+--+++--++--++-+-++++-+-++--+++-----+--++--+-++++++-+--++---+-+-+-
-+--+-+++------+++-+--+-------+--+-+++-++++-+++-+--+--+++-+++--++-+-++++-+-++--+++--------++--+-++++++-+--++---+-+-+
--+--+-+++------+++-+--+-------+--+-+++-++++-+++-+--+--+++-+++--++-+-++++-+-++--+++----+---++--+-++++++-+--++---+-+-
---+--+-+++------+++-+--+----+--+--+-+++-++++-+++-+--+--++--+++--++-+-++++-+-++--+++----+---++--+-++++++-+--++---+-+
-+-+++--++-+------+-++--+++-++++---++--+-+----+-+--++---++++--+--+-+++-++++-+++-+--+--+----+--+-+++------+++-+--+---
+-+-+++--++-+------+-++--+++-++++---++--+-+----+-+--++---++++--+--+-+++-++++-+++-+--+-------+--+-+++------+++-+--+--
-+-+-+++--++-+------+-++--++++++++---++--+-+----+-+--++----+++--+--+-+++-++++-+++-+--+-------+--+-+++------+++-+--+-
+-+-+-+++--++-+------+-++--++-+++++---++--+-+----+-+--++----+++--+--+-+++-++++-+++-+--+-------+--+-+++------+++-+--+
++-+-+-+++--++-+------+-++--+--+++++---++--+-+----+-+--++-+--+++--+--+-+++-++++-+++-+--+-------+--+-+++------+++-+--
+++-+-+-+++--++-+------+-++-----+++++---++--+-+----+-+--++-+--+++--+--+-+++-++++-+++-+--+-------+--+-+++------+++-+-
-+++-+-+-+++--++-+------+-++-+---+++++---++--+-+----+-+--+--+--+++--+--+-+++-++++-+++-+--+-------+--+-+++------+++-+
--+++-+-+-+++--++-+------+-++++---+++++---++--+-+----+-+--+--+--+++--+--+-+++-++++-+++-+--+-------+--+-+++------+++-
+--+++-+-+-+++--++-+------+-+-++---+++++---++--+-+----+-+--+--+--+++--+--+-+++-++++-+++-+--+-------+--+-+++------+++
++--+++-+-+-+++--++-+------+---++---+++++---++--+-+----+-++-+--+--+++--+--+-+++-++++-+++-+--+-------+--+-+++------++
-++--+++-+-+-+++--++-+------++--++---+++++---++--+-+----+-++-+--+--+++--+--+-+++-++++-+++-+--+-------+--+-+++------+
+-++--+++-+-+-+++--++-+-------+--++---+++++---++--+-+----++++-+--+--+++--+--+-+++-++++-+++-+--+-------+--+-+++------
-+-++--+++-+-+-+++--++-+-----+-+--++---+++++---++--+-+-----+++-+--+--+++--+--+-+++-++++-+++-+--+-------+--+-+++-----
--+-++--+++-+-+-+++--++-+-----+-+--++---+++++---++--+-+---+-+++-+--+--+++--+--+-+++-+++--+++-+--+-------+--+-+++----
---+-++--+++-+-+-+++--++-+-----+-+--++---+++++---++--+-+--++-+++-+--+--+++--+--+-+++-++---+++-+--+-------+--+-+++---
----+-++--+++-+-+-+++--++-+-----+-+--++---+++++---++--+-+-+++-+++-+--+--+++--+--+-+++-+----+++-+--+-------+--+-+++--
-----+-++--+++-+-+-+++--++-+-----+-+--++---+++++---++--+-+++++-+++-+--+--+++--+--+-+++------+++-+--+-------+--+-+++-
------+-++--+++-+-+-+++--++-++----+-+--++---+++++---++--+--++++-+++-+--+--+++--+--+-+++------+++-+--+-------+--+-+++
+------+-++--+++-+-+-+++--++--+----+-+--++---+++++---++--++-++++-+++-+--+--+++--+--+-+++------+++-+--+-------+--+-++
-+------+-++--+++-+-+-+++--+++-+----+-+--++---+++++---++--++-++++-+++-+--+--+++--+--+-+++------+++-+--+-------+--+-+
+-+------+-++--+++-+-+-+++--+-+-+----+-+--++---+++++---++-+++-++++-+++-+--+--+++--+--+-+++------+++-+--+-------+--+-
++-+------+-++--+++-+-+-+++----+-+----+-+--++---+++++---++-+++-++++-+++-+--+--+++--+--+-+++------+++-+--+-------+--+
-++-+------+-++--+++-+-+-+++-+--+-+----+-+--++---+++++---++-+++-++++-+++-+--+--+++--+--+-+++------+++-+--+-------+--
--++-+------+-++--+++-+-+-+++++--+-+----+-+--++---+++++----+-+++-++++-+++-+--+--+++--+--+-+++------+++-+--+-------+-
+--++-+------+-++--+++-+-+-++-++--+-+----+-+--++---+++++----+-+++-++++-+++-+--+--+++--+--+-+++------+++-+--+-------+
++--++-+------+-++--+++-+-+-+--++--+-+----+-+--++---+++++-+--+-+++-++++-+++-+--+--+++--+--+-+++------+++-+--+-------
+++--++-+------+-++--+++-+-+----++--+-+----+-+--++---+++++-+--+-+++-++++-+++-+--+--+++--+--+-+++------+++-+--+------
-+++--++-+------+-++--+++-+-++---++--+-+----+-+--++---++++--+--+-+++-++++-+++-+--+--+++--+--+-+++------+++-+--+-----
+-+++--++-+------+-++--+++-+-++---++--+-+----+-+--++---++++--+--+-+++-++++-+++-+--+--++---+--+-+++------+++-+--+----
---+++--++-+-++++-+-++--+++---+-+++--++-+------+-++--+++-+++++-++-+---++++++---+-++-+++++--+--+-+++-++++-+++-+--+--+
----+++--++-+-++++-+-++--+++-+-+-+++--++-+------+-++--+++-+++++-++-+---++++++---+-++-+++++--+--+-+++-++++-+++-+--+--
-----+++--++-+-++++-+-++--+++-+-+-+++--++-+------+-++--+++++++++-++-+---++++++---+-++-+-+++--+--+-+++-++++-+++-+--+-
+-----+++--++-+-++++-+-++--+++-+-+-+++--++-+------+-++--+++++++++-++-+---++++++---+-++---+++--+--+-+++-++++-+++-+--+
++-----+++--++-+-++++-+-++--+++-+-+-+++--++-+------+-++--+-+++++++-++-+---++++++---+-+++--+++--+--+-+++-++++-+++-+--
+++-----+++--++-+-++++-+-++--+++-+-+-+++--++-+------+-++--+-+++++++-++-+---++++++---+-+-+--+++--+--+-+++-++++-+++-+-
-+++-----+++--++-+-++++-+-++--+++-+-+-+++--++-+------+-++-++-+++++++-++-+---++++++---+---+--+++--+--+-+++-++++-+++-+
--+++-----+++--++-+-++++-+-++--+++-+-+-+++--++-+------+-++-++-+++++++-++-+---++++++---++--+--+++--+--+-+++-++++-+++-
+--+++-----+++--++-+-++++-+-++--+++-+-+-+++--++-+------+-++-++-+++++++-++-+---++++++----+--+--+++--+--+-+++-++++-+++
++--+++-----+++--++-+-++++-+-++--+++-+-+-+++--++-+------+--+-++-+++++++-++-+---++++++--+-+--+--+++--+--+-+++-++++-++
-++--+++-----+++--++-+-++++-+-++--+++-+-+-+++--++-+------+--+-++-+++++++-++-+---++++++-++-+--+--+++--+--+-+++-++++-+
+-++--+++-----+++--++-+-++++-+-++--+++-+-+-+++--++-+---------+-++-+++++++-++-+---+++++++++-+--+--+++--+--+-+++-++++-
-+-++--+++-----+++--++-+-++++-+-++--+++-+-+-+++--++-+-----+---+-++-+++++++-++-+---+++++-+++-+--+--+++--+--+-+++-++++
+-+-++--+++-----+++--++-+-+++--+-++--+++-+-+-+++--++-+----++---+-++-+++++++-++-+---+++++-+++-+--+--+++--+--+-+++-+++
++-+-++--+++-----+++--++-+-++---+-++--+++-+-+-+++--++-+---+++---+-++-+++++++-++-+---+++++-+++-+--+--+++--+--+-+++-++
+++-+-++--+++-----+++--++-+-+----+-++--+++-+-+-+++--++-+--++++---+-++-+++++++-++-+---+++++-+++-+--+--+++--+--+-+++-+
++++-+-++--+++-----+++--++-+------+-++--+++-+-+-+++--++-+-+++++---+-++-+++++++-++-+---+++++-+++-+--+--+++--+--+-+++-
-++++-+-++--+++-----+++--++-+------+-++--+++-+-+-+++--++-+++++++---+-++-+++++++-++-+----++++-+++-+--+--+++--+--+-+++
+-++++-+-++--+++-----+++--++-+------+-++--+++-+-+-+++--++--++++++---+-++-+++++++-++-+--+-++++-+++-+--+--+++--+--+-++
-+-++++-+-++--+++-----+++--++-+------+-++--+++-+-+-+++--++--++++++---+-++-+++++++-++-+-++-++++-+++-+--+--+++--+--+-+
+-+-++++-+-++--+++-----+++--++-+------+-++--+++-+-+-+++--+---++++++---+-++-+++++++-++-++++-++++-+++-+--+--+++--+--+-
++-+-++++-+-++--+++-----+++--++-+------+-++--+++-+-+-+++--+---++++++---+-++-+++++++-++--+++-++++-+++-+--+--+++--+--+
-++-+-++++-+-++--+++-----+++--++-+------+-++--+++-+-+-+++--+---++++++---+-++-+++++++-+++-+++-++++-+++-+--+--+++--+--
--++-+-++++-+-++--+++-----+++--++-+------+-++--+++-+-+-++++-+---++++++---+-++-+++++++-+-+-+++-++++-+++-+--+--+++--+-
+--++-+-++++-+-++--+++-----+++--++-+------+-++--+++-+-+-++++-+---++++++---+-++-+++++++---+-+++-++++-+++-+--+--+++--+
++--++-+-++++-+-++--+++-----+++--++-+------+-++--+++-+-+-+-++-+---++++++---+-++-++++++++--+-+++-++++-+++-+--+--+++--
+++--++-+-++++-+-++--+++-----+++--++-+------+-++--+++-+-+-+-++-+---++++++---+-++-++++++-+--+-+++-++++-+++-+--+--+++-
-+++--++-+-++++-+-++--+++-----+++--++-+------+-++--+++-+-+++-++-+---++++++---+-++-+++++--+--+-+++-++++-+++-+--+--+++
--+++--++-+-++++-+-++--+++---+-+++--++-+------+-++--+++-+-+++-++-+---++++++---+-++-+++++--+--+-+++-++++-+++-+--+--++
</content>

<content full_path="exllamav3/util/hadamard_data/hadamard_100.txt">
+-++++----++--++----++++-+++-+++---+-++-+---+++-+++-++-+++-++----++-+++-++-+--++-+-+--------+-+-++--
-+-++++----++--++----++++++++-+++---+-++-+---+++-+-+-++-+++-++----++-+++-++-+--++-+-+--------+-+-++-
+-+-++++----++--++----++++++++-+++---+-++-+---+++-+-+-++-+++-++----++-+++-+--+--++-+-+--------+-+-++
++-+-++++----++--++----++-+++++-+++---+-++-+---+++++-+-++-+++-++----++-+++-+--+--++-+-+--------+-+-+
+++-+-++++----++--++----++-+++++-+++---+-++-+---++-++-+-++-+++-++----++-+++++--+--++-+-+--------+-+-
++++-+-++++----++--++----++-+++++-+++---+-++-+---++-++-+-++-+++-++----++-++-++--+--++-+-+--------+-+
-++++-+-++++----++--++---+++-+++++-+++---+-++-+---++-++-+-++-+++-++----++-++-++--+--++-+-+--------+-
--++++-+-++++----++--++---+++-+++++-+++---+-++-+--+++-++-+-++-+++-++----++--+-++--+--++-+-+--------+
---++++-+-++++----++--++---+++-+++++-+++---+-++-+--+++-++-+-++-+++-++----+++-+-++--+--++-+-+--------
----++++-+-++++----++--++---+++-+++++-+++---+-++-++-+++-++-+-++-+++-++----+-+-+-++--+--++-+-+-------
+----++++-+-++++----++--++---+++-+++++-+++---+-++-++-+++-++-+-++-+++-++------+-+-++--+--++-+-+------
++----++++-+-++++----++---+---+++-+++++-+++---+-++-++-+++-++-+-++-+++-++------+-+-++--+--++-+-+-----
-++----++++-+-++++----++-+-+---+++-+++++-+++---+-+--++-+++-++-+-++-+++-++------+-+-++--+--++-+-+----
--++----++++-+-++++----++++-+---+++-+++++-+++---+----++-+++-++-+-++-+++-++------+-+-++--+--++-+-+---
+--++----++++-+-++++----+-++-+---+++-+++++-+++---+----++-+++-++-+-++-+++-++------+-+-++--+--++-+-+--
++--++----++++-+-++++----+-++-+---+++-+++++-+++---+----++-+++-++-+-++-+++-+-------+-+-++--+--++-+-+-
-++--++----++++-+-++++----+-++-+---+++-+++++-+++--++----++-+++-++-+-++-+++---------+-+-++--+--++-+-+
--++--++----++++-+-++++----+-++-+---+++-+++++-+++--++----++-+++-++-+-++-++++--------+-+-++--+--++-+-
---++--++----++++-+-++++----+-++-+---+++-+++++-++++-++----++-+++-++-+-++-++-+--------+-+-++--+--++-+
----++--++----++++-+-+++++---+-++-+---+++-+++++-++++-++----++-+++-++-+-++-++-+--------+-+-++--+--++-
+----++--++----++++-+-+++++---+-++-+---+++-+++++-++++-++----++-+++-++-+-++--+-+--------+-+-++--+--++
++----++--++----++++-+-+++++---+-++-+---+++-+++++--+++-++----++-+++-++-+-+++-+-+--------+-+-++--+--+
+++----++--++----++++-+-+-+++---+-++-+---+++-++++++-+++-++----++-+++-++-+-+++-+-+--------+-+-++--+--
++++----++--++----++++-+-+-+++---+-++-+---+++-++++++-+++-++----++-+++-++-+--++-+-+--------+-+-++--+-
-++++----++--++----++++-+++-+++---+-++-+---+++-+++-++-+++-++----++-+++-++-+--++-+-+--------+-+-++--+
---+---+++-+--+-+++---+--+-++++----++--++----++++--++--+-+-++++++++-+-+--+++-++-+++-++----++-+++-++-
----+---+++-+--+-+++---+--+-++++----++--++----+++++-++--+-+-++++++++-+-+--+-+-++-+++-++----++-+++-++
-----+---+++-+--+-+++---++-+-++++----++--++----+++++-++--+-+-++++++++-+-+--+-+-++-+++-++----++-+++-+
+-----+---+++-+--+-+++---++-+-++++----++--++----++-++-++--+-+-++++++++-+-+-++-+-++-+++-++----++-+++-
-+-----+---+++-+--+-+++--+++-+-++++----++--++----+--++-++--+-+-++++++++-+-+-++-+-++-+++-++----++-+++
--+-----+---+++-+--+-+++-++++-+-++++----++--++----+--++-++--+-+-++++++++-+-+-++-+-++-+++-++----++-++
---+-----+---+++-+--+-+++-++++-+-++++----++--++----+--++-++--+-+-++++++++-+++-++-+-++-+++-++----++-+
+---+-----+---+++-+--+-++--++++-+-++++----++--++--+-+--++-++--+-+-++++++++-+++-++-+-++-+++-++----++-
++---+-----+---+++-+--+-+---++++-+-++++----++--++--+-+--++-++--+-+-++++++++-+++-++-+-++-+++-++----++
+++---+-----+---+++-+--+-----++++-+-++++----++--+++-+-+--++-++--+-+-++++++++-+++-++-+-++-+++-++----+
-+++---+-----+---+++-+--++----++++-+-++++----++--+++-+-+--++-++--+-+-++++++++-+++-++-+-++-+++-++----
+-+++---+-----+---+++-+--++----++++-+-++++----++--+++-+-+--++-++--+-+-+++++-++-+++-++-+-++-+++-++---
-+-+++---+-----+---+++-+--++----++++-+-++++----++-++++-+-+--++-++--+-+-++++--++-+++-++-+-++-+++-++--
--+-+++---+-----+---+++-+--++----++++-+-++++----+++++++-+-+--++-++--+-+-+++---++-+++-++-+-++-+++-++-
+--+-+++---+-----+---+++-+--++----++++-+-++++----+++++++-+-+--++-++--+-+-++----++-+++-++-+-++-+++-++
-+--+-+++---+-----+---+++++--++----++++-+-++++----+++++++-+-+--++-++--+-+-++----++-+++-++-+-++-+++-+
+-+--+-+++---+-----+---++-++--++----++++-+-++++---++++++++-+-+--++-++--+-+-++----++-+++-++-+-++-+++-
++-+--+-+++---+-----+---+--++--++----++++-+-++++---++++++++-+-+--++-++--+-+-++----++-+++-++-+-++-+++
+++-+--+-+++---+-----+------++--++----++++-+-++++-+-++++++++-+-+--++-++--+-+-++----++-+++-++-+-++-++
-+++-+--+-+++---+-----+------++--++----++++-+-++++-+-++++++++-+-+--++-++--+++-++----++-+++-++-+-++-+
--+++-+--+-+++---+-----+-+----++--++----++++-+-++++-+-++++++++-+-+--++-++--+++-++----++-+++-++-+-++-
---+++-+--+-+++---+-----+++----++--++----++++-+-++-+-+-++++++++-+-+--++-++--+++-++----++-+++-++-+-++
+---+++-+--+-+++---+-----+++----++--++----++++-+-+--+-+-++++++++-+-+--++-+++-+++-++----++-+++-++-+-+
-+---+++-+--+-+++---+----++++----++--++----++++-+-+--+-+-++++++++-+-+--++-+++-+++-++----++-+++-++-+-
--+---+++-+--+-+++---+----++++----++--++----++++-+++--+-+-++++++++-+-+--++--++-+++-++----++-+++-++-+
-+--+---+--++++--+---+--++--++-+-+--------+-+-++--+-++++----++--++----++++----+---+++-+--+-+++---+--
+-+--+---+--++++--+---+---+--++-+-+--------+-+-++--+-++++----++--++----++++----+---+++-+--+-+++---+-
-+-+--+---+--++++--+---+---+--++-+-+--------+-+-+++-+-++++----++--++----+++-----+---+++-+--+-+++---+
--+-+--+---+--++++--+---++--+--++-+-+--------+-+-+++-+-++++----++--++----+++-----+---+++-+--+-+++---
+--+-+--+---+--++++--+---++--+--++-+-+--------+-+-+++-+-++++----++--++----+-+-----+---+++-+--+-+++--
-+--+-+--+---+--++++--+---++--+--++-+-+--------+-+++++-+-++++----++--++------+-----+---+++-+--+-+++-
--+--+-+--+---+--++++--+-+-++--+--++-+-+--------+--++++-+-++++----++--++------+-----+---+++-+--+-+++
---+--+-+--+---+--++++--+-+-++--+--++-+-+--------+--++++-+-++++----++--++--+---+-----+---+++-+--+-++
+---+--+-+--+---+--++++--+-+-++--+--++-+-+-----------++++-+-++++----++--++-++---+-----+---+++-+--+-+
-+---+--+-+--+---+--++++--+-+-++--+--++-+-+-----------++++-+-++++----++--+++++---+-----+---+++-+--+-
--+---+--+-+--+---+--++++--+-+-++--+--++-+-+------+----++++-+-++++----++--+-+++---+-----+---+++-+--+
+--+---+--+-+--+---+--+++---+-+-++--+--++-+-+-----++----++++-+-++++----++--+-+++---+-----+---+++-+--
++--+---+--+-+--+---+--++----+-+-++--+--++-+-+-----++----++++-+-++++----++--+-+++---+-----+---+++-+-
+++--+---+--+-+--+---+--+-----+-+-++--+--++-+-+-----++----++++-+-++++----++--+-+++---+-----+---+++-+
++++--+---+--+-+--+---+--------+-+-++--+--++-+-+--+--++----++++-+-++++----++--+-+++---+-----+---+++-
-++++--+---+--+-+--+---+--------+-+-++--+--++-+-+-++--++----++++-+-++++-----+--+-+++---+-----+---+++
--++++--+---+--+-+--+---+--------+-+-++--+--++-+-+-++--++----++++-+-++++---+-+--+-+++---+-----+---++
+--++++--+---+--+-+--+---+--------+-+-++--+--++-+---++--++----++++-+-++++--++-+--+-+++---+-----+---+
-+--++++--+---+--+-+--+---+--------+-+-++--+--++-+---++--++----++++-+-++++-+++-+--+-+++---+-----+---
--+--++++--+---+--+-+--+-+-+--------+-+-++--+--++-----++--++----++++-+-++++-+++-+--+-+++---+-----+--
---+--++++--+---+--+-+--+-+-+--------+-+-++--+--+++----++--++----++++-+-+++--+++-+--+-+++---+-----+-
+---+--++++--+---+--+-+--+-+-+--------+-+-++--+--+++----++--++----++++-+-++---+++-+--+-+++---+-----+
-+---+--++++--+---+--+-+-++-+-+--------+-+-++--+--+++----++--++----++++-+-++---+++-+--+-+++---+-----
--+---+--++++--+---+--+-+-++-+-+--------+-+-++--+-++++----++--++----++++-+--+---+++-+--+-+++---+----
+--+---+--++++--+---+--+---++-+-+--------+-+-++--+-++++----++--++----++++-+--+---+++-+--+-+++---+---
-++--+-+-++++++++-+-+--++-+--+---+--++++--+---+--++++-+++---+-++-+---+++-+++-++++----++--++----++++-
+-++--+-+-++++++++-+-+--++-+--+---+--++++--+---+--++++-+++---+-++-+---+++-+-+-++++----++--++----++++
++-++--+-+-++++++++-+-+---+-+--+---+--++++--+---+-+++++-+++---+-++-+---+++-+-+-++++----++--++----+++
-++-++--+-+-++++++++-+-+---+-+--+---+--++++--+---+-+++++-+++---+-++-+---+++++-+-++++----++--++----++
--++-++--+-+-++++++++-+-++--+-+--+---+--++++--+---+-+++++-+++---+-++-+---+++++-+-++++----++--++----+
+--++-++--+-+-++++++++-+--+--+-+--+---+--++++--+--++-+++++-+++---+-++-+---+++++-+-++++----++--++----
-+--++-++--+-+-++++++++-+--+--+-+--+---+--++++--+-+++-+++++-+++---+-++-+----++++-+-++++----++--++---
+-+--++-++--+-+-++++++++----+--+-+--+---+--++++--+-+++-+++++-+++---+-++-+----++++-+-++++----++--++--
-+-+--++-++--+-+-+++++++++---+--+-+--+---+--++++----+++-+++++-+++---+-++-+----++++-+-++++----++--++-
+-+-+--++-++--+-+-+++++++-+---+--+-+--+---+--++++----+++-+++++-+++---+-++-+----++++-+-++++----++--++
++-+-+--++-++--+-+-++++++--+---+--+-+--+---+--+++++---+++-+++++-+++---+-++-+----++++-+-++++----++--+
+++-+-+--++-++--+-+-++++++--+---+--+-+--+---+--+++-+---+++-+++++-+++---+-++++----++++-+-++++----++--
++++-+-+--++-++--+-+-++++++--+---+--+-+--+---+--+++-+---+++-+++++-+++---+-+-++----++++-+-++++----++-
+++++-+-+--++-++--+-+-++++++--+---+--+-+--+---+--+++-+---+++-+++++-+++---+---++----++++-+-++++----++
++++++-+-+--++-++--+-+-++++++--+---+--+-+--+---+---++-+---+++-+++++-+++---++--++----++++-+-++++----+
+++++++-+-+--++-++--+-+-+-++++--+---+--+-+--+---+-+-++-+---+++-+++++-+++---++--++----++++-+-++++----
++++++++-+-+--++-++--+-+---++++--+---+--+-+--+---+-+-++-+---+++-+++++-+++---++--++----++++-+-++++---
-++++++++-+-+--++-++--+-++--++++--+---+--+-+--+-----+-++-+---+++-+++++-+++---++--++----++++-+-++++--
+-++++++++-+-+--++-++--+--+--++++--+---+--+-+--+-----+-++-+---+++-+++++-+++---++--++----++++-+-++++-
-+-++++++++-+-+--++-++--+--+--++++--+---+--+-+--+-+---+-++-+---+++-+++++-++----++--++----++++-+-++++
+-+-++++++++-+-+--++-++-----+--++++--+---+--+-+--+++---+-++-+---+++-+++++-++----++--++----++++-+-+++
-+-+-++++++++-+-+--++-++-+---+--++++--+---+--+-+--+++---+-++-+---+++-+++++-++----++--++----++++-+-++
--+-+-++++++++-+-+--++-++-+---+--++++--+---+--+-+--+++---+-++-+---+++-++++++++----++--++----++++-+-+
+--+-+-++++++++-+-+--++-+--+---+--++++--+---+--+-++-+++---+-++-+---+++-++++++++----++--++----++++-+-
++--+-+-++++++++-+-+--++-+--+---+--++++--+---+--+-++-+++---+-++-+---+++-+++-++++----++--++----++++-+
</content>

<content full_path="exllamav3/util/hadamard_data/hadamard_244.txt">
++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--++---+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++
+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+--+++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-++
-+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+-++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+
--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-++++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-
+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+--+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+
-+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-++-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-
--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--+++-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---++++--+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+
+--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--+-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---+++++-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----
++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--+-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---+++-+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++---
-++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+-++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---++--+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++--
--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---+---+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++-
+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++-------+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++
-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++--+----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--++
+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++-++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+
-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--+++++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--
+-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--+-+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++-
++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+----+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++
+++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--+-++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+-+--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-+
++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+----++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-
-++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++--++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+
--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-+++-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--
+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-+-+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+-
-+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+----++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+---+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+
--+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+----++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---++--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-
---+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+--+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++----+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++
----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+--+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++--+-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------++
-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+--+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++-++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+
+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+---------+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---+++++++++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------
-+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+-----+---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---+++++-+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++-----
--+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+----++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++--+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++----
---+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+---+++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---+++---+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++---
----+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+--++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++----+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++--
-----+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+-+++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---+-----+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++-
------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++
+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+------++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+--+------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-++
-+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+------++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+-++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+
--+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+------++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-++++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-
---+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+--+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++--+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+
----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+--+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-+++-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--
-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-+-+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+-
+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+---+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+
-+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--++--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-
--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-+++++-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++---+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++
+--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-+++-+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++-+-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--+
++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--
+++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---+-++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++-
++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++-----++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++
-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++--+--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----++
+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++-++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+
-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-+++++++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----
+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-+++-+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+---
-+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++-++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++--+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+--
--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--+++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-+---+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+-
+--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--+++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+
++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+---++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-++----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-
-++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+--+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+
--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---++-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-
+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+----+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++
-+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+--+-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-++++++
--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+-++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++
+--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--++---+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-++++
--++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++-++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+----+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---
---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-+++++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+--
+---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-+-+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+-
++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++---+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+
-++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--+++--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-
+-++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--+-+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+
++-++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++----+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--+++-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-
-++-++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++-+--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--+-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---++++
--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--+-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---+++
+--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-+-++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+-++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---++
++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+---++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---+
-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-++--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++---
+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++----+--+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++--
-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++----++-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++-
+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++-----+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--++
-+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++---+-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--+
--+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++--++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+--
---+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++-+++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--+-++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+-
----+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+----++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-+
+----+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-+-++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++-
++----+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++---++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-++
-++----+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-++++++--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-+
+-++----+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-++++-+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+----++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+-
++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++--+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+----++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---+
+++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-++---+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+--+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++---
++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+--+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++--
+++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-++++++------+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+--+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++-
-+++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-+++++++-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+---------+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++++
+-+++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-+++++-+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+-----+---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---+++++
++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-++++--+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+----++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++++
+++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-+++---+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+---+++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---+++
++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-++----+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+--++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---++
+++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-+-----+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+-+++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---+
++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+---
-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-++++++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+------++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+--
+-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-++++-+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+------++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+-
++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-+++--+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+------++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-+
+++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-++---+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+--+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++-
++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-+----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+--+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-++
+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++------+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-+
-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----+++-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+-
+-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----+-+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--+
++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+------+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-+++++-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++--
-++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+---+--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-+++-+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++-
--++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+--++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---++
---++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+-+++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---+
----++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++---
+----++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+--++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++--
-+----++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-++-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++-
+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++--+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++++
-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--+++-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-+++
+-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--+-+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++-++---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-++
++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---++-++----+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--+++++---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-+
-++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---++-++-+--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--+++++---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+-
--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---++-++++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+---++++---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-+
+--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---++-+-++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+-
++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---++---++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---+
-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---+++--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+---
+-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---+-+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+--
++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++-----+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+-
-++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++--+--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--++---+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+---+
-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+--++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++-
+-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+-----++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++
++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+-+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+-+---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-+
+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+---+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-
-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-++--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++---++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++
+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++-+-++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--+
-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++++-++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--
+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--+-++-++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++-
-+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++---++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+----++-++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++
--+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++---++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+-+--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-+
---+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--+++---++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-
----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--++++--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+--++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-+
+----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--++-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-++-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++----+-
++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++--++-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++--+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++----+
+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++---+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--+++++-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++----
-+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++-+-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--+++-+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++---
--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++--+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++--
+--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--+---+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++-
++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+------+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-++
-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+--+-++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+-+----+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-+
+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+----++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+++----+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++-
-+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+-+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+------++----+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++++
--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++-+-+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+----+-++----+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-++++
+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++---+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+---++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+++
-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+++---+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+--+++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-++
+-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------++----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-+
++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------++++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-++++++-
+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+-------+++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-++++++
-+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+-----+-+++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-+++++
--+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+----++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-++++
---+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+---+++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-+++
----+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+--++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-++
-----+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+-+++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-+
------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++-
+------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+------++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-+++++
++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+-+-+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+----+-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-++++
+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+---+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+---++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-+++
-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+--+---+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+--+++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-++
+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+------+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-+
-+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-+------+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--++++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++-
--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++-++-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++---+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----++
+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++-+-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----+
-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+----
+-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-+++-++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+---
++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++--++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++--++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+--
-++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++-+++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-+---++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+-
--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----+++++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-----++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-+
+--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----++-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-++----++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+-
++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+----++-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+--+----++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-+
+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+-----+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--++-+----++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++-
-+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+---+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++---+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--++
--+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+---+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++-+-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--+
---+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-+---+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---++-++--
----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--+-++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---++-++-
+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+----++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---++-++
-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++-+-++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+-+--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---++-+
+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++++---++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---++-
-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-++++++++--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++---++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---++
+-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-++++++-+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++-+-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---+
++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++---
+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--++-++-++--++-+-+----++-+++++-++++++-+++++-++----+-+-++--++-++--
----+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+----+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+
-----+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+--+-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--
------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+-++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+-+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+-
-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-++++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+---+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+
+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+--+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-++--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--
-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-++-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++-
+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++++--+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++
-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---+++++-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--+
+-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---+++-+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++---++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--
++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---++--+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++---++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+-
+++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++---+---+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++---++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+
++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++-------+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--++++--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-
-++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++--+----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--++-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+
--++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--++-++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++--++-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-
---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--+++++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++---+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++
+---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+--+-+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++-+-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--+++
++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+----+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++
-++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+-+--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--+
--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++-+++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--
+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-++--++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+--+-++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+-
-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-+++-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+----++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+
+-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+-+-+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+-+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----
++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---+---+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++-+-+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+----
-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++---++--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++---+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+---
+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++----+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+++---+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+--
-+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++--+-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------++----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-
--+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++++-++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+
---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---+++++++++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------
+---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---+++++-+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+-----
++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++++--+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+----
+++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---+++---+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+---
++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---++----+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+--
+++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---+-----+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+-
++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+---------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+
-++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+--+------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+++------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----
--++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-+-++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+-+-+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+----
---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++-++++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+---+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+---
+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-++--+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+--+---+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+--
-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-+++-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+------+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-
+-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+-+-+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-+------+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+
++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--+---+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++-++-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--
-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++--++--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++-
+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++---+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++
-+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++-+-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-+++
--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---++++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++--++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++
+--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++---+-++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++-+++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-+
++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++-----++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----+++++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-
-++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++--+--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----++-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+
--++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++++-++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+----++-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-
---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-+++++++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+-----+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+
+---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-+++-+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+---+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--
++---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-++--+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+---+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++-
+++---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-+---+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-+---+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++
++++---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-+-----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--+
-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+-++----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--
+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+-------+--+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++-+-++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+-
-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+-------++-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+++---++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+
+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+--------+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-++++--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--
-+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+------+-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-++-+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++-
--+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+-----++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++
---+-+-++++---++--+-++-+---++++++---+-++-+--++---++++-+-+----+++-+-+----+++--++-+--+-+++------+++-+--+-++--+++----+-+-+++-+--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--+++--+--++--+-+-++++--+-----+------+-----+--++++-+-+--++--+--++
</content>

<content full_path="exllamav3/util/hadamard_data/hadamard_1.txt">
+
</content>

<content full_path="exllamav3/util/hadamard_data/hadamard_52.txt">
+-+--++++--+-+---++++++---++-+--++--+-++----+--+----
-+-+--++++--+-+---++++++--+++-+--++--+--+----+--+---
+-+-+--++++----+---++++++--+++-+--++--+--+----+--+--
-+-+-+--++++----+---+++++++-+++-+--++-----+----+--+-
--+-+-+--+++++---+---+++++-+-+++-+--++-----+----+--+
+--+-+-+--+++++---+---++++--+-+++-+--+++----+----+--
++--+-+-+--+++++---+---++++--+-+++-+--+-+----+----+-
+++--+-+-+--+++++---+---++++--+-+++-+----+----+----+
++++--+-+-+--+++++---+---+-++--+-+++-+-+--+----+----
-++++--+-+-+-++++++---+-----++--+-+++-+-+--+----+---
--++++--+-+-+-++++++---+--+--++--+-+++---+--+----+--
+--++++--+-+---++++++---+--+--++--+-+++---+--+----+-
-+--++++--+-+---++++++---++-+--++--+-++----+--+----+
-+++------++++-+--++++--+--++++-++-++++++-+--++--+-+
+-+++------++-+-+--++++--++-++++-++-++++++-+--++--+-
++-+++------++-+-+--++++--++-++++-++-++-+++-+--++--+
+++-+++-------+-+-+--++++-+++-++++-++-++-+++-+--++--
-+++-+++-------+-+-+--++++++++-++++-++--+-+++-+--++-
--+++-+++----+--+-+-+--+++-++++-++++-++--+-+++-+--++
---+++-+++---++--+-+-+--+++-++++-++++-++--+-+++-+--+
----+++-+++--+++--+-+-+--+++-++++-++++-++--+-+++-+--
-----+++-+++-++++--+-+-+---++-++++-++++-++--+-+++-+-
------+++-+++-++++--+-+-+-+-++-++++-+++--++--+-+++-+
+------+++-++--++++--+-+-+++-++-++++-+++--++--+-+++-
++------+++-++--++++--+-+-+++-++-++++-+-+--++--+-+++
+++------+++--+--++++--+-+++++-++-++++-+-+--++--+-++
--+-++--++-+-+----+--+----+-+--++++--+--+++------+++
---+-++--++-+-+----+--+----+-+--++++--++-+++------++
+---+-++--++---+----+--+--+-+-+--++++--++-+++------+
-+---+-++--++---+----+--+--+-+-+--++++-+++-+++------
+-+---+-++--+----+----+--+--+-+-+--++++-+++-+++-----
++-+---+-++--+----+----+--+--+-+-+--+++--+++-+++----
-++-+---+-++--+----+----+-++--+-+-+--++---+++-+++---
--++-+---+-++--+----+----++++--+-+-+--+----+++-+++--
+--++-+---+-++--+----+----++++--+-+-+-------+++-+++-
++--++-+---+--+--+----+----++++--+-+-+-------+++-+++
-++--++-+---+--+--+----+----++++--+-+-++------+++-++
+-++--++-+------+--+----+-+--++++--+-+-++------+++-+
-+-++--++-+------+--+----+-+--++++--+-++++------+++-
-++++-++-++++--+-++--++-+-+---++++++---+-+--++++--+-
+-++++-++-+++---+-++--++-+-+---++++++---+-+--++++--+
++-++++-++-+++---+-++--++---+---++++++-+-+-+--++++--
+++-++++-++-+-+---+-++--++---+---++++++-+-+-+--++++-
++++-++++-++-+-+---+-++--++---+---+++++--+-+-+--++++
-++++-++++-++++-+---+-++--++---+---+++++--+-+-+--+++
+-++++-++++-+-++-+---+-++-+++---+---+++++--+-+-+--++
++-++++-++++---++-+---+-++++++---+---+++++--+-+-+--+
-++-++++-+++++--++-+---+-++++++---+---+++++--+-+-+--
+-++-++++-+++++--++-+---+-++++++---+----++++--+-+-+-
++-++-++++-++-++--++-+---+-++++++---+----++++--+-+-+
+++-++-++++-++-++--++-+-----++++++---+-+--++++--+-+-
++++-++-++++--+-++--++-+-----++++++---+-+--++++--+-+
</content>

<content full_path="exllamav3/util/hadamard_data/hadamard_236.txt">
+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++---++-+---+-++---+--++++-++++-++-+---+-++++--++--+-++++-++++---++++-++++--+---++-+---+-+++-+-+++-+----++--++-+----+-----+--+-+++-+--+++-++----+----+-+-+---+-++++--++--+-++++-++++-
-+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++-++-+---+-++---+--++++-++++-++-+---+-++++--++--+-++++-++++---++++-++++--+---++-+---+-+++-+-+++-+----++--++-+----+--------+-+++-+--+++-++----+----+-+-+---+-++++--++--+-++++-++++-+
--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-+++++-+---+-++---+--++++-++++-++-+---+-++++--++--+-++++-++++--+++++-++++--+---++-+---+-+++-+-+++-+----++--++-+----+--------+-+++-+--+++-++----+----+-+-+---+-++++--++--+-++++-++++-+-
+--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-+++-+---+-++---+--++++-++++-++-+---+-++++--++--+-++++-++++--+++++-++++--+---++-+---+-+++-+-+++-+----++--++-+----+-------++-+++-+--+++-++----+----+-+-+---+-++++--++--+-++++-++++-+--
++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-+++---+-++---+--++++-++++-++-+---+-++++--++--+-++++-++++--++-++-++++--+---++-+---+-+++-+-+++-+----++--++-+----+-------++-+++-+--+++-++----+----+-+-+---+-++++--++--+-++++-++++-+--+
+++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-+---+-++---+--++++-++++-++-+---+-++++--++--+-++++-++++--++-++-++++--+---++-+---+-+++-+-+++-+----++--++-+----+-------++++++-+--+++-++----+----+-+-+---+-++++--++--+-++++-++++-+--+-
++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++---+-++---+--++++-++++-++-+---+-++++--++--+-++++-++++--++-+--++++--+---++-+---+-+++-+-+++-+----++--++-+----+-------++++++-+--+++-++----+----+-+-+---+-++++--++--+-++++-++++-+--+-+
-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-+-++---+--++++-++++-++-+---+-++++--++--+-++++-++++--++-+--++++--+---++-+---+-+++-+-+++-+----++--++-+----+-------++++-+-+--+++-++----+----+-+-+---+-++++--++--+-++++-++++-+--+-++
+-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++---+--++++-++++-++-+---+-++++--++--+-++++-++++--++-+---+++--+---++-+---+-+++-+-+++-+----++--++-+----+-------++++-+-+--+++-++----+----+-+-+---+-++++--++--+-++++-++++-+--+-+++
++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++-++---+--++++-++++-++-+---+-++++--++--+-++++-++++--++-+---+++--+---++-+---+-+++-+-+++-+----++--++-+----+-------++++-+++--+++-++----+----+-+-+---+-++++--++--+-++++-++++-+--+-+++-
+++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--+++---+--++++-++++-++-+---+-++++--++--+-++++-++++--++-+---+-+--+---++-+---+-+++-+-+++-+----++--++-+----+-------++++-+++--+++-++----+----+-+-+---+-++++--++--+-++++-++++-+--+-+++-+
++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--+---+--++++-++++-++-+---+-++++--++--+-++++-++++--++-+---+-+--+---++-+---+-+++-+-+++-+----++--++-+----+-------++++-++++-+++-++----+----+-+-+---+-++++--++--+-++++-++++-+--+-+++-+-
-++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+----+--++++-++++-++-+---+-++++--++--+-++++-++++--++-+---+-++-+---++-+---+-+++-+-+++-+----++--++-+----+-------++++-++++-+++-++----+----+-+-+---+-++++--++--+-++++-++++-+--+-+++-+--
--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--+--++++-++++-++-+---+-++++--++--+-++++-++++--++-+---+-++-+---++-+---+-+++-+-+++-+----++--++-+----+-------++++-++++--++-++----+----+-+-+---+-++++--++--+-++++-++++-+--+-+++-+--+
+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++----+--++++-++++-++-+---+-++++--++--+-++++-++++--++-+---+-++-----++-+---+-+++-+-+++-+----++--++-+----+-------++++-++++--++-++----+----+-+-+---+-++++--++--+-++++-++++-+--+-+++-+--++
-+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++--+--++++-++++-++-+---+-++++--++--+-++++-++++--++-+---+-++-----++-+---+-+++-+-+++-+----++--++-+----+-------++++-++++--+--++----+----+-+-+---+-++++--++--+-++++-++++-+--+-+++-+--+++
--+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---++++-++++-++-+---+-++++--++--+-++++-++++--++-+---+-++---+-++-+---+-+++-+-+++-+----++--++-+----+-------++++-++++--+--++----+----+-+-+---+-++++--++--+-++++-++++-+--+-+++-+--+++-
---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++-++++-++++-++-+---+-++++--++--+-++++-++++--++-+---+-++---+-++-+---+-+++-+-+++-+----++--++-+----+-------++++-++++--+---+----+----+-+-+---+-++++--++--+-++++-++++-+--+-+++-+--+++-+
+---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-+++++-++++-++-+---+-++++--++--+-++++-++++--++-+---+-++---+--+-+---+-+++-+-+++-+----++--++-+----+-------++++-++++--+---+----+----+-+-+---+-++++--++--+-++++-++++-+--+-+++-+--+++-++
++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-+++-++++-++-+---+-++++--++--+-++++-++++--++-+---+-++---+--+-+---+-+++-+-+++-+----++--++-+----+-------++++-++++--+---++---+----+-+-+---+-++++--++--+-++++-++++-+--+-+++-+--+++-++-
-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+++-++++-++-+---+-++++--++--+-++++-++++--++-+---+-++---+--+++---+-+++-+-+++-+----++--++-+----+-------++++-++++--+---++---+----+-+-+---+-++++--++--+-++++-++++-+--+-+++-+--+++-++--
+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++++-++-+---+-++++--++--+-++++-++++--++-+---+-++---+--+++---+-+++-+-+++-+----++--++-+----+-------++++-++++--+---++-+-+----+-+-+---+-++++--++--+-++++-++++-+--+-+++-+--+++-++---
-+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---++++-++-+---+-++++--++--+-++++-++++--++-+---+-++---+--++++--+-+++-+-+++-+----++--++-+----+-------++++-++++--+---++-+-+----+-+-+---+-++++--++--+-++++-++++-+--+-+++-+--+++-++----
--+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+-++++-++-+---+-++++--++--+-++++-++++--++-+---+-++---+--++++--+-+++-+-+++-+----++--++-+----+-------++++-++++--+---++-+------+-+-+---+-++++--++--+-++++-++++-+--+-+++-+--+++-++----+
---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-++++-++-+---+-++++--++--+-++++-++++--++-+---+-++---+--++++-++-+++-+-+++-+----++--++-+----+-------++++-++++--+---++-+------+-+-+---+-++++--++--+-++++-++++-+--+-+++-+--+++-++----+-
+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-++-++-+---+-++++--++--+-++++-++++--++-+---+-++---+--++++-++-+++-+-+++-+----++--++-+----+-------++++-++++--+---++-+---+--+-+-+---+-++++--++--+-++++-++++-+--+-+++-+--+++-++----+--
-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-+++++-++-+---+-++++--++--+-++++-++++--++-+---+-++---+--++++-++++++-+-+++-+----++--++-+----+-------++++-++++--+---++-+---+--+-+-+---+-++++--++--+-++++-++++-+--+-+++-+--+++-++----+---
+-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-+++-++-+---+-++++--++--+-++++-++++--++-+---+-++---+--++++-++++++-+-+++-+----++--++-+----+-------++++-++++--+---++-+---+-++-+-+---+-++++--++--+-++++-++++-+--+-+++-+--+++-++----+----
++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++++--++--+-++++-++++--++-+---+-++---+--++++-++++-+-+-+++-+----++--++-+----+-------++++-++++--+---++-+---+-++-+-+---+-++++--++--+-++++-++++-+--+-+++-+--+++-++----+----+
+++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++-+---+-++++--++--+-++++-++++--++-+---+-++---+--++++-++++-+-+-+++-+----++--++-+----+-------++++-++++--+---++-+---+-++++-+---+-++++--++--+-++++-++++-+--+-+++-+--+++-++----+----+-
++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+--+---+-++++--++--+-++++-++++--++-+---+-++---+--++++-++++-+++-+++-+----++--++-+----+-------++++-++++--+---++-+---+-+++--+---+-++++--++--+-++++-++++-+--+-+++-+--+++-++----+----+-+
-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---++---+-++++--++--+-++++-++++--++-+---+-++---+--++++-++++-++--+++-+----++--++-+----+-------++++-++++--+---++-+---+-+++-++---+-++++--++--+-++++-++++-+--+-+++-+--+++-++----+----+-+-
+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+------+-++++--++--+-++++-++++--++-+---+-++---+--++++-++++-++-++++-+----++--++-+----+-------++++-++++--+---++-+---+-+++-+----+-++++--++--+-++++-++++-+--+-+++-+--+++-++----+----+-+-+
-+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+----+-++++--++--+-++++-++++--++-+---+-++---+--++++-++++-++-+-++-+----++--++-+----+-------++++-++++--+---++-+---+-+++-+-+--+-++++--++--+-++++-++++-+--+-+++-+--+++-++----+----+-+-+-
--+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+--+-++++--++--+-++++-++++--++-+---+-++---+--++++-++++-++-+--+-+----++--++-+----+-------++++-++++--+---++-+---+-+++-+-++-+-++++--++--+-++++-++++-+--+-+++-+--+++-++----+----+-+-+--
---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-++-++++--++--+-++++-++++--++-+---+-++---+--++++-++++-++-+----+----++--++-+----+-------++++-++++--+---++-+---+-+++-+-++++-++++--++--+-++++-++++-+--+-+++-+--+++-++----+----+-+-+---
+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++--++++--++--+-++++-++++--++-+---+-++---+--++++-++++-++-+---++----++--++-+----+-------++++-++++--+---++-+---+-+++-+-+++--++++--++--+-++++-++++-+--+-+++-+--+++-++----+----+-+-+---+
-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++++++--++--+-++++-++++--++-+---+-++---+--++++-++++-++-+---+-----++--++-+----+-------++++-++++--+---++-+---+-+++-+-+++-+++++--++--+-++++-++++-+--+-+++-+--+++-++----+----+-+-+---+-
+-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++++--++--+-++++-++++--++-+---+-++---+--++++-++++-++-+---+-+---++--++-+----+-------++++-++++--+---++-+---+-+++-+-+++-+-+++--++--+-++++-++++-+--+-+++-+--+++-++----+----+-+-+---+-+
++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++--++--+-++++-++++--++-+---+-++---+--++++-++++-++-+---+-++--++--++-+----+-------++++-++++--+---++-+---+-+++-+-+++-+--++--++--+-++++-++++-+--+-+++-+--+++-++----+----+-+-+---+-++
+++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++--++--+-++++-++++--++-+---+-++---+--++++-++++-++-+---+-+++-++--++-+----+-------++++-++++--+---++-+---+-+++-+-+++-+---+--++--+-++++-++++-+--+-+++-+--+++-++----+----+-+-+---+-+++
++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++----++--+-++++-++++--++-+---+-++---+--++++-++++-++-+---+-++++++--++-+----+-------++++-++++--+---++-+---+-+++-+-+++-+------++--+-++++-++++-+--+-+++-+--+++-++----+----+-+-+---+-++++
-++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++--+-++++-++++--++-+---+-++---+--++++-++++-++-+---+-++++-+--++-+----+-------++++-++++--+---++-+---+-+++-+-+++-+----+-++--+-++++-++++-+--+-+++-+--+++-++----+----+-+-+---+-++++-
--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++++--+-++++-++++--++-+---+-++---+--++++-++++-++-+---+-++++----++-+----+-------++++-++++--+---++-+---+-+++-+-+++-+----++++--+-++++-++++-+--+-+++-+--+++-++----+----+-+-+---+-++++--
+--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--+-++++-++++--++-+---+-++---+--++++-++++-++-+---+-++++--+-++-+----+-------++++-++++--+---++-+---+-+++-+-+++-+----++-+--+-++++-++++-+--+-+++-+--+++-++----+----+-+-+---+-++++--+
++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+----+-++++-++++--++-+---+-++---+--++++-++++-++-+---+-++++--++++-+----+-------++++-++++--+---++-+---+-+++-+-+++-+----++----+-++++-++++-+--+-+++-+--+++-++----+----+-+-+---+-++++--++
-++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--+-++++-++++--++-+---+-++---+--++++-++++-++-+---+-++++--++-+-+----+-------++++-++++--+---++-+---+-+++-+-+++-+----++--+-+-++++-++++-+--+-+++-+--+++-++----+----+-+-+---+-++++--++-
--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-++-++++-++++--++-+---+-++---+--++++-++++-++-+---+-++++--++---+----+-------++++-++++--+---++-+---+-+++-+-+++-+----++--+++-++++-++++-+--+-+++-+--+++-++----+----+-+-+---+-++++--++--
+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++--++++-++++--++-+---+-++---+--++++-++++-++-+---+-++++--++--++----+-------++++-++++--+---++-+---+-+++-+-+++-+----++--++--++++-++++-+--+-+++-+--+++-++----+----+-+-+---+-++++--++--+
-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++++++-++++--++-+---+-++---+--++++-++++-++-+---+-++++--++--+-----+-------++++-++++--+---++-+---+-+++-+-+++-+----++--++-+++++-++++-+--+-+++-+--+++-++----+----+-+-+---+-++++--++--+-
+-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++++-++++--++-+---+-++---+--++++-++++-++-+---+-++++--++--+-+---+-------++++-++++--+---++-+---+-+++-+-+++-+----++--++-+-+++-++++-+--+-+++-+--+++-++----+----+-+-+---+-++++--++--+-+
++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-++++--++-+---+-++---+--++++-++++-++-+---+-++++--++--+-++--+-------++++-++++--+---++-+---+-+++-+-+++-+----++--++-+--++-++++-+--+-+++-+--+++-++----+----+-+-+---+-++++--++--+-++
+++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++-++++--++-+---+-++---+--++++-++++-++-+---+-++++--++--+-+++-+-------++++-++++--+---++-+---+-+++-+-+++-+----++--++-+---+-++++-+--+-+++-+--+++-++----+----+-+-+---+-++++--++--+-+++
++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++--++++--++-+---+-++---+--++++-++++-++-+---+-++++--++--+-+++++-------++++-++++--+---++-+---+-+++-+-+++-+----++--++-+-----++++-+--+-+++-+--+++-++----+----+-+-+---+-++++--++--+-++++
-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++++++--++-+---+-++---+--++++-++++-++-+---+-++++--++--+-++++--------++++-++++--+---++-+---+-+++-+-+++-+----++--++-+----+++++-+--+-+++-+--+++-++----+----+-+-+---+-++++--++--+-++++-
+-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++++--++-+---+-++---+--++++-++++-++-+---+-++++--++--+-++++-+------++++-++++--+---++-+---+-+++-+-+++-+----++--++-+----+-+++-+--+-+++-+--+++-++----+----+-+-+---+-++++--++--+-++++-+
++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++--++-+---+-++---+--++++-++++-++-+---+-++++--++--+-++++-++-----++++-++++--+---++-+---+-+++-+-+++-+----++--++-+----+--++-+--+-+++-+--+++-++----+----+-+-+---+-++++--++--+-++++-++
+++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++--++-+---+-++---+--++++-++++-++-+---+-++++--++--+-++++-+++----++++-++++--+---++-+---+-+++-+-+++-+----++--++-+----+---+-+--+-+++-+--+++-++----+----+-+-+---+-++++--++--+-++++-+++
++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+--++-+---+-++---+--++++-++++-++-+---+-++++--++--+-++++-++++---++++-++++--+---++-+---+-+++-+-+++-+----++--++-+----+-----+--+-+++-+--+++-++----+----+-+-+---+-++++--++--+-++++-++++
+--+-+++-+--+++-++----+----+--+-+++-+----++--++-+----+----++++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++------+----+-++--++----+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++---
--+-+++-+--+++-++----+----+--+-+++-+----++--++-+----+----++-+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++----+----+-++--++----+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++----
-+-+++-+--+++-++----+----+--+-+++-+----++--++-+----+----++---+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+----+-++--++----+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-----
+-+++-+--+++-++----+----+--+-+++-+----++--++-+----+----++--+--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-+++-+----+-++--++----+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++------
-+++-+--+++-++----+----+--+-+++-+----++--++-+----+----++--+++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-+++----+-++--++----+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------
+++-+--+++-++----+----+--+-+++-+----++--++-+----+----++--+-+++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-+----+-++--++----+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+
++-+--+++-++----+----+--+-+++-+----++--++-+----+----++--+-+++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++----+-++--++----+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+-
+-+--+++-++----+----+--+-+++-+----++--++-+----+----++--+-++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++--+-++--++----+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+--
-+--+++-++----+----+--+-+++-+----++--++-+----+----++--+-++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--+++-+-++--++----+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+---
+--+++-++----+----+--+-+++-+----++--++-+----+----++--+-+++-++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--+++-++--++----+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----
--+++-++----+----+--+-+++-+----++--++-+----+----++--+-+++-++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--+-++--++----+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+
-+++-++----+----+--+-+++-+----++--++-+----+----++--+-+++-+-++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++--++----+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-
+++-++----+----+--+-+++-+----++--++-+----+----++--+-+++-+---++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+-+--++----+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-+
++-++----+----+--+-+++-+----++--++-+----+----++--+-+++-+--+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++----+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-++
+-++----+----+--+-+++-+----++--++-+----+----++--+-+++-+--+++--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++----++----+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-++-
-++----+----+--+-+++-+----++--++-+----+----++--+-+++-+--+++-+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++--++----+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-++--
++----+----+--+-+++-+----++--++-+----+----++--+-+++-+--+++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++-+----+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-++--+
+----+----+--+-+++-+----++--++-+----+----++--+-+++-+--+++-+---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++----+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-++--++
----+----+--+-+++-+----++--++-+----+----++--+-+++-+--+++-+++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-+---+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-++--++-
---+----+--+-+++-+----++--++-+----+----++--+-+++-+--+++-++-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+---+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-++--++--
--+----+--+-+++-+----++--++-+----+----++--+-+++-+--+++-++---++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-++--++---
-+----+--+-+++-+----++--++-+----+----++--+-+++-+--+++-++---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-++--++----
+----+--+-+++-+----++--++-+----+----++--+-+++-+--+++-++-----+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-++--++----+
----+--+-+++-+----++--++-+----+----++--+-+++-+--+++-++----+--+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-++--++----+-
---+--+-+++-+----++--++-+----+----++--+-+++-+--+++-++----+----+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-++--++----+-+
--+--+-+++-+----++--++-+----+----++--+-+++-+--+++-++----+--+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-++--++----+-++
-+--+-+++-+----++--++-+----+----++--+-+++-+--+++-++----+----+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-++--++----+-+++
+--+-+++-+----++--++-+----+----++--+-+++-+--+++-++----+----+-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-++--++----+-+++-
--+-+++-+----++--++-+----+----++--+-+++-+--+++-++----+----+++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++-+-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-++--++----+-+++-+
-+-+++-+----++--++-+----+----++--+-+++-+--+++-++----+----+-+++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-++--++----+-+++-+-
+-+++-+----++--++-+----+----++--+-+++-+--+++-++----+----+--++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+--++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-------+----+-++--++----+-+++-+-+
-+++-+----++--++-+----+----++--+-+++-+--+++-++----+----+--+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+-+-+---+-++---+--++++-++++-------+----+-++--++----+-+++-+-++
+++-+----++--++-+----+----++--+-+++-+--+++-++----+----+--+-+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+-+-+---+-++---+--++++-++++-------+----+-++--++----+-+++-+-+++
++-+----++--++-+----+----++--+-+++-+--+++-++----+----+--+-+-+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+--++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+-+++---+-++---+--++++-++++-------+----+-++--++----+-+++-+-+++-
+-+----++--++-+----+----++--+-+++-+--+++-++----+----+--+-++--+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+-+-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+-+++---+-++---+--++++-++++-------+----+-++--++----+-+++-+-+++-+
-+----++--++-+----+----++--+-+++-+--+++-++----+----+--+-+++---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+-++++--+-++---+--++++-++++-------+----+-++--++----+-+++-+-+++-+-
+----++--++-+----+----++--+-+++-+--+++-++----+----+--+-+++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+-++++--+-++---+--++++-++++-------+----+-++--++----+-+++-+-+++-+--
----++--++-+----+----++--+-+++-+--+++-++----+----+--+-+++-+-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--+++++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+-++++-++-++---+--++++-++++-------+----+-++--++----+-+++-+-+++-+---
---++--++-+----+----++--+-+++-+--+++-++----+----+--+-+++-+-+-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--+++++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+-++++-++-++---+--++++-++++-------+----+-++--++----+-+++-+-+++-+---+
--++--++-+----+----++--+-+++-+--+++-++----+----+--+-+++-+--++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--+++--+---++-+---+-++-+----+----+-++--++----+-+++-+-+-++++-+++++---+--++++-++++-------+----+-++--++----+-+++-+-+++-+---+-
-++--++-+----+----++--+-+++-+--+++-++----+----+--+-+++-+---+++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--+--+---++-+---+-++-+----+----+-++--++----+-+++-+-+-++++-+++++---+--++++-++++-------+----+-++--++----+-+++-+-+++-+---+-+
++--++-+----+----++--+-+++-+--+++-++----+----+--+-+++-+----++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++---+---++-+---+-++-+----+----+-++--++----+-+++-+-+-++++-++++----+--++++-++++-------+----+-++--++----+-+++-+-+++-+---+-++
+--++-+----+----++--+-+++-+--+++-++----+----+--+-+++-+----+-++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++-+---++-+---+-++-+----+----+-++--++----+-+++-+-+-++++-++++----+--++++-++++-------+----+-++--++----+-+++-+-+++-+---+-++-
--++-+----+----++--+-+++-+--+++-++----+----+--+-+++-+----++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++---++-+---+-++-+----+----+-++--++----+-+++-+-+-++++-++++--+-+--++++-++++-------+----+-++--++----+-+++-+-+++-+---+-++--
-++-+----+----++--+-+++-+--+++-++----+----+--+-+++-+----++-+--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--+--++-+---+-++-+----+----+-++--++----+-+++-+-+-++++-++++--+-+--++++-++++-------+----+-++--++----+-+++-+-+++-+---+-++---
++-+----+----++--+-+++-+--+++-++----+----+--+-+++-+----++--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+---++-+---+-++-+----+----+-++--++----+-+++-+-+-++++-++++--+----++++-++++-------+----+-++--++----+-+++-+-+++-+---+-++---+
+-+----+----++--+-+++-+--+++-++----+----+--+-+++-+----++--+-++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+-++-+---+-++-+----+----+-++--++----+-+++-+-+-++++-++++--+----++++-++++-------+----+-++--++----+-+++-+-+++-+---+-++---+-
-+----+----++--+-+++-+--+++-++----+----+--+-+++-+----++--++--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-++-+---+-++-+----+----+-++--++----+-+++-+-+-++++-++++--+---+++++-++++-------+----+-++--++----+-+++-+-+++-+---+-++---+--
+----+----++--+-+++-+--+++-++----+----+--+-+++-+----++--++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++--+---+-++-+----+----+-++--++----+-+++-+-+-++++-++++--+---+++++-++++-------+----+-++--++----+-+++-+-+++-+---+-++---+--+
----+----++--+-+++-+--+++-++----+----+--+-+++-+----++--++-+-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-+++++---+-++-+----+----+-++--++----+-+++-+-+-++++-++++--+---++-++-++++-------+----+-++--++----+-+++-+-+++-+---+-++---+--++
---+----++--+-+++-+--+++-++----+----+--+-+++-+----++--++-+-+-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-+++---+-++-+----+----+-++--++----+-+++-+-+-++++-++++--+---++-++-++++-------+----+-++--++----+-+++-+-+++-+---+-++---+--+++
--+----++--+-+++-+--+++-++----+----+--+-+++-+----++--++-+--++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++--+-++-+----+----+-++--++----+-+++-+-+-++++-++++--+---++-+--++++-------+----+-++--++----+-+++-+-+++-+---+-++---+--++++
-+----++--+-+++-+--+++-++----+----+--+-+++-+----++--++-+---+++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-+-+-++-+----+----+-++--++----+-+++-+-+-++++-++++--+---++-+--++++-------+----+-++--++----+-+++-+-+++-+---+-++---+--++++-
+----++--+-+++-+--+++-++----+----+--+-+++-+----++--++-+----++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-+-++-+----+----+-++--++----+-+++-+-+-++++-++++--+---++-+---+++-------+----+-++--++----+-+++-+-+++-+---+-++---+--++++-+
----++--+-+++-+--+++-++----+----+--+-+++-+----++--++-+----+-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++-+----+----+-++--++----+-+++-+-+-++++-++++--+---++-+---+++-------+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++
---++--+-+++-+--+++-++----+----+--+-+++-+----++--++-+----+-+-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--++++++-+----+----+-++--++----+-+++-+-+-++++-++++--+---++-+---+-+-------+----+-++--++----+-+++-+-+++-+---+-++---+--++++-+++
--++--+-+++-+--+++-++----+----+--+-+++-+----++--++-+----+--++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--++++-+----+----+-++--++----+-+++-+-+-++++-++++--+---++-+---+-+-------+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++
-++--+-+++-+--+++-++----+----+--+-+++-+----++--++-+----+---+++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--++-+----+----+-++--++----+-+++-+-+-++++-++++--+---++-+---+-++------+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++-
++--+-+++-+--+++-++----+----+--+-+++-+----++--++-+----+----++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--++----+----+-++--++----+-+++-+-+-++++-++++--+---++-+---+-++------+----+-++--++----+-+++-+-+++-+---+-++---+--++++-++++--
++----+----++-+++--+-+++-+---+-+---+-++++--++--+-++++-+++++++++-++++-+--++--++++-+---+-+-+----+----++-+++--+-+++-+--+-+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++------+----+-++--++----+-+++-+--+----+----++-+++--+-+++-+--++
+----+----++-+++--+-+++-+---+-+---+-++++--++--+-++++-+++++++++-++++-+--++--++++-+---+-+-+----+----++-+++--+-+++-+--+-+-+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++----+----+-++--++----+-+++-+--+----+----++-+++--+-+++-+--++-
----+----++-+++--+-+++-+---+-+---+-++++--++--+-++++-+++++++++-++++-+--++--++++-+---+-+-+----+----++-+++--+-+++-+--+-++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+----+-++--++----+-+++-+--+----+----++-+++--+-+++-+--++--
---+----++-+++--+-+++-+---+-+---+-++++--++--+-++++-+++++++-+-++++-+--++--++++-+---+-+-+----+----++-+++--+-+++-+--+-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-+++-+----+-++--++----+-+++-+--+----+----++-+++--+-+++-+--++---
--+----++-+++--+-+++-+---+-+---+-++++--++--+-++++-+++++++---++++-+--++--++++-+---+-+-+----+----++-+++--+-+++-+--+-++++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-+++----+-++--++----+-+++-+--+----+----++-+++--+-+++-+--++----
-+----++-+++--+-+++-+---+-+---+-++++--++--+-++++-+++++++---++++-+--++--++++-+---+-+-+----+----++-+++--+-+++-+--+-++++-+++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-+----+-++--++----+-+++-+--+----+----++-+++--+-+++-+--++----+
+----++-+++--+-+++-+---+-+---+-++++--++--+-++++-+++++++----+++-+--++--++++-+---+-+-+----+----++-+++--+-+++-+--+-++++-+++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++----+-++--++----+-+++-+--+----+----++-+++--+-+++-+--++----+-
----++-+++--+-+++-+---+-+---+-++++--++--+-++++-+++++++----+++-+--++--++++-+---+-+-+----+----++-+++--+-+++-+--+-++++-++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++--+-++--++----+-+++-+--+----+----++-+++--+-+++-+--++----+--
---++-+++--+-+++-+---+-+---+-++++--++--+-++++-+++++++----+-+-+--++--++++-+---+-+-+----+----++-+++--+-+++-+--+-++++-++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--+++-+-++--++----+-+++-+--+----+----++-+++--+-+++-+--++----+---
--++-+++--+-+++-+---+-+---+-++++--++--+-++++-+++++++----+---+--++--++++-+---+-+-+----+----++-+++--+-+++-+--+-++++-++++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--+++-++--++----+-+++-+--+----+----++-+++--+-+++-+--++----+----
-++-+++--+-+++-+---+-+---+-++++--++--+-++++-+++++++----+---+--++--++++-+---+-+-+----+----++-+++--+-+++-+--+-++++-++++-+++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--+-++--++----+-+++-+--+----+----++-+++--+-+++-+--++----+----+
++-+++--+-+++-+---+-+---+-++++--++--+-++++-+++++++----+------++--++++-+---+-+-+----+----++-+++--+-+++-+--+-++++-++++-+++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++--++----+-+++-+--+----+----++-+++--+-+++-+--++----+----+-
+-+++--+-+++-+---+-+---+-++++--++--+-++++-+++++++----+----+-++--++++-+---+-+-+----+----++-+++--+-+++-+--+-++++-++++-+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+-+--++----+-+++-+--+----+----++-+++--+-+++-+--++----+----+-+
-+++--+-+++-+---+-+---+-++++--++--+-++++-+++++++----+----++++--++++-+---+-+-+----+----++-+++--+-+++-+--+-++++-++++-+----++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++----+-+++-+--+----+----++-+++--+-+++-+--++----+----+-++
+++--+-+++-+---+-+---+-++++--++--+-++++-+++++++----+----++-+--++++-+---+-+-+----+----++-+++--+-+++-+--+-++++-++++-+--++--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++----++----+-+++-+--+----+----++-+++--+-+++-+--++----+----+-++-
++--+-+++-+---+-+---+-++++--++--+-++++-+++++++----+----++-+--++++-+---+-+-+----+----++-+++--+-+++-+--+-++++-++++-+--++-+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++--++----+-+++-+--+----+----++-+++--+-+++-+--++----+----+-++--
+--+-+++-+---+-+---+-++++--++--+-++++-+++++++----+----++-++-++++-+---+-+-+----+----++-+++--+-+++-+--+-++++-++++-+--++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++-+----+-+++-+--+----+----++-+++--+-+++-+--++----+----+-++--+
--+-+++-+---+-+---+-++++--++--+-++++-+++++++----+----++-+++++++-+---+-+-+----+----++-+++--+-+++-+--+-++++-++++-+--++-----+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++----+-+++-+--+----+----++-+++--+-+++-+--++----+----+-++--++
-+-+++-+---+-+---+-++++--++--+-++++-+++++++----+----++-+++-+++-+---+-+-+----+----++-+++--+-+++-+--+-++++-++++-+--++--++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-+---+-+++-+--+----+----++-+++--+-+++-+--++----+----+-++--++-
+-+++-+---+-+---+-++++--++--+-++++-+++++++----+----++-+++--++-+---+-+-+----+----++-+++--+-+++-+--+-++++-++++-+--++--++++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+---+-+++-+--+----+----++-+++--+-+++-+--++----+----+-++--++--
-+++-+---+-+---+-++++--++--+-++++-+++++++----+----++-+++--++-+---+-+-+----+----++-+++--+-+++-+--+-++++-++++-+--++--+++-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-+-+++-+--+----+----++-+++--+-+++-+--++----+----+-++--++---
+++-+---+-+---+-++++--++--+-++++-+++++++----+----++-+++--+--+---+-+-+----+----++-+++--+-+++-+--+-++++-++++-+--++--+++++-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-+++-+--+----+----++-+++--+-+++-+--++----+----+-++--++----
++-+---+-+---+-++++--++--+-++++-+++++++----+----++-+++--+-++---+-+-+----+----++-+++--+-+++-+--+-++++-++++-+--++--++++--+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+++-+--+----+----++-+++--+-+++-+--++----+----+-++--++----+
+-+---+-+---+-++++--++--+-++++-+++++++----+----++-+++--+-++---+-+-+----+----++-+++--+-+++-+--+-++++-++++-+--++--++++-+--+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+-+++-+--+----+----++-+++--+-+++-+--++----+----+-++--++----+-
-+---+-+---+-++++--++--+-++++-+++++++----+----++-+++--+-+++--+-+-+----+----++-+++--+-+++-+--+-++++-++++-+--++--++++-+----+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+++-+--+----+----++-+++--+-+++-+--++----+----+-++--++----+-+
+---+-+---+-++++--++--+-++++-+++++++----+----++-+++--+-+++--+-+-+----+----++-+++--+-+++-+--+-++++-++++-+--++--++++-+--+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+-+--+----+----++-+++--+-+++-+--++----+----+-++--++----+-++
---+-+---+-++++--++--+-++++-+++++++----+----++-+++--+-+++-++-+-+----+----++-+++--+-+++-+--+-++++-++++-+--++--++++-+----+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+--+----+----++-+++--+-+++-+--++----+----+-++--++----+-+++
--+-+---+-++++--++--+-++++-+++++++----+----++-+++--+-+++-+--+-+----+----++-+++--+-+++-+--+-++++-++++-+--++--++++-+---++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++--+----+----++-+++--+-+++-+--++----+----+-++--++----+-+++-
-+-+---+-++++--++--+-++++-+++++++----+----++-+++--+-+++-+--+-+----+----++-+++--+-+++-+--+-++++-++++-+--++--++++-+---+-++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++--+----+----++-+++--+-+++-+--++----+----+-++--++----+-+++-+
+-+---+-++++--++--+-++++-+++++++----+----++-+++--+-+++-+----+----+----++-+++--+-+++-+--+-++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-+-+----+----++-+++--+-+++-+--++----+----+-++--++----+-+++-+-
-+---+-++++--++--+-++++-+++++++----+----++-+++--+-+++-+---++----+----++-+++--+-+++-+--+-++++-++++-+--++--++++-+---+-+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-+----+----++-+++--+-+++-+--++----+----+-++--++----+-+++-+--
+---+-++++--++--+-++++-+++++++----+----++-+++--+-+++-+---+-----+----++-+++--+-+++-+--+-++++-++++-+--++--++++-+---+-+-+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+----+----++-+++--+-+++-+--++----+----+-++--++----+-+++-+--+
---+-++++--++--+-++++-+++++++----+----++-+++--+-+++-+---+-+---+----++-+++--+-+++-+--+-++++-++++-+--++--++++-+---+-+-+-+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+------+----++-+++--+-+++-+--++----+----+-++--++----+-+++-+--+-
--+-++++--++--+-++++-+++++++----+----++-+++--+-+++-+---+-+---+----++-+++--+-+++-+--+-++++-++++-+--++--++++-+---+-+-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+----+----++-+++--+-+++-+--++----+----+-++--++----+-+++-+--+--
-+-++++--++--+-++++-+++++++----+----++-+++--+-+++-+---+-+---+----++-+++--+-+++-+--+-++++-++++-+--++--++++-+---+-+-+-----+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+--+----++-+++--+-+++-+--++----+----+-++--++----+-+++-+--+---
+-++++--++--+-++++-+++++++----+----++-+++--+-+++-+---+-+---+----++-+++--+-+++-+--+-++++-++++-+--++--++++-+---+-+-+-------+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-++----++-+++--+-+++-+--++----+----+-++--++----+-+++-+--+----
-++++--++--+-++++-+++++++----+----++-+++--+-+++-+---+-+---+----++-+++--+-+++-+--+-++++-++++-+--++--++++-+---+-+-+----++---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-----++-+++--+-+++-+--++----+----+-++--++----+-+++-+--+----+
++++--++--+-++++-+++++++----+----++-+++--+-+++-+---+-+---+----++-+++--+-+++-+--+-++++-++++-+--++--++++-+---+-+-+----+--+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++---++-+++--+-+++-+--++----+----+-++--++----+-+++-+--+----+-
+++--++--+-++++-+++++++----+----++-+++--+-+++-+---+-+---+-+--++-+++--+-+++-+--+-++++-++++-+--++--++++-+---+-+-+----+--+-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--+++--++-+++--+-+++-+--++----+----+-++--++----+-+++-+--+----+--
++--++--+-++++-+++++++----+----++-+++--+-+++-+---+-+---+-++-++-+++--+-+++-+--+-++++-++++-+--++--++++-+---+-+-+----+---++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++-++-+++--+-+++-+--++----+----+-++--++----+-+++-+--+----+---
+--++--+-++++-+++++++----+----++-+++--+-+++-+---+-+---+-+++++-+++--+-+++-+--+-++++-++++-+--++--++++-+---+-+-+----+----+++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--+++-+++--+-+++-+--++----+----+-++--++----+-+++-+--+----+----
--++--+-++++-+++++++----+----++-+++--+-+++-+---+-+---+-+++++-+++--+-+++-+--+-++++-++++-+--++--++++-+---+-+-+----+----+++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--+-+++--+-+++-+--++----+----+-++--++----+-+++-+--+----+----+
-++--+-++++-+++++++----+----++-+++--+-+++-+---+-+---+-++++--+++--+-+++-+--+-++++-++++-+--++--++++-+---+-+-+----+----++-++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--+++--+-+++-+--++----+----+-++--++----+-+++-+--+----+----++
++--+-++++-+++++++----+----++-+++--+-+++-+---+-+---+-++++--+++--+-+++-+--+-++++-++++-+--++--++++-+---+-+-+----+----++---++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--+++++--+-+++-+--++----+----+-++--++----+-+++-+--+----+----++-
+--+-++++-+++++++----+----++-+++--+-+++-+---+-+---+-++++--+++--+-+++-+--+-++++-++++-+--++--++++-+---+-+-+----+----++-++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--+++--+-+++-+--++----+----+-++--++----+-+++-+--+----+----++-+
--+-++++-+++++++----+----++-+++--+-+++-+---+-+---+-++++--+++--+-+++-+--+-++++-++++-+--++--++++-+---+-+-+----+----++-++++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--+--+-+++-+--++----+----+-++--++----+-+++-+--+----+----++-++
-+-++++-+++++++----+----++-+++--+-+++-+---+-+---+-++++--++---+-+++-+--+-++++-++++-+--++--++++-+---+-+-+----+----++-+++-++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+---+-+++-+--++----+----+-++--++----+-+++-+--+----+----++-+++
+-++++-+++++++----+----++-+++--+-+++-+---+-+---+-++++--++---+-+++-+--+-++++-++++-+--++--++++-+---+-+-+----+----++-+++---++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+-+-+++-+--++----+----+-++--++----+-+++-+--+----+----++-+++-
-++++-+++++++----+----++-+++--+-+++-+---+-+---+-++++--++--++-+++-+--+-++++-++++-+--++--++++-+---+-+-+----+----++-+++--+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+-+++-+--++----+----+-++--++----+-+++-+--+----+----++-+++--
++++-+++++++----+----++-+++--+-+++-+---+-+---+-++++--++--+--+++-+--+-++++-++++-+--++--++++-+---+-+-+----+----++-+++--+-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+++-+--++----+----+-++--++----+-+++-+--+----+----++-+++--+
+++-+++++++----+----++-+++--+-+++-+---+-+---+-++++--++--+-++++-+--+-++++-++++-+--++--++++-+---+-+-+----+----++-+++--+-+-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++++-+--++----+----+-++--++----+-+++-+--+----+----++-+++--+-
++-+++++++----+----++-+++--+-+++-+---+-+---+-++++--++--+-++++-+--+-++++-++++-+--++--++++-+---+-+-+----+----++-+++--+-+++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++----+----+-++--++----+-+++-+--+----+----++-+++--+-+
+-+++++++----+----++-+++--+-+++-+---+-+---+-++++--++--+-++++-+--+-++++-++++-+--++--++++-+---+-+-+----+----++-+++--+-+++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++-+--++----+----+-++--++----+-+++-+--+----+----++-+++--+-++
-+++++++----+----++-+++--+-+++-+---+-+---+-++++--++--+-++++-+--+-++++-++++-+--++--++++-+---+-+-+----+----++-+++--+-+++++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++--+--++----+----+-++--++----+-+++-+--+----+----++-+++--+-+++
+++++++----+----++-+++--+-+++-+---+-+---+-++++--++--+-++++-+--+-++++-++++-+--++--++++-+---+-+-+----+----++-+++--+-+++--++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--++++++--++----+----+-++--++----+-+++-+--+----+----++-+++--+-+++-
++++++----+----++-+++--+-+++-+---+-+---+-++++--++--+-++++-+--+-++++-++++-+--++--++++-+---+-+-+----+----++-+++--+-+++-++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--++++--++----+----+-++--++----+-+++-+--+----+----++-+++--+-+++-+
+++++----+----++-+++--+-+++-+---+-+---+-++++--++--+-++++-++-+-++++-++++-+--++--++++-+---+-+-+----+----++-+++--+-+++-+-++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++-++----+----+-++--++----+-+++-+--+----+----++-+++--+-+++-+-
++++----+----++-+++--+-+++-+---+-+---+-++++--++--+-++++-++++-++++-++++-+--++--++++-+---+-+-+----+----++-+++--+-+++-+--+++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--++++----+----+-++--++----+-+++-+--+----+----++-+++--+-+++-+--
+++----+----++-+++--+-+++-+---+-+---+-++++--++--+-++++-++++-++++-++++-+--++--++++-+---+-+-+----+----++-+++--+-+++-+--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--++----+----+-++--++----+-+++-+--+----+----++-+++--+-+++-+--+
-++-+---+-++---+--++++-++++-+-+-+++-+----++--++-+----+----+++++-++++-+--++--++++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--
++-+---+-++---+--++++-++++-+-+-+++-+----++--++-+----+----+-+++-++++-+--++--++++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--+-+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++-
+-+---+-++---+--++++-++++-+-+-+++-+----++--++-+----+----+-+++-++++-+--++--++++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++
-+---+-++---+--++++-++++-+-+-+++-+----++--++-+----+----+-+++-++++-+--++--++++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-+++
+---+-++---+--++++-++++-+-+-+++-+----++--++-+----+----+-++--++++-+--++--++++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++
---+-++---+--++++-++++-+-+-+++-+----++--++-+----+----+-++-+++++-+--++--++++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-+++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-+
--+-++---+--++++-++++-+-+-+++-+----++--++-+----+----+-++-+-+++-+--++--++++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-+++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-
-+-++---+--++++-++++-+-+-+++-+----++--++-+----+----+-++-+--++-+--++--++++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++
+-++---+--++++-++++-+-+-+++-+----++--++-+----+----+-++-+---+-+--++--++++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--+++
-++---+--++++-++++-+-+-+++-+----++--++-+----+----+-++-+---+-+--++--++++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++
++---+--++++-++++-+-+-+++-+----++--++-+----+----+-++-+---+-+--++--++++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--+
+---+--++++-++++-+-+-+++-+----++--++-+----+----+-++-+---+-+--++--++++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+--
---+--++++-++++-+-+-+++-+----++--++-+----+----+-++-+---+-++-++--++++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+-
--+--++++-++++-+-+-+++-+----++--++-+----+----+-++-+---+-++-++--++++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+----++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---+
-+--++++-++++-+-+-+++-+----++--++-+----+----+-++-+---+-++--+--++++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+--++--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++---
+--++++-++++-+-+-+++-+----++--++-+----+----+-++-+---+-++-----++++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+--++-+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++--
--++++-++++-+-+-+++-+----++--++-+----+----+-++-+---+-++---+-++++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+--++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++-
-++++-++++-+-+-+++-+----++--++-+----+----+-++-+---+-++---+-++++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+--++-----+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-++
++++-++++-+-+-+++-+----++--++-+----+----+-++-+---+-++---+--+++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+--++--++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-+
+++-++++-+-+-+++-+----++--++-+----+----+-++-+---+-++---+--+++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+--++--++++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+-
++-++++-+-+-+++-+----++--++-+----+----+-++-+---+-++---+--+++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+--++--+++-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---+
+-++++-+-+-+++-+----++--++-+----+----+-++-+---+-++---+--+++-+---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+--++--+++++-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+---
-++++-+-+-+++-+----++--++-+----+----+-++-+---+-++---+--+++++---+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+--++--++++--+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+--
++++-+-+-+++-+----++--++-+----+----+-++-+---+-++---+--++++----+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+--++--++++-+--+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+-
+++-+-+-+++-+----++--++-+----+----+-++-+---+-++---+--++++-+--+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+--++--++++-+----+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-+
++-+-+-+++-+----++--++-+----+----+-++-+---+-++---+--++++-++-+-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+--++--++++-+--+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++-
+-+-+-+++-+----++--++-+----+----+-++-+---+-++---+--++++-++++-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+--++--++++-+----+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++++
-+-+-+++-+----++--++-+----+----+-++-+---+-++---+--++++-++++-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+--++--++++-+---++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-+++
+-+-+++-+----++--++-+----+----+-++-+---+-++---+--++++-++++-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+--++--++++-+---+-++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-++
-+-+++-+----++--++-+----+----+-++-+---+-++---+--++++-++++-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++--++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-+
+-+++-+----++--++-+----+----+-++-+---+-++---+--++++-++++-+---+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-+--++++-++++--+---++-+---+-++--++++-++++-+--++--++++-+---+-++++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+-
-+++-+----++--++-+----+----+-++-+---+-++---+--++++-++++-+-+-+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-+--++++-++++--+---++-+---+-++--++++-++++-+--++--++++-+---+-++--++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---+
+++-+----++--++-+----+----+-++-+---+-++---+--++++-++++-+-+-+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-+---+++-++++--+---++-+---+-++--++++-++++-+--++--++++-+---+-++-++-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+---
++-+----++--++-+----+----+-++-+---+-++---+--++++-++++-+-+-+-+++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-+---+++-++++--+---++-+---+-++--++++-++++-+--++--++++-+---+-++-++-+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+--
+-+----++--++-+----+----+-++-+---+-++---+--++++-++++-+-+-+++++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-+---+-+-++++--+---++-+---+-++--++++-++++-+--++--++++-+---+-++-+++--+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+-
-+----++--++-+----+----+-++-+---+-++---+--++++-++++-+-+-+++++-+--+++-++----+----+++++++-++++-+--++--++++-+---+-+---+-+-++++--+---++-+---+-++--++++-++++-+--++--++++-+---+-++-++++---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-+
+----++--++-+----+----+-++-+---+-++---+--++++-++++-+-+-+++-+-+--+++-++----+----+++++++-++++-+--++--++++-+---+-+---+-++++++--+---++-+---+-++--++++-++++-+--++--++++-+---+-++-++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++-
----++--++-+----+----+-++-+---+-++---+--++++-++++-+-+-+++-+-+--+++-++----+----+++++++-++++-+--++--++++-+---+-+---+-++++++--+---++-+---+-++--++++-++++-+--++--++++-+---+-++-++++-+-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++++
---++--++-+----+----+-++-+---+-++---+--++++-++++-+-+-+++-+-+--+++-++----+----+++++++-++++-+--++--++++-+---+-+---+-+++-++--+---++-+---+-++--++++-++++-+--++--++++-+---+-++-++++-+++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--+++
--++--++-+----+----+-++-+---+-++---+--++++-++++-+-+-+++-+----+++-++----+----+++++++-++++-+--++--++++-+---+-+---+-+++-++--+---++-+---+-++--++++-++++-+--++--++++-+---+-++-++++-+++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--++
-++--++-+----+----+-++-+---+-++---+--++++-++++-+-+-+++-+----+++-++----+----+++++++-++++-+--++--++++-+---+-+---+-+++-+---+---++-+---+-++--++++-++++-+--++--++++-+---+-++-++++-+++++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--+
++--++-+----+----+-++-+---+-++---+--++++-++++-+-+-+++-+----+++-++----+----+++++++-++++-+--++--++++-+---+-+---+-+++-+---+---++-+---+-++--++++-++++-+--++--++++-+---+-++-++++-++++-++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++--
+--++-+----+----+-++-+---+-++---+--++++-++++-+-+-+++-+----+++-++----+----+++++++-++++-+--++--++++-+---+-+---+-+++-+--++---++-+---+-++--++++-++++-+--++--++++-+---+-++-++++-++++---++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++-
--++-+----+----+-++-+---+-++---+--++++-++++-+-+-+++-+----+++-++----+----+++++++-++++-+--++--++++-+---+-+---+-+++-+--++---++-+---+-++--++++-++++-+--++--++++-+---+-++-++++-++++--+--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--++
-++-+----+----+-++-+---+-++---+--++++-++++-+-+-+++-+----++--++----+----+++++++-++++-+--++--++++-+---+-+---+-+++-+--+++--++-+---+-++--++++-++++-+--++--++++-+---+-++-++++-++++--+-+--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--+
++-+----+----+-++-+---+-++---+--++++-++++-+-+-+++-+----++--++----+----+++++++-++++-+--++--++++-+---+-+---+-+++-+--+++--++-+---+-++--++++-++++-+--++--++++-+---+-++-++++-++++--+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+--
+-+----+----+-++-+---+-++---+--++++-++++-+-+-+++-+----++--++----+----+++++++-++++-+--++--++++-+---+-+---+-+++-+--+++-+++-+---+-++--++++-++++-+--++--++++-+---+-++-++++-++++--+----++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+-
-+----+----+-++-+---+-++---+--++++-++++-+-+-+++-+----++--++----+----+++++++-++++-+--++--++++-+---+-+---+-+++-+--+++-+++-+---+-++--++++-++++-+--++--++++-+---+-++-++++-++++--+---+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-+
+----+----+-++-+---+-++---+--++++-++++-+-+-+++-+----++--++----+----+++++++-++++-+--++--++++-+---+-+---+-+++-+--+++-++--+---+-++--++++-++++-+--++--++++-+---+-++-++++-++++--+---+++--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++-
----+----+-++-+---+-++---+--++++-++++-+-+-+++-+----++--++-+--+----+++++++-++++-+--++--++++-+---+-+---+-+++-+--+++-++--+---+-++--++++-++++-+--++--++++-+---+-++-++++-++++--+---++--+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++++
---+----+-++-+---+-++---+--++++-++++-+-+-+++-+----++--++-+--+----+++++++-++++-+--++--++++-+---+-+---+-+++-+--+++-++------+-++--++++-++++-+--++--++++-+---+-++-++++-++++--+---++-++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-+++
--+----+-++-+---+-++---+--++++-++++-+-+-+++-+----++--++-+--+----+++++++-++++-+--++--++++-+---+-+---+-+++-+--+++-++------+-++--++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+-++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-++
-+----+-++-+---+-++---+--++++-++++-+-+-+++-+----++--++-+-------+++++++-++++-+--++--++++-+---+-+---+-+++-+--+++-++----+-+-++--++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+--+++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-+
+----+-++-+---+-++---+--++++-++++-+-+-+++-+----++--++-+-------+++++++-++++-+--++--++++-+---+-+---+-+++-+--+++-++----+-+-++--++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++-
----+-++-+---+-++---+--++++-++++-+-+-+++-+----++--++-+----+--+++++++-++++-+--++--++++-+---+-+---+-+++-+--+++-++----+---++--++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++++
---+-++-+---+-++---+--++++-++++-+-+-+++-+----++--++-+----+--+++++++-++++-+--++--++++-+---+-+---+-+++-+--+++-++----+---++--++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-+-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--++++
--+-++-+---+-++---+--++++-++++-+-+-+++-+----++--++-+----+--+++++++-++++-+--++--++++-+---+-+---+-+++-+--+++-++----+----+--++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-+++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+++
-+-++-+---+-++---+--++++-++++-+-+-+++-+----++--++-+----+---++++++-++++-+--++--++++-+---+-+---+-+++-+--+++-++----+----+--++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-+++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--++
+-++-+---+-++---+--++++-++++-+-+-+++-+----++--++-+----+----+++++-++++-+--++--++++-+---+-+---+-+++-+--+++-++----+----++-++++-++++-+--++--++++-+---+-++-++++-++++--+---++-+---+-++-++++-++++-+--++--++++-+---+-++++-+---+-++---+--++++-++++--+
</content>

<content full_path="exllamav3/util/hadamard_data/hadamard_92.txt">
+++-+++-+------+-+++-+++++---++-+-++-+-++---+++-++-++--++++++--++-++-++---+---+-++-+---+---+
++++-+++-+------+-+++-+++++---++-+-++-+-++---+-+-++-++--++++++--++-+++++---+---+-++-+---+---
+++++-+++-+------+-+++-+++++---++-+-++-+-++---+-+-++-++--++++++--++-+-+++---+---+-++-+---+--
-+++++-+++-+------+-+++-+++++---++-+-++-+-++--++-+-++-++--++++++--++---+++---+---+-++-+---+-
+-+++++-+++-+------+-++--+++++---++-+-++-+-++--++-+-++-++--++++++--++---+++---+---+-++-+---+
++-+++++-+++-+------+-+---+++++---++-+-++-+-+++-++-+-++-++--++++++--++---+++---+---+-++-+---
+++-+++++-+++-+------+-+---+++++---++-+-++-+-+++-++-+-++-++--++++++---+---+++---+---+-++-+--
-+++-+++++-+++-+------+++---+++++---++-+-++-+--++-++-+-++-++--++++++---+---+++---+---+-++-+-
+-+++-+++++-+++-+-------++---+++++---++-+-++-+--++-++-+-++-++--++++++---+---+++---+---+-++-+
-+-+++-+++++-+++-+-----+-++---+++++---++-+-++-+--++-++-+-++-++--++++++---+---+++---+---+-++-
--+-+++-+++++-+++-+-----+-++---+++++---++-+-++++--++-++-+-++-++--++++-+---+---+++---+---+-++
---+-+++-+++++-+++-+---+-+-++---+++++---++-+-++++--++-++-+-++-++--++++-+---+---+++---+---+-+
----+-+++-+++++-+++-+--++-+-++---+++++---++-+-++++--++-++-+-++-++--++++-+---+---+++---+---+-
-----+-+++-+++++-+++-+--++-+-++---+++++---++-++++++--++-++-+-++-++--+-++-+---+---+++---+---+
------+-+++-+++++-+++-++-++-+-++---+++++---++-++++++--++-++-+-++-++--+-++-+---+---+++---+---
+------+-+++-+++++-+++--+-++-+-++---+++++---++-++++++--++-++-+-++-++--+-++-+---+---+++---+--
-+------+-+++-+++++-++++-+-++-+-++---+++++---+--++++++--++-++-+-++-++--+-++-+---+---+++---+-
+-+------+-+++-+++++-++++-+-++-+-++---+++++---+--++++++--++-++-+-++-+---+-++-+---+---+++---+
++-+------+-+++-+++++-+-++-+-++-+-++---+++++--++--++++++--++-++-+-++-+---+-++-+---+---+++---
+++-+------+-+++-+++++---++-+-++-+-++---+++++--++--++++++--++-++-+-++-+---+-++-+---+---+++--
-+++-+------+-+++-+++++---++-+-++-+-++---++++++-++--++++++--++-++-+-+--+---+-++-+---+---+++-
+-+++-+------+-+++-+++++---++-+-++-+-++---++++++-++--++++++--++-++-+----+---+-++-+---+---+++
++-+++-+------+-+++-+++++---++-+-++-+-++---+++-++-++--++++++--++-++-++---+---+-++-+---+---++
---+++--+-+--+-+--+++--+++-+++-+------+-+++-++--+++-+++-+--+-+++-+++-+-++-++--++++++--++-++-
----+++--+-+--+-+--+++-++++-+++-+------+-+++-+---+++-+++-+--+-+++-+++-+-++-++--++++++--++-++
-----+++--+-+--+-+--++++++++-+++-+------+-+++-+---+++-+++-+--+-+++-+++-+-++-++--++++++--++-+
+-----+++--+-+--+-+--++-+++++-+++-+------+-+++++---+++-+++-+--+-+++-+++-+-++-++--++++++--++-
++-----+++--+-+--+-+--++-+++++-+++-+------+-+++++---+++-+++-+--+-+++--++-+-++-++--++++++--++
+++-----+++--+-+--+-+--++-+++++-+++-+------+-+-+++---+++-+++-+--+-++++-++-+-++-++--++++++--+
-+++-----+++--+-+--+-+-+++-+++++-+++-+------+-+-+++---+++-+++-+--+-++++-++-+-++-++--++++++--
--+++-----+++--+-+--+-+-+++-+++++-+++-+------+++-+++---+++-+++-+--+-+-++-++-+-++-++--++++++-
+--+++-----+++--+-+--+-+-+++-+++++-+++-+------+++-+++---+++-+++-+--+---++-++-+-++-++--++++++
-+--+++-----+++--+-+--+-+-+++-+++++-+++-+------+++-+++---+++-+++-+--++--++-++-+-++-++--+++++
+-+--+++-----+++--+-+----+-+++-+++++-+++-+----+-+++-+++---+++-+++-+--++--++-++-+-++-++--++++
-+-+--+++-----+++--+-+----+-+++-+++++-+++-+----+-+++-+++---+++-+++-+-+++--++-++-+-++-++--+++
--+-+--+++-----+++--+-+----+-+++-+++++-+++-+----+-+++-+++---+++-+++-+++++--++-++-+-++-++--++
+--+-+--+++-----+++--+------+-+++-+++++-+++-+-+--+-+++-+++---+++-+++-+++++--++-++-+-++-++--+
-+--+-+--+++-----+++--+------+-+++-+++++-+++-+-+--+-+++-+++---+++-+++++++++--++-++-+-++-++--
+-+--+-+--+++-----+++--+------+-+++-+++++-+++-+-+--+-+++-+++---+++-++-++++++--++-++-+-++-++-
-+-+--+-+--+++-----+++--+------+-+++-+++++-+++++-+--+-+++-+++---+++-+--++++++--++-++-+-++-++
--+-+--+-+--+++-----++++-+------+-+++-+++++-+++++-+--+-+++-+++---+++-+--++++++--++-++-+-++-+
+--+-+--+-+--+++-----++++-+------+-+++-+++++-+-+++-+--+-+++-+++---+++++--++++++--++-++-+-++-
++--+-+--+-+--+++-----++++-+------+-+++-+++++-+-+++-+--+-+++-+++---++-++--++++++--++-++-+-++
+++--+-+--+-+--+++------+++-+------+-+++-+++++++-+++-+--+-+++-+++---++-++--++++++--++-++-+-+
-+++--+-+--+-+--+++----+-+++-+------+-+++-+++++++-+++-+--+-+++-+++---++-++--++++++--++-++-+-
--+++--+-+--+-+--+++---++-+++-+------+-+++-+++-+++-+++-+--+-+++-+++---++-++--++++++--++-++-+
-+--+--++------++--+--+++---+---+-++-+---+---++++-+++-+------+-+++-++---+++--+-+--+-+--+++--
+-+--+--++------++--+--+++---+---+-++-+---+---++++-+++-+------+-+++-+----+++--+-+--+-+--+++-
-+-+--+--++------++--+--+++---+---+-++-+---+--+++++-+++-+------+-+++------+++--+-+--+-+--+++
--+-+--+--++------++--+--+++---+---+-++-+---+--+++++-+++-+------+-++++-----+++--+-+--+-+--++
+--+-+--+--++------++-----+++---+---+-++-+---++-+++++-+++-+------+-++++-----+++--+-+--+-+--+
-+--+-+--+--++------++-+---+++---+---+-++-+---++-+++++-+++-+------+-++++-----+++--+-+--+-+--
--+--+-+--+--++------++-+---+++---+---+-++-+--+++-+++++-+++-+------+--+++-----+++--+-+--+-+-
+--+--+-+--+--++------+--+---+++---+---+-++-+--+++-+++++-+++-+------+--+++-----+++--+-+--+-+
++--+--+-+--+--++---------+---+++---+---+-++-++-+++-+++++-+++-+------+--+++-----+++--+-+--+-
-++--+--+-+--+--++-----+---+---+++---+---+-++--+-+++-+++++-+++-+------+--+++-----+++--+-+--+
--++--+--+-+--+--++-----+---+---+++---+---+-++--+-+++-+++++-+++-+----+-+--+++-----+++--+-+--
---++--+--+-+--+--++---+-+---+---+++---+---+-+---+-+++-+++++-+++-+----+-+--+++-----+++--+-+-
----++--+--+-+--+--++--++-+---+---+++---+---+-----+-+++-+++++-+++-+----+-+--+++-----+++--+-+
-----++--+--+-+--+--++--++-+---+---+++---+---+-----+-+++-+++++-+++-+-+--+-+--+++-----+++--+-
------++--+--+-+--+--+++-++-+---+---+++---+---------+-+++-+++++-+++-+-+--+-+--+++-----+++--+
+------++--+--+-+--+--+-+-++-+---+---+++---+--+------+-+++-+++++-+++-+-+--+-+--+++-----+++--
++------++--+--+-+--+----+-++-+---+---+++---+--+------+-+++-+++++-+++-+-+--+-+--+++-----+++-
-++------++--+--+-+--+----+-++-+---+---+++---++-+------+-+++-+++++-++--+-+--+-+--+++-----+++
--++------++--+--+-+--++---+-++-+---+---+++---++-+------+-+++-+++++-++--+-+--+-+--+++-----++
+--++------++--+--+-+---+---+-++-+---+---+++--+++-+------+-+++-+++++-++--+-+--+-+--+++-----+
-+--++------++--+--+-+---+---+-++-+---+---+++--+++-+------+-+++-++++++++--+-+--+-+--+++-----
--+--++------++--+--+-+---+---+-++-+---+---++++-+++-+------+-+++-++++-+++--+-+--+-+--+++----
+--+--++------++--+--+-+---+---+-++-+---+---++++-+++-+------+-+++-+++--+++--+-+--+-+--+++---
--+++-+++-+--+-+++-+++--+--+--++------++--+--++++---++-+-++-+-++---+++++-+++-+------+-+++-++
---+++-+++-+--+-+++-++++-+--+--++------++--+--++++---++-+-++-+-++---+++++-+++-+------+-+++-+
+---+++-+++-+--+-+++-++-+-+--+--++------++--+-+++++---++-+-++-+-++---+++++-+++-+------+-+++-
++---+++-+++-+--+-+++-+--+-+--+--++------++--+-+++++---++-+-++-+-++---+++++-+++-+------+-+++
+++---+++-+++-+--+-+++-+--+-+--+--++------++----+++++---++-+-++-+-++-+-+++++-+++-+------+-++
-+++---+++-+++-+--+-+++-+--+-+--+--++------++----+++++---++-+-++-+-++++-+++++-+++-+------+-+
+-+++---+++-+++-+--+-++--+--+-+--+--++------+++---+++++---++-+-++-+-++++-+++++-+++-+------+-
++-+++---+++-+++-+--+-++--+--+-+--+--++------+++---+++++---++-+-++-+--+++-+++++-+++-+------+
+++-+++---+++-+++-+--+-++--+--+-+--+--++-------++---+++++---++-+-++-++-+++-+++++-+++-+------
-+++-+++---+++-+++-+--+-++--+--+-+--+--++-----+-++---+++++---++-+-++--+-+++-+++++-+++-+-----
+-+++-+++---+++-+++-+----++--+--+-+--+--++-----+-++---+++++---++-+-++--+-+++-+++++-+++-+----
-+-+++-+++---+++-+++-+----++--+--+-+--+--++---+-+-++---+++++---++-+-+---+-+++-+++++-+++-+---
--+-+++-+++---+++-+++-+----++--+--+-+--+--++--++-+-++---+++++---++-+-----+-+++-+++++-+++-+--
+--+-+++-+++---+++-+++------++--+--+-+--+--++--++-+-++---+++++---++-+-----+-+++-+++++-+++-+-
-+--+-+++-+++---+++-+++------++--+--+-+--+--+++-++-+-++---+++++---++-------+-+++-+++++-+++-+
+-+--+-+++-+++---+++-+++------++--+--+-+--+--+-+-++-+-++---+++++---+++------+-+++-+++++-+++-
++-+--+-+++-+++---+++-+++------++--+--+-+--+--+-+-++-+-++---+++++---+-+------+-+++-+++++-+++
+++-+--+-+++-+++---+++--++------++--+--+-+--+-++-+-++-+-++---+++++---+-+------+-+++-+++++-++
-+++-+--+-+++-+++---+++--++------++--+--+-+--+-++-+-++-+-++---+++++--++-+------+-+++-+++++-+
+-+++-+--+-+++-+++---+++--++------++--+--+-+----++-+-++-+-++---+++++-+++-+------+-+++-+++++-
++-+++-+--+-+++-+++---+-+--++------++--+--+-+----++-+-++-+-++---+++++-+++-+------+-+++-+++++
+++-+++-+--+-+++-+++-----+--++------++--+--+-++---++-+-++-+-++---+++++-+++-+------+-+++-++++
-+++-+++-+--+-+++-+++--+--+--++------++--+--+-++---++-+-++-+-++---+++++-+++-+------+-+++-+++
</content>

<content full_path="exllamav3/util/hadamard_data/hadamard_156.txt">
+++--+-+-----+--++----++--+-----+-+--++++++---+--++----+-+--+-+----++--+---++++++--++-+---+-+--+----+--+-+---+-++--+++---++-+-+-----+++-++-+++-----+-+-++---
++++--+-+-----+--++----++--+-----+-+--++++++---+--++----+-+--+-+----++--+---++++++--++-+---+-+--+----+--+-+---+-++--+-+---++-+-+-----+++-++-+++-----+-+-++--
+++++--+-+-----+--++----++--+-----+-+--++++++---+--++----+-+--+-+----++--+---++++++--++-+---+-+--+----+--+-+---+-++----+---++-+-+-----+++-++-+++-----+-+-++-
-+++++--+-+-----+--++----++--+-----+-+-+++++++---+--++----+-+--+-+----++--+----+++++--++-+---+-+--+----+--+-+---+-++----+---++-+-+-----+++-++-+++-----+-+-++
--+++++--+-+-----+--++----++--+-----+-+-+++++++---+--++----+-+--+-+----++--+----+++++--++-+---+-+--+----+--+-+---+-+++---+---++-+-+-----+++-++-+++-----+-+-+
+--+++++--+-+-----+--++----++--+-----+---+++++++---+--++----+-+--+-+----++--+-+--+++++--++-+---+-+--+----+--+-+---+-+++---+---++-+-+-----+++-++-+++-----+-+-
-+--+++++--+-+-----+--++----++--+-----+---+++++++---+--++----+-+--+-+----++--+++--+++++--++-+---+-+--+----+--+-+---+--++---+---++-+-+-----+++-++-+++-----+-+
+-+--+++++--+-+-----+--++----++--+-----+---+++++++---+--++----+-+--+-+----++---++--+++++--++-+---+-+--+----+--+-+---++-++---+---++-+-+-----+++-++-+++-----+-
-+-+--+++++--+-+-----+--++----++--+-----+---+++++++---+--++----+-+--+-+----++-+-++--+++++--++-+---+-+--+----+--+-+----+-++---+---++-+-+-----+++-++-+++-----+
--+-+--+++++--+-+-----+--++----++--+-----+---+++++++---+--++----+-+--+-+----++-+-++--+++++--++-+---+-+--+----+--+-+--+-+-++---+---++-+-+-----+++-++-+++-----
---+-+--+++++--+-+-----+--++----++--+--+--+---+++++++---+--++----+-+--+-+----+--+-++--+++++--++-+---+-+--+----+--+-+--+-+-++---+---++-+-+-----+++-++-+++----
----+-+--+++++--+-+-----+--++----++--+-++--+---+++++++---+--++----+-+--+-+-------+-++--+++++--++-+---+-+--+----+--+-+--+-+-++---+---++-+-+-----+++-++-+++---
-----+-+--+++++--+-+-----+--++----++--+-++--+---+++++++---+--++----+-+--+-+---+---+-++--+++++--++-+---+-+--+----+--+----+-+-++---+---++-+-+-----+++-++-+++--
+-----+-+--+++++--+-+-----+--++----++----++--+---+++++++---+--++----+-+--+-+---+---+-++--+++++--++-+---+-+--+----+--+----+-+-++---+---++-+-+-----+++-++-+++-
-+-----+-+--+++++--+-+-----+--++----++----++--+---+++++++---+--++----+-+--+-+-+-+---+-++--+++++--++-+---+-+--+----+-------+-+-++---+---++-+-+-----+++-++-+++
--+-----+-+--+++++--+-+-----+--++----++----++--+---+++++++---+--++----+-+--+-+-+-+---+-++--+++++--++-+---+-+--+----+-+-----+-+-++---+---++-+-+-----+++-++-++
+--+-----+-+--+++++--+-+-----+--++----++----++--+---+++++++---+--++----+-+--+---+-+---+-++--+++++--++-+---+-+--+----+++-----+-+-++---+---++-+-+-----+++-++-+
++--+-----+-+--+++++--+-+-----+--++-----+----++--+---+++++++---+--++----+-+--++--+-+---+-++--+++++--++-+---+-+--+----+++-----+-+-++---+---++-+-+-----+++-++-
-++--+-----+-+--+++++--+-+-----+--++---+-+----++--+---+++++++---+--++----+-+---+--+-+---+-++--+++++--++-+---+-+--+----+++-----+-+-++---+---++-+-+-----+++-++
--++--+-----+-+--+++++--+-+-----+--++---+-+----++--+---+++++++---+--++----+-+---+--+-+---+-++--+++++--++-+---+-+--+--+-+++-----+-+-++---+---++-+-+-----+++-+
---++--+-----+-+--+++++--+-+-----+--++---+-+----++--+---+++++++---+--++----+-+---+--+-+---+-++--+++++--++-+---+-+--+-++-+++-----+-+-++---+---++-+-+-----+++-
----++--+-----+-+--+++++--+-+-----+--+++--+-+----++--+---+++++++---+--++----+-----+--+-+---+-++--+++++--++-+---+-+--+-++-+++-----+-+-++---+---++-+-+-----+++
+----++--+-----+-+--+++++--+-+-----+--+-+--+-+----++--+---+++++++---+--++----++----+--+-+---+-++--+++++--++-+---+-+--+-++-+++-----+-+-++---+---++-+-+-----++
++----++--+-----+-+--+++++--+-+-----+--+-+--+-+----++--+---+++++++---+--++-----+----+--+-+---+-++--+++++--++-+---+-+-++-++-+++-----+-+-++---+---++-+-+-----+
-++----++--+-----+-+--+++++--+-+-----+--+-+--+-+----++--+---+++++++---+--++-----+----+--+-+---+-++--+++++--++-+---+-++++-++-+++-----+-+-++---+---++-+-+-----
--++----++--+-----+-+--+++++--+-+-----+--+-+--+-+----++--+---+++++++---+--++--+--+----+--+-+---+-++--+++++--++-+---+--+++-++-+++-----+-+-++---+---++-+-+----
+--++----++--+-----+-+--+++++--+-+--------+-+--+-+----++--+---+++++++---+--++--+--+----+--+-+---+-++--+++++--++-+---+--+++-++-+++-----+-+-++---+---++-+-+---
-+--++----++--+-----+-+--+++++--+-+--------+-+--+-+----++--+---+++++++---+--+++-+--+----+--+-+---+-++--+++++--++-+------+++-++-+++-----+-+-++---+---++-+-+--
--+--++----++--+-----+-+--+++++--+-+---+----+-+--+-+----++--+---+++++++---+--+-+-+--+----+--+-+---+-++--+++++--++-+------+++-++-+++-----+-+-++---+---++-+-+-
---+--++----++--+-----+-+--+++++--+-+--++----+-+--+-+----++--+---+++++++---+----+-+--+----+--+-+---+-++--+++++--++-+------+++-++-+++-----+-+-++---+---++-+-+
----+--++----++--+-----+-+--+++++--+-+--++----+-+--+-+----++--+---+++++++---+----+-+--+----+--+-+---+-++--+++++--++-++-----+++-++-+++-----+-+-++---+---++-+-
-----+--++----++--+-----+-+--+++++--+-+--++----+-+--+-+----++--+---+++++++---++---+-+--+----+--+-+---+-++--+++++--++--+-----+++-++-+++-----+-+-++---+---++-+
+-----+--++----++--+-----+-+--+++++--+-+--++----+-+--+-+----++--+---+++++++----+---+-+--+----+--+-+---+-++--+++++--+++-+-----+++-++-+++-----+-+-++---+---++-
-+-----+--++----++--+-----+-+--+++++--+-+--++----+-+--+-+----++--+---+++++++--+-+---+-+--+----+--+-+---+-++--+++++--+-+-+-----+++-++-+++-----+-+-++---+---++
+-+-----+--++----++--+-----+-+--+++++----+--++----+-+--+-+----++--+---+++++++-++-+---+-+--+----+--+-+---+-++--+++++--+-+-+-----+++-++-+++-----+-+-++---+---+
-+-+-----+--++----++--+-----+-+--+++++----+--++----+-+--+-+----++--+---+++++++-++-+---+-+--+----+--+-+---+-++--+++++-++-+-+-----+++-++-+++-----+-+-++---+---
--+-+-----+--++----++--+-----+-+--++++++---+--++----+-+--+-+----++--+---++++++--++-+---+-+--+----+--+-+---+-++--+++++-++-+-+-----+++-++-+++-----+-+-++---+--
+--+-+-----+--++----++--+-----+-+--++++++---+--++----+-+--+-+----++--+---++++++--++-+---+-+--+----+--+-+---+-++--++++--++-+-+-----+++-++-+++-----+-+-++---+-
++--+-+-----+--++----++--+-----+-+--++++++---+--++----+-+--+-+----++--+---++++++--++-+---+-+--+----+--+-+---+-++--+++---++-+-+-----+++-++-+++-----+-+-++---+
----+++-++--++++-+-++-+-++++--++-+++---+++--+-+-----+--++----++--+-----+-+--++-+++--+-+-+++++---+--+---+++++-+-+--++++++--++-+---+-+--+----+--+-+---+-++--++
-----+++-++--++++-+-++-+-++++--++-+++--++++--+-+-----+--++----++--+-----+-+--++-+++--+-+-+++++---+--+---+++++-+-+--++++++--++-+---+-+--+----+--+-+---+-++--+
------+++-++--++++-+-++-+-++++--++-+++-+++++--+-+-----+--++----++--+-----+-+--++-+++--+-+-+++++---+--+---+++++-+-+--++++++--++-+---+-+--+----+--+-+---+-++--
-------+++-++--++++-+-++-+-++++--++-+++-+++++--+-+-----+--++----++--+-----+-+-+++-+++--+-+-+++++---+--+---+++++-+-+---+++++--++-+---+-+--+----+--+-+---+-++-
+-------+++-++--++++-+-++-+-++++--++-++--+++++--+-+-----+--++----++--+-----+-+-+++-+++--+-+-+++++---+--+---+++++-+-+---+++++--++-+---+-+--+----+--+-+---+-++
++-------+++-++--++++-+-++-+-++++--++-++--+++++--+-+-----+--++----++--+-----+---+++-+++--+-+-+++++---+--+---+++++-+-++--+++++--++-+---+-+--+----+--+-+---+-+
+++-------+++-++--++++-+-++-+-++++--++--+--+++++--+-+-----+--++----++--+-----++--+++-+++--+-+-+++++---+--+---+++++-+-++--+++++--++-+---+-+--+----+--+-+---+-
-+++-------+++-++--++++-+-++-+-++++--+++-+--+++++--+-+-----+--++----++--+------+--+++-+++--+-+-+++++---+--+---+++++-+-++--+++++--++-+---+-+--+----+--+-+---+
+-+++-------+++-++--++++-+-++-+-++++--+-+-+--+++++--+-+-----+--++----++--+----+-+--+++-+++--+-+-+++++---+--+---+++++-+-++--+++++--++-+---+-+--+----+--+-+---
++-+++-------+++-++--++++-+-++-+-++++----+-+--+++++--+-+-----+--++----++--+----+-+--+++-+++--+-+-+++++---+--+---+++++-+-++--+++++--++-+---+-+--+----+--+-+--
-++-+++-------+++-++--++++-+-++-+-++++----+-+--+++++--+-+-----+--++----++--+--+-+-+--+++-+++--+-+-+++++---+--+---++++--+-++--+++++--++-+---+-+--+----+--+-+-
--++-+++-------+++-++--++++-+-++-+-++++----+-+--+++++--+-+-----+--++----++--+-++-+-+--+++-+++--+-+-+++++---+--+---+++---+-++--+++++--++-+---+-+--+----+--+-+
+--++-+++-------+++-++--++++-+-++-+-+++-----+-+--+++++--+-+-----+--++----++--++++-+-+--+++-+++--+-+-+++++---+--+---+++---+-++--+++++--++-+---+-+--+----+--+-
++--++-+++-------+++-++--++++-+-++-+-+++-----+-+--+++++--+-+-----+--++----++--++++-+-+--+++-+++--+-+-+++++---+--+---+-+---+-++--+++++--++-+---+-+--+----+--+
+++--++-+++-------+++-++--++++-+-++-+-+-+-----+-+--+++++--+-+-----+--++----++-+++++-+-+--+++-+++--+-+-+++++---+--+---+-+---+-++--+++++--++-+---+-+--+----+--
++++--++-+++-------+++-++--++++-+-++-+---+-----+-+--+++++--+-+-----+--++----++-+++++-+-+--+++-+++--+-+-+++++---+--+---+-+---+-++--+++++--++-+---+-+--+----+-
-++++--++-+++-------+++-++--++++-+-++-++--+-----+-+--+++++--+-+-----+--++----+--+++++-+-+--+++-+++--+-+-+++++---+--+---+-+---+-++--+++++--++-+---+-+--+----+
+-++++--++-+++-------+++-++--++++-+-++-++--+-----+-+--+++++--+-+-----+--++-------+++++-+-+--+++-+++--+-+-+++++---+--++--+-+---+-++--+++++--++-+---+-+--+----
-+-++++--++-+++-------+++-++--++++-+-++-++--+-----+-+--+++++--+-+-----+--++---+---+++++-+-+--+++-+++--+-+-+++++---+---+--+-+---+-++--+++++--++-+---+-+--+---
+-+-++++--++-+++-------+++-++--++++-+-+--++--+-----+-+--+++++--+-+-----+--++---+---+++++-+-+--+++-+++--+-+-+++++---+---+--+-+---+-++--+++++--++-+---+-+--+--
++-+-++++--++-+++-------+++-++--++++-+----++--+-----+-+--+++++--+-+-----+--++---+---+++++-+-+--+++-+++--+-+-+++++---+---+--+-+---+-++--+++++--++-+---+-+--+-
-++-+-++++--++-+++-------+++-++--++++-+----++--+-----+-+--+++++--+-+-----+--+++--+---+++++-+-+--+++-+++--+-+-+++++-------+--+-+---+-++--+++++--++-+---+-+--+
+-++-+-++++--++-+++-------+++-++--++++-+----++--+-----+-+--+++++--+-+-----+--+-+--+---+++++-+-+--+++-+++--+-+-+++++--+----+--+-+---+-++--+++++--++-+---+-+--
-+-++-+-++++--++-+++-------+++-++--++++++----++--+-----+-+--+++++--+-+-----+----+--+---+++++-+-+--+++-+++--+-+-+++++--+----+--+-+---+-++--+++++--++-+---+-+-
+-+-++-+-++++--++-+++-------+++-++--+++-++----++--+-----+-+--+++++--+-+-----+----+--+---+++++-+-+--+++-+++--+-+-+++++--+----+--+-+---+-++--+++++--++-+---+-+
++-+-++-+-++++--++-+++-------+++-++--++--++----++--+-----+-+--+++++--+-+-----++---+--+---+++++-+-+--+++-+++--+-+-+++++--+----+--+-+---+-++--+++++--++-+---+-
+++-+-++-+-++++--++-+++-------+++-++--++--++----++--+-----+-+--+++++--+-+-----++---+--+---+++++-+-+--+++-+++--+-+-+++-+--+----+--+-+---+-++--+++++--++-+---+
++++-+-++-+-++++--++-+++-------+++-++---+--++----++--+-----+-+--+++++--+-+----+++---+--+---+++++-+-+--+++-+++--+-+-+++-+--+----+--+-+---+-++--+++++--++-+---
-++++-+-++-+-++++--++-+++-------+++-++---+--++----++--+-----+-+--+++++--+-+---++++---+--+---+++++-+-+--+++-+++--+-+-+-+-+--+----+--+-+---+-++--+++++--++-+--
--++++-+-++-+-++++--++-+++-------+++-++---+--++----++--+-----+-+--+++++--+-+--+++++---+--+---+++++-+-+--+++-+++--+-+---+-+--+----+--+-+---+-++--+++++--++-+-
+--++++-+-++-+-++++--++-+++-------+++-+----+--++----++--+-----+-+--+++++--+-+--+++++---+--+---+++++-+-+--+++-+++--+-+---+-+--+----+--+-+---+-++--+++++--++-+
++--++++-+-++-+-++++--++-+++-------+++------+--++----++--+-----+-+--+++++--+-++-+++++---+--+---+++++-+-+--+++-+++--+-+---+-+--+----+--+-+---+-++--+++++--++-
-++--++++-+-++-+-++++--++-+++-------++++-----+--++----++--+-----+-+--+++++--+--+-+++++---+--+---+++++-+-+--+++-+++--+-+---+-+--+----+--+-+---+-++--+++++--++
+-++--++++-+-++-+-++++--++-+++-------++-+-----+--++----++--+-----+-+--+++++--++-+-+++++---+--+---+++++-+-+--+++-+++--+-+---+-+--+----+--+-+---+-++--+++++--+
++-++--++++-+-++-+-++++--++-+++-------++-+-----+--++----++--+-----+-+--+++++---+-+-+++++---+--+---+++++-+-+--+++-+++-++-+---+-+--+----+--+-+---+-++--+++++--
+++-++--++++-+-++-+-++++--++-+++--------+-+-----+--++----++--+-----+-+--+++++---+-+-+++++---+--+---+++++-+-+--+++-+++-++-+---+-+--+----+--+-+---+-++--+++++-
-+++-++--++++-+-++-+-++++--++-+++--------+-+-----+--++----++--+-----+-+--++++++--+-+-+++++---+--+---+++++-+-+--+++-++--++-+---+-+--+----+--+-+---+-++--+++++
--+++-++--++++-+-++-+-++++--++-+++-----+--+-+-----+--++----++--+-----+-+--++++++--+-+-+++++---+--+---+++++-+-+--+++-++--++-+---+-+--+----+--+-+---+-++--++++
---+++-++--++++-+-++-+-++++--++-+++----++--+-+-----+--++----++--+-----+-+--++++++--+-+-+++++---+--+---+++++-+-+--+++-++--++-+---+-+--+----+--+-+---+-++--+++
---++--+-+++-+-++-++++-++-+-+++-+--++--+---++-+-+-----+++-++-+++-----+-+-++---+++--+-+-----+--++----++--+-----+-+--++----+++-++--++++-+-++-+-++++--++-+++---
----++--+-+++-+-++-++++-++-+-+++-+--++--+---++-+-+-----+++-++-+++-----+-+-++--++++--+-+-----+--++----++--+-----+-+--+-----+++-++--++++-+-++-+-++++--++-+++--
-----++--+-+++-+-++-++++-++-+-+++-+--++--+---++-+-+-----+++-++-+++-----+-+-++-+++++--+-+-----+--++----++--+-----+-+--------+++-++--++++-+-++-+-++++--++-+++-
+-----++--+-+++-+-++-++++-++-+-+++-+--+---+---++-+-+-----+++-++-+++-----+-+-++-+++++--+-+-----+--++----++--+-----+-+--------+++-++--++++-+-++-+-++++--++-+++
++-----++--+-+++-+-++-++++-++-+-+++-+--+---+---++-+-+-----+++-++-+++-----+-+-+--+++++--+-+-----+--++----++--+-----+-++-------+++-++--++++-+-++-+-++++--++-++
-++-----++--+-+++-+-++-++++-++-+-+++-+-++---+---++-+-+-----+++-++-+++-----+-+-+--+++++--+-+-----+--++----++--+-----+-++-------+++-++--++++-+-++-+-++++--++-+
--++-----++--+-+++-+-++-++++-++-+-+++-+-++---+---++-+-+-----+++-++-+++-----+-+-+--+++++--+-+-----+--++----++--+-----++++-------+++-++--++++-+-++-+-++++--++-
+--++-----++--+-+++-+-++-++++-++-+-+++-+-++---+---++-+-+-----+++-++-+++-----+-+-+--+++++--+-+-----+--++----++--+------+++-------+++-++--++++-+-++-+-++++--++
-+--++-----++--+-+++-+-++-++++-++-+-+++-+-++---+---++-+-+-----+++-++-+++-----+-+-+--+++++--+-+-----+--++----++--+----+-+++-------+++-++--++++-+-++-+-++++--+
+-+--++-----++--+-+++-+-++-++++-++-+-+++-+-++---+---++-+-+-----+++-++-+++-------+-+--+++++--+-+-----+--++----++--+---++-+++-------+++-++--++++-+-++-+-++++--
++-+--++-----++--+-+++-+-++-++++-++-+-+-+-+-++---+---++-+-+-----+++-++-+++-------+-+--+++++--+-+-----+--++----++--+---++-+++-------+++-++--++++-+-++-+-++++-
+++-+--++-----++--+-+++-+-++-++++-++-+---+-+-++---+---++-+-+-----+++-++-+++-------+-+--+++++--+-+-----+--++----++--+---++-+++-------+++-++--++++-+-++-+-++++
-+++-+--++-----++--+-+++-+-++-++++-++-+---+-+-++---+---++-+-+-----+++-++-+++-------+-+--+++++--+-+-----+--++----++--++--++-+++-------+++-++--++++-+-++-+-+++
+-+++-+--++-----++--+-+++-+-++-++++-++-----+-+-++---+---++-+-+-----+++-++-+++-+-----+-+--+++++--+-+-----+--++----++--++--++-+++-------+++-++--++++-+-++-+-++
-+-+++-+--++-----++--+-+++-+-++-++++-++-----+-+-++---+---++-+-+-----+++-++-+++-+-----+-+--+++++--+-+-----+--++----++-+++--++-+++-------+++-++--++++-+-++-+-+
+-+-+++-+--++-----++--+-+++-+-++-++++-++-----+-+-++---+---++-+-+-----+++-++-++--+-----+-+--+++++--+-+-----+--++----++++++--++-+++-------+++-++--++++-+-++-+-
++-+-+++-+--++-----++--+-+++-+-++-++++-++-----+-+-++---+---++-+-+-----+++-++-++--+-----+-+--+++++--+-+-----+--++----+-++++--++-+++-------+++-++--++++-+-++-+
-++-+-+++-+--++-----++--+-+++-+-++-+++++++-----+-+-++---+---++-+-+-----+++-++-++--+-----+-+--+++++--+-+-----+--++----+-++++--++-+++-------+++-++--++++-+-++-
+-++-+-+++-+--++-----++--+-+++-+-++-+++-+++-----+-+-++---+---++-+-+-----+++-++-++--+-----+-+--+++++--+-+-----+--++----+-++++--++-+++-------+++-++--++++-+-++
++-++-+-+++-+--++-----++--+-+++-+-++-+++-+++-----+-+-++---+---++-+-+-----+++-+--++--+-----+-+--+++++--+-+-----+--++--+-+-++++--++-+++-------+++-++--++++-+-+
+++-++-+-+++-+--++-----++--+-+++-+-++-+++-+++-----+-+-++---+---++-+-+-----+++----++--+-----+-+--+++++--+-+-----+--++-++-+-++++--++-+++-------+++-++--++++-+-
++++-++-+-+++-+--++-----++--+-+++-+-++--++-+++-----+-+-++---+---++-+-+-----+++----++--+-----+-+--+++++--+-+-----+--++-++-+-++++--++-+++-------+++-++--++++-+
-++++-++-+-+++-+--++-----++--+-+++-+-+++-++-+++-----+-+-++---+---++-+-+-----+++----++--+-----+-+--+++++--+-+-----+--++-++-+-++++--++-+++-------+++-++--++++-
+-++++-++-+-+++-+--++-----++--+-+++-+-+++-++-+++-----+-+-++---+---++-+-+-----+++----++--+-----+-+--+++++--+-+-----+---+-++-+-++++--++-+++-------+++-++--++++
++-++++-++-+-+++-+--++-----++--+-+++-+-+++-++-+++-----+-+-++---+---++-+-+------++----++--+-----+-+--+++++--+-+-----+-+-+-++-+-++++--++-+++-------+++-++--+++
-++-++++-++-+-+++-+--++-----++--+-+++-+-+++-++-+++-----+-+-++---+---++-+-+------++----++--+-----+-+--+++++--+-+-----+++-+-++-+-++++--++-+++-------+++-++--++
+-++-++++-++-+-+++-+--++-----++--+-+++---+++-++-+++-----+-+-++---+---++-+-+---+--++----++--+-----+-+--+++++--+-+-----+++-+-++-+-++++--++-+++-------+++-++--+
-+-++-++++-++-+-+++-+--++-----++--+-+++---+++-++-+++-----+-+-++---+---++-+-+---+--++----++--+-----+-+--+++++--+-+----++++-+-++-+-++++--++-+++-------+++-++--
+-+-++-++++-++-+-+++-+--++-----++--+-++----+++-++-+++-----+-+-++---+---++-+-+---+--++----++--+-----+-+--+++++--+-+----++++-+-++-+-++++--++-+++-------+++-++-
++-+-++-++++-++-+-+++-+--++-----++--+-+-----+++-++-+++-----+-+-++---+---++-+-+---+--++----++--+-----+-+--+++++--+-+----++++-+-++-+-++++--++-+++-------+++-++
+++-+-++-++++-++-+-+++-+--++-----++--+-+-----+++-++-+++-----+-+-++---+---++-+-----+--++----++--+-----+-+--+++++--+-+-+--++++-+-++-+-++++--++-+++-------+++-+
-+++-+-++-++++-++-+-+++-+--++-----++--+-+-----+++-++-+++-----+-+-++---+---++-+-----+--++----++--+-----+-+--+++++--+-+++--++++-+-++-+-++++--++-+++-------+++-
+-+++-+-++-++++-++-+-+++-+--++-----++--+-+-----+++-++-+++-----+-+-++---+---++-+-----+--++----++--+-----+-+--+++++--+--++--++++-+-++-+-++++--++-+++-------+++
-+-+++-+-++-++++-++-+-+++-+--++-----++--+-+-----+++-++-+++-----+-+-++---+---++-+-----+--++----++--+-----+-+--+++++--++-++--++++-+-++-+-++++--++-+++-------++
--+-+++-+-++-++++-++-+-+++-+--++-----+++-+-+-----+++-++-+++-----+-+-++---+---++-+-----+--++----++--+-----+-+--+++++--++-++--++++-+-++-+-++++--++-+++-------+
+--+-+++-+-++-++++-++-+-+++-+--++-----+++-+-+-----+++-++-+++-----+-+-++---+----+-+-----+--++----++--+-----+-+--+++++-+++-++--++++-+-++-+-++++--++-+++-------
++--+-+++-+-++-++++-++-+-+++-+--++------++-+-+-----+++-++-+++-----+-+-++---+----+-+-----+--++----++--+-----+-+--+++++-+++-++--++++-+-++-+-++++--++-+++------
-++--+-+++-+-++-++++-++-+-+++-+--++------++-+-+-----+++-++-+++-----+-+-++---+-+--+-+-----+--++----++--+-----+-+--++++--+++-++--++++-+-++-+-++++--++-+++-----
--++--+-+++-+-++-++++-++-+-+++-+--++------++-+-+-----+++-++-+++-----+-+-++---+++--+-+-----+--++----++--+-----+-+--+++---+++-++--++++-+-++-+-++++--++-+++----
-+++--+-+-+++++---+--+---+++++-+-+--+++---++--+-+++-+-++-++++-++-+-+++-+--++--++++---+--++----+-+--+-+----++--+---++++++--+-+-----+--++----++--+-----+-+--++
+-+++--+-+-+++++---+--+---+++++-+-+--++----++--+-+++-+-++-++++-++-+-+++-+--++-+++++---+--++----+-+--+-+----++--+---++++++--+-+-----+--++----++--+-----+-+--+
++-+++--+-+-+++++---+--+---+++++-+-+--+-----++--+-+++-+-++-++++-++-+-+++-+--++++++++---+--++----+-+--+-+----++--+---++++++--+-+-----+--++----++--+-----+-+--
+++-+++--+-+-+++++---+--+---+++++-+-+--+-----++--+-+++-+-++-++++-++-+-+++-+--++++++++---+--++----+-+--+-+----++--+----+++++--+-+-----+--++----++--+-----+-+-
-+++-+++--+-+-+++++---+--+---+++++-+-+-++-----++--+-+++-+-++-++++-++-+-+++-+---+++++++---+--++----+-+--+-+----++--+----+++++--+-+-----+--++----++--+-----+-+
--+++-+++--+-+-+++++---+--+---+++++-+-+-++-----++--+-+++-+-++-++++-++-+-+++-+---+++++++---+--++----+-+--+-+----++--+-+--+++++--+-+-----+--++----++--+-----+-
+--+++-+++--+-+-+++++---+--+---+++++-+---++-----++--+-+++-+-++-++++-++-+-+++-+---+++++++---+--++----+-+--+-+----++--+-+--+++++--+-+-----+--++----++--+-----+
-+--+++-+++--+-+-+++++---+--+---+++++-++--++-----++--+-+++-+-++-++++-++-+-+++-+---+++++++---+--++----+-+--+-+----++--+-+--+++++--+-+-----+--++----++--+-----
+-+--+++-+++--+-+-+++++---+--+---+++++--+--++-----++--+-+++-+-++-++++-++-+-+++-+---+++++++---+--++----+-+--+-+----++--+-+--+++++--+-+-----+--++----++--+----
-+-+--+++-+++--+-+-+++++---+--+---++++++-+--++-----++--+-+++-+-++-++++-++-+-++--+---+++++++---+--++----+-+--+-+----++--+-+--+++++--+-+-----+--++----++--+---
+-+-+--+++-+++--+-+-+++++---+--+---++++++-+--++-----++--+-+++-+-++-++++-++-+-++--+---+++++++---+--++----+-+--+-+----+---+-+--+++++--+-+-----+--++----++--+--
++-+-+--+++-+++--+-+-+++++---+--+---++++++-+--++-----++--+-+++-+-++-++++-++-+-++--+---+++++++---+--++----+-+--+-+--------+-+--+++++--+-+-----+--++----++--+-
+++-+-+--+++-+++--+-+-+++++---+--+---++-+++-+--++-----++--+-+++-+-++-++++-++-+-++--+---+++++++---+--++----+-+--+-+--------+-+--+++++--+-+-----+--++----++--+
++++-+-+--+++-+++--+-+-+++++---+--+---++-+++-+--++-----++--+-+++-+-++-++++-++---++--+---+++++++---+--++----+-+--+-+--+-----+-+--+++++--+-+-----+--++----++--
+++++-+-+--+++-+++--+-+-+++++---+--+----+-+++-+--++-----++--+-+++-+-++-++++-++---++--+---+++++++---+--++----+-+--+-+--+-----+-+--+++++--+-+-----+--++----++-
-+++++-+-+--+++-+++--+-+-+++++---+--+--+-+-+++-+--++-----++--+-+++-+-++-++++-+----++--+---+++++++---+--++----+-+--+-+--+-----+-+--+++++--+-+-----+--++----++
--+++++-+-+--+++-+++--+-+-+++++---+--+-++-+-+++-+--++-----++--+-+++-+-++-++++-+----++--+---+++++++---+--++----+-+--+-+--+-----+-+--+++++--+-+-----+--++----+
---+++++-+-+--+++-+++--+-+-+++++---+--+-++-+-+++-+--++-----++--+-+++-+-++-++++-+----++--+---+++++++---+--++----+-+--+++--+-----+-+--+++++--+-+-----+--++----
+---+++++-+-+--+++-+++--+-+-+++++---+--+-++-+-+++-+--++-----++--+-+++-+-++-++++-+----++--+---+++++++---+--++----+-+---++--+-----+-+--+++++--+-+-----+--++---
-+---+++++-+-+--+++-+++--+-+-+++++---+-++-++-+-+++-+--++-----++--+-+++-+-++-++-+-+----++--+---+++++++---+--++----+-+---++--+-----+-+--+++++--+-+-----+--++--
--+---+++++-+-+--+++-+++--+-+-+++++---++++-++-+-+++-+--++-----++--+-+++-+-++-+--+-+----++--+---+++++++---+--++----+-+---++--+-----+-+--+++++--+-+-----+--++-
+--+---+++++-+-+--+++-+++--+-+-+++++---++++-++-+-+++-+--++-----++--+-+++-+-++-+--+-+----++--+---+++++++---+--++----+-----++--+-----+-+--+++++--+-+-----+--++
-+--+---+++++-+-+--+++-+++--+-+-+++++---++++-++-+-+++-+--++-----++--+-+++-+-++-+--+-+----++--+---+++++++---+--++----++----++--+-----+-+--+++++--+-+-----+--+
--+--+---+++++-+-+--+++-+++--+-+-+++++-+-++++-++-+-+++-+--++-----++--+-+++-+-++-+--+-+----++--+---+++++++---+--++----++----++--+-----+-+--+++++--+-+-----+--
---+--+---+++++-+-+--+++-+++--+-+-+++++++-++++-++-+-+++-+--++-----++--+-+++-+--+-+--+-+----++--+---+++++++---+--++----++----++--+-----+-+--+++++--+-+-----+-
+---+--+---+++++-+-+--+++-+++--+-+-++++-++-++++-++-+-+++-+--++-----++--+-+++-+--+-+--+-+----++--+---+++++++---+--++----++----++--+-----+-+--+++++--+-+-----+
++---+--+---+++++-+-+--+++-+++--+-+-++++-++-++++-++-+-+++-+--++-----++--+-+++----+-+--+-+----++--+---+++++++---+--++-+--++----++--+-----+-+--+++++--+-+-----
+++---+--+---+++++-+-+--+++-+++--+-+-++-+-++-++++-++-+-+++-+--++-----++--+-+++----+-+--+-+----++--+---+++++++---+--++-+--++----++--+-----+-+--+++++--+-+----
++++---+--+---+++++-+-+--+++-+++--+-+-++-+-++-++++-++-+-+++-+--++-----++--+-+++----+-+--+-+----++--+---+++++++---+--+--+--++----++--+-----+-+--+++++--+-+---
+++++---+--+---+++++-+-+--+++-+++--+-+-++-+-++-++++-++-+-+++-+--++-----++--+-+++----+-+--+-+----++--+---+++++++---+-----+--++----++--+-----+-+--+++++--+-+--
-+++++---+--+---+++++-+-+--+++-+++--+-++++-+-++-++++-++-+-+++-+--++-----++--+--++----+-+--+-+----++--+---+++++++---+-----+--++----++--+-----+-+--+++++--+-+-
+-+++++---+--+---+++++-+-+--+++-+++--+--+++-+-++-++++-++-+-+++-+--++-----++--+--++----+-+--+-+----++--+---+++++++---+-----+--++----++--+-----+-+--+++++--+-+
-+-+++++---+--+---+++++-+-+--+++-+++--++-+++-+-++-++++-++-+-+++-+--++-----++--+--++----+-+--+-+----++--+---+++++++---+-----+--++----++--+-----+-+--+++++--+-
+-+-+++++---+--+---+++++-+-+--+++-+++---+-+++-+-++-++++-++-+-+++-+--++-----++--+--++----+-+--+-+----++--+---+++++++---+-----+--++----++--+-----+-+--+++++--+
-+-+-+++++---+--+---+++++-+-+--+++-+++---+-+++-+-++-++++-++-+-+++-+--++-----++--+--++----+-+--+-+----++--+---+++++++-+-+-----+--++----++--+-----+-+--+++++--
--+-+-+++++---+--+---+++++-+-+--+++-++++--+-+++-+-++-++++-++-+-+++-+--++-----+---+--++----+-+--+-+----++--+---+++++++-+-+-----+--++----++--+-----+-+--+++++-
+--+-+-+++++---+--+---+++++-+-+--+++-++++--+-+++-+-++-++++-++-+-+++-+--++-----+---+--++----+-+--+-+----++--+---++++++--+-+-----+--++----++--+-----+-+--+++++
++--+-+-+++++---+--+---+++++-+-+--+++-+-++--+-+++-+-++-++++-++-+-+++-+--++----++---+--++----+-+--+-+----++--+---++++++--+-+-----+--++----++--+-----+-+--++++
+++--+-+-+++++---+--+---+++++-+-+--+++---++--+-+++-+-++-++++-++-+-+++-+--++---+++---+--++----+-+--+-+----++--+---++++++--+-+-----+--++----++--+-----+-+--+++
</content>

<content full_path="exllamav3/tokenizer/__init__.py">
from .tokenizer import Tokenizer
</content>

<content full_path="exllamav3/tokenizer/tokenizer.py">
from __future__ import annotations
import torch
import os, json, re
from tokenizers import Tokenizer as HFTokenizer, models
from exllamav3.util import synchronized
from exllamav3.util.file import maybe_read_json
from exllamav3.models import Config
from functools import lru_cache

class Tokenizer:

    def __init__(
        self,
        config: Config,
    ):
        self.config = config

        # Defaults
        self.unk_token = "<unk>"
        self.bos_token = "<s>"
        self.eos_token = "</s>"
        self.pad_token = ""
        self.newline_token = "\n"
        self.space_token = " "
        self.special_delimiters = None
        self.unspecial_delimiters = None
        self.extended_piece_to_id = {}
        self.extended_id_to_piece = {}
        self.unspecial_piece_to_id = {}
        self.unspecial_id_to_piece = {}
        self.vocab = None

        # Regex
        self.ord_exp = re.compile(r"^<0x([0-9A-Fa-f]+)>$")

        # Files
        self.path_tokenizer_json = os.path.join(self.config.directory, "tokenizer.json")
        self.path_tokenizer_config_json = os.path.join(self.config.directory, "tokenizer_config.json")
        self.path_added_tokens_json = os.path.join(self.config.directory, "added_tokens.json")
        self.tokenizer = HFTokenizer.from_file(self.path_tokenizer_json)
        self.tokenizer_config_dict = maybe_read_json(self.path_tokenizer_config_json)
        self.added_tokens_dict = maybe_read_json(self.path_added_tokens_json)

        # Disable truncation
        self.tokenizer.no_truncation()

        # Deduce placeholders used for space and newline chars in raw vocabulary
        self.space_char_ = " "
        self.newline_char_ = "\n"
        m = self.tokenizer.model
        if isinstance(m, models.BPE):
            def deduce_char_map(input_char):
                char_id = self.tokenizer.encode(input_char, add_special_tokens = False).ids[-1]
                char_str = self.tokenizer.id_to_token(char_id)
                match = self.ord_exp.match(char_str)
                if match:
                    h = match.group(1)
                    o = int(h, 16)
                    char_str = chr(o)
                else:
                    char_str = char_str[-1]
                return char_str
            self.space_char_ = deduce_char_map(" ")  # "Ġ"
            self.newline_char_ = deduce_char_map("\n")  # "Ċ"

        # Add tokens from added_tokens.json if present, assume they're all special
        self.extended_piece_to_id.update(self.added_tokens_dict)

        # Add special tokens from tokenizer_config.json
        atd = self.tokenizer_config_dict.get("added_tokens_decoder", {})
        for (k, v) in atd.items():
            if not v["special"]:
                continue
            token_id = int(k)
            token_str = v["content"]
            self.extended_piece_to_id[token_str] = token_id

        # Remove unspecial added tokens that exist in the base tokenizer already, but only if they decode correctly
        # see https://github.com/huggingface/tokenizers/issues/1392
        ok_tokens = []
        for p, i in self.unspecial_piece_to_id.items():
            try:
                itp = self.tokenizer.decode([i])
                if itp == p: ok_tokens.append(p)
            except IndexError:
                pass
        for t in ok_tokens: del self.unspecial_piece_to_id[t]

        # Invert extended dictionaries
        self.extended_id_to_piece = {v: k for k, v in self.extended_piece_to_id.items()}
        self.unspecial_id_to_piece = {v: k for k, v in self.unspecial_piece_to_id.items()}

        # Get control token IDs
        ut = self.tokenizer.model.unk_token
        self.unk_token_id = None if ut is None else self.tokenizer.token_to_id(ut)
        self.eos_token_id = self.config.eos_token_id
        self.bos_token_id = self.config.bos_token_id
        self.pad_token_id = self.config.pad_token_id

        # If model config doesn't specify BOS and EOS tokens, try to load from tokenizer config
        def get_default_token_id(config_key: str, current: int | None, default: int | None):
            if current is not None: return current
            if self.tokenizer_config_dict is not None and config_key in self.tokenizer_config_dict:
                st = self.tokenizer_config_dict[config_key]
                if st is None: return None
                if isinstance(st, dict):
                    stc: str | None = st.get("content", None)
                    if stc is None:
                        return None
                    else:
                        return self.tokenizer.token_to_id(stc)
                elif isinstance(st, str):
                    return self.tokenizer.token_to_id(st)
                else:
                    return None
            else:
                return default
        self.pad_token_id = get_default_token_id("pad_token", self.pad_token_id, None)
        self.bos_token_id = get_default_token_id("bos_token", self.bos_token_id, 1)
        self.eos_token_id = get_default_token_id("eos_token", self.eos_token_id, 2)

        # Get control token strings
        self.unk_token = self.tokenizer.model.unk_token
        self.bos_token = None if self.bos_token_id is None else \
            (self.extended_id_to_piece.get(self.bos_token_id) or self.tokenizer.id_to_token(self.bos_token_id))
        self.eos_token = None if self.eos_token_id is None else \
            (self.extended_id_to_piece.get(self.eos_token_id) or self.tokenizer.id_to_token(self.eos_token_id))

        # Use "<pad>" or BOS token as fallback for padding token
        if self.pad_token_id is None:
            pad_test = self.tokenizer.token_to_id("<pad>")
            if pad_test:
                self.pad_token_id = pad_test
            elif self.eos_token_id != self.bos_token_id:
                self.pad_token_id = self.eos_token_id
            else:
                self.pad_token_id = -1

        # Special case if <unk> and <pad> have the same ID
        if self.unk_token_id == self.pad_token_id:
            self.unk_token = self.pad_token

        # Make sure extended vocab contains control tokens, but avoid empty pieces
        if self.unk_token:
            self.extended_piece_to_id[self.unk_token] = self.unk_token_id
            self.extended_id_to_piece[self.unk_token_id] = self.unk_token
        if self.bos_token:
            self.extended_piece_to_id[self.bos_token] = self.bos_token_id
            self.extended_id_to_piece[self.bos_token_id] = self.bos_token
        if self.eos_token:
            self.extended_piece_to_id[self.eos_token] = self.eos_token_id
            self.extended_id_to_piece[self.eos_token_id] = self.eos_token

        self.actual_vocab_size = 1 + max(
            list(self.extended_id_to_piece.keys()) + \
            list(self.unspecial_id_to_piece.keys()) + \
            [self.tokenizer.get_vocab_size() - 1]
        )

        # Useful token IDs
        try:
            self.newline_token_id = self.tokenizer.encode(self.newline_token, add_special_tokens = False).ids[-1]
        except:
            self.newline_token_id = None
        try:
            self.space_token_id = self.tokenizer.encode(self.space_token, add_special_tokens = False).ids[-1]
        except:
            self.space_token_id = None

        # Dictionaries and lists
        self.id_to_ord = None
        self.id_to_piece = None
        self.id_to_piece_with_special = None
        self.piece_to_id = None
        self.get_id_to_ord_list()
        self.get_id_to_piece_list(True)
        self.get_id_to_piece_list(False)
        self.get_piece_to_id_dict()


    # Get single token

    def single_token(self, token_id: int) -> torch.Tensor:
        """
        Get single token as tensor

        :param token_id:
            Token ID

        :return:
            LongTensor of shape (1, 1) of token ID
        """
        return torch.tensor([[token_id]], dtype = torch.long)

    def single_id(self, token: str) -> int:
        """
        Get the ID of a single token from exact string match

        :param token:
            Token

        :return:
            int
        """
        tid = self.extended_piece_to_id.get(token, self.get_piece_to_id_dict().get(token))
        return tid

    # Encode string with added, unspecial tokens

    def encode_unspecial(self, text: str) -> list[int]:
        if not self.unspecial_piece_to_id:
            return self.tokenizer.encode(text, add_special_tokens = False).ids

        if self.unspecial_delimiters is None:
            self.unspecial_delimiters = re.compile(
                "(" + "|".join(map(re.escape, self.unspecial_piece_to_id.keys())) + ")")

        split = self.unspecial_delimiters.split(text)
        encoded = []

        i = 0
        while i < len(split):
            if split[i] != "": encoded += self.tokenizer.encode(split[i], add_special_tokens = False).ids
            if i + 1 < len(split): encoded += [self.unspecial_piece_to_id[split[i + 1]]]
            i += 2

        return encoded

    # Encode string with special tokens

    def encode_special(self, text: str) -> list[int]:
        # if self.special_delimiters is None:
        #     self.special_delimiters = re.compile("(" + "|".join(map(re.escape, self.extended_piece_to_id.keys())) + ")")
        #
        # split = self.special_delimiters.split(text)
        # encoded = []
        #
        # i = 0
        # while i < len(split):
        #     if split[i] != "": encoded += self.tokenizer.encode(split[i], add_special_tokens = False).ids
        #     if i + 1 < len(split): encoded += [self.extended_piece_to_id[split[i + 1]]]
        #     i += 2

        # TODO: Test if the above is actually no longer needed (was written for SentencePiece lib)
        encoded = self.tokenizer.encode(text, add_special_tokens = False).ids
        return encoded

    def encode_special_or_unspecial(
        self,
        text: str,
        special: bool,
        # embeddings: list[ExLlamaV2MMEmbedding]
    ):
        out_parts = []

        # if embeddings:
        #     aliases = {e.text_alias: e for e in embeddings}
        #     split_pattern = r"(" + "|".join(re.escape(k) for k in aliases.keys()) + ")"
        #     in_parts = re.split(split_pattern, text)
        # else:
        aliases = {}
        in_parts = [text]

        for text in in_parts:
            if text in aliases:
                out_parts += aliases[text].get_ids()
            else:
                if special:
                    out_parts += self.encode_special(text)
                else:
                    out_parts += self.encode_unspecial(text)

        return out_parts

    # Encode string or list of strings

    def encode(
        self,
        text: str | list[str],
        add_bos: bool = False,
        add_eos: bool = False,
        encode_special_tokens: bool = False,
        return_offsets: bool = False,
        # embeddings: list[ExLlamaV2MMEmbedding] | None = None
    ) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]:

        """
        Encode string or list of strings

        :param text:
            str or list[str]: Input text

        :param add_bos:
            Prepend BOS token before each sequence

        :param add_eos:
            Append EOS token to each sequence

        :param encode_special_tokens:
            Encode any special tokens that appear in the input as such. If False, substrings like "<bos>"
            will be encoded as text.

        :param return_offsets:
            Also return a tensor of padding lengths

        # :param embeddings:
        #     List of ExLlamaV2MMEmbeddings. If present, aliases in the input will be replaced with token ID ranges.

        :return:
            Tensor of shape (batch_size, max_seq_len), optionally offsets Tensor of shape (batch_size)
        """

        # if embeddings is None:
        embeddings = []

        if isinstance(text, list):

            # text is a list of strings

            list_ids = [self.encode_special_or_unspecial(t, encode_special_tokens) for t in text]

            if add_bos and self.bos_token_id is not None:
                for ids in list_ids: ids.insert(0, self.bos_token_id)
            if add_eos and self.eos_token_id is not None:
                for ids in list_ids: ids.append(self.eos_token_id)

            max_length = max([len(ids) for ids in list_ids])

            padded_ids = []
            offsets = []
            for ids in list_ids:
                padding_length = max_length - len(ids)
                padding = torch.full((padding_length,), self.pad_token_id)
                padded_ids.append(torch.cat((padding, torch.tensor(ids)), dim=0))
                offsets.append(-padding_length)

            stacked_ids = torch.stack(padded_ids, dim=0)

            if return_offsets:
                return stacked_ids, torch.tensor(offsets, dtype=torch.int)
            else:
                return stacked_ids

        else:

            # text is a single string

            # ids = self.encode_special(text) if encode_special_tokens else self.encode_unspecial(text)
            ids = self.encode_special_or_unspecial(text, encode_special_tokens)
            if add_bos and self.bos_token_id is not None:
                ids.insert(0, self.bos_token_id)
            if add_eos and self.eos_token_id is not None:
                ids.append(self.eos_token_id)

            ids = torch.tensor(ids).to(torch.long).unsqueeze(0)
            if return_offsets:
                return ids, torch.tensor([0], dtype=torch.int)
            else:
                return ids

    # Decode sequence with added, unspecial tokens

    def decode_unspecial(self, seq):

        if not self.unspecial_id_to_piece:
            return self.tokenizer.decode(seq)

        text = ""
        start = 0
        end = 0
        while end < len(seq):
            if seq[end] in self.unspecial_id_to_piece:
                if end > start: text += self.tokenizer.decode(seq[start: end])
                text += self.unspecial_id_to_piece[seq[end]]
                end += 1
                start = end
            else:
                end += 1
        if end > start: text += self.tokenizer.decode(seq[start: end])
        return text

    # Decode sequence with or without special tokens

    def decode_(self, seq, decode_special_tokens):

        if not decode_special_tokens:

            max_token = self.tokenizer.get_vocab_size()
            seq = [t for t in seq if (t != self.pad_token_id and t < max_token and t != self.eos_token_id)]
            if self.eos_token_id in seq: seq = seq[:seq.index(self.eos_token_id)]
            return self.decode_unspecial(seq)

        else:

            max_token = self.tokenizer.get_vocab_size()
            seq = [t for t in seq if (t != self.pad_token_id and t < max_token)]
            text = ""
            start = 0
            end = 0
            while end < len(seq):
                if seq[end] in self.extended_id_to_piece:
                    if end > start: text += self.tokenizer.decode(seq[start: end])
                    text += self.extended_id_to_piece[seq[end]]
                    end += 1
                    start = end
                else:
                    end += 1
            if end > start: text += self.decode_unspecial(seq[start: end])

        return text

    # Decode IDs, or a list of IDs

    def decode(
        self,
        ids: torch.Tensor | list[torch.Tensor],
        decode_special_tokens: bool = False
    ) -> str | list[str]:
        """
        Decode IDs or list of IDs

        :param ids:
            Tensor of ids, shape (batch_size, max_seq_len)

        :param decode_special_tokens:
            Decode special tokens in the input to their text representation. If False, tokens such as
            "<bos>" will become empty strings in the output.

        :return:
            str if batch_size == 1, else list[str]
        """

        if isinstance(ids, list):

            texts = []
            for i in ids:
                texts.append(self.decode(i, decode_special_tokens))
            return texts

        assert isinstance(ids, torch.Tensor), "ids must be Tensor"

        if ids.dim() > 1:

            texts = []
            for i in range(ids.shape[0]):
                seq = ids[i].tolist()
                texts.append(self.decode_(seq, decode_special_tokens))
            return texts

        else:

            ids = ids.tolist()
            text = self.decode_(ids, decode_special_tokens)
            return text

    # Create padding mask

    def padding_mask(self, ids: torch.Tensor) -> torch.Tensor:
        """
        Create padding mask corresponding to IDs tensor

        :param ids:
            Token IDs

        :return:
            Additive bias Tensor of -inf where ids == pad_token_id, 0 elsewhere
        """

        mask_bool = (ids == self.pad_token_id)
        mask = mask_bool.int()
        mask *= -65505 * 2
        mask = mask.half()
        return mask

    def num_tokens(self, text):
        """
        Measure tokenized length of text

        :param text:
            Input text

        :return:
            Tokenized length of text
        """

        ids = self.tokenizer.encode(text)
        return len(ids)

    # Get ordinals of single-byte tokens

    @synchronized
    def get_id_to_ord_list(self):
        if self.id_to_ord is not None: return self.id_to_ord

        self.id_to_ord = []
        for idx in range(self.tokenizer.get_vocab_size()):
            p = self.tokenizer.id_to_token(idx)
            if not p: p = ""
            self.id_to_ord.append(self.tokenizer.token_to_id(p))

        def clean_special_chars(p):
            p = p.replace(self.space_char_, " ")
            p = p.replace(self.newline_char_, "\n")
            return p

        def piece_to_ord(p):
            match = self.ord_exp.match(p)
            if match:
                h = match.group(1)
                return int(h, 16)
            if len(p) == 1:
                p = clean_special_chars(p)
                o = ord(p)
                if o <= 255: return o
            return -1

        i = self.tokenizer.get_vocab_size()
        while True:
            if i in self.extended_id_to_piece:
                self.id_to_ord.append(piece_to_ord(self.extended_id_to_piece[i]))
            elif i in self.unspecial_id_to_piece:
                self.id_to_ord.append(piece_to_ord(self.unspecial_id_to_piece[i]))
            elif i < self.actual_vocab_size:
                self.id_to_ord.append(-1)
            else:
                break
            i += 1

        return self.id_to_ord

    # Copy vocabulary from model

    @synchronized
    def get_id_to_piece_list(self, include_special_tokens = False):

        def enumerate_tokens():
            if self.vocab is not None: return enumerate(self.vocab)
            self.vocab = []

            test_enc = self.tokenizer.encode(" t", add_special_tokens=False)
            test_count = len(test_enc.ids)
            assert test_count > 0, "Tokenizer error, test string encodes to zero tokens"
            test_id = test_enc.ids[0]
            test_piece = self.tokenizer.decode([test_id])

            if test_count == 1 and len(test_piece) == len(" t"):
                for i in range(self.tokenizer.get_vocab_size()):
                    d = self.tokenizer.decode([i])
                    self.vocab.append(d)
            else:
                prefix_id = self.tokenizer.encode(" ", add_special_tokens=False).ids[0]
                prefix_piece = self.tokenizer.decode([prefix_id])
                prefix_len = len(prefix_piece)
                for i in range(self.tokenizer.get_vocab_size()):
                    dt = self.tokenizer.decode([prefix_id, i])
                    d = dt[prefix_len:]
                    self.vocab.append(d)

            return enumerate(self.vocab)

        if include_special_tokens:
            if self.id_to_piece_with_special is not None: return self.id_to_piece_with_special

            id_to_piece_extended = self.get_id_to_piece_list().copy()
            for k, v in self.extended_id_to_piece.items():
                id_to_piece_extended[k] = v

            self.id_to_piece_with_special = id_to_piece_extended
            return self.id_to_piece_with_special

        if self.id_to_piece is not None: return self.id_to_piece

        self.id_to_piece = [""] * self.tokenizer.get_vocab_size()
        for idx, p in enumerate_tokens():
            self.id_to_piece[idx] = p

        i = self.tokenizer.get_vocab_size()
        while True:
            if i in self.extended_id_to_piece:
                self.id_to_piece.append(self.extended_id_to_piece[i])
            elif i in self.unspecial_id_to_piece:
                self.id_to_piece.append(self.unspecial_id_to_piece[i])
            elif i < self.actual_vocab_size:
                self.id_to_piece.append("��_undefined_token_��")
            else:
                break
            i += 1

        return self.id_to_piece

    @synchronized
    def get_piece_to_id_dict(self):
        if self.piece_to_id is not None: return self.piece_to_id
        all_pieces = self.get_id_to_piece_list()
        self.piece_to_id = {piece: idx for idx, piece in enumerate(all_pieces)}
        return self.piece_to_id

    @staticmethod
    def from_config(config: Config):
        return Tokenizer(config)

    @lru_cache(1000)
    def get_tokens_with_prefix_string(self, prefix: str):
        """
        Return list of token IDs for pieces that start with the given prefix
        """
        id_to_piece = self.get_id_to_piece_list()
        tokens = [idx for idx, piece in enumerate(id_to_piece) if piece.startswith(prefix)]
        return tokens

    @lru_cache(1000)
    def get_tokens_with_prefix_id(self, prefix_id: int):
        """
        Return list of token IDs for pieces that start with the given prefix
        """
        id_to_piece = self.get_id_to_piece_list()
        prefix = id_to_piece[prefix_id]
        return self.get_tokens_with_prefix_string(prefix)

</content>

<content full_path="exllamav3/exllamav3_ext/rope.cuh">
#pragma once

#include <ATen/Tensor.h>

void rope
(
    const at::Tensor& q,
    at::Tensor& out_q,
    const c10::optional<at::Tensor>& k,
    c10::optional<at::Tensor>& out_k,
    const at::Tensor& inv_freq,
    uint32_t position,
    const c10::optional<at::Tensor>& positions,
    const c10::optional<at::Tensor>& position_ids,
    int rope_mode,
    float attn_factor,
    const c10::optional<at::Tensor>& q_norm,
    const c10::optional<at::Tensor>& k_norm,
    float norm_eps,
    float norm_constant_bias
);
</content>

<content full_path="exllamav3/exllamav3_ext/norm.cu">
#include "norm.cuh"
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include "util.h"
#include "util.cuh"

#define NUM_THREADS 1024

#if defined(USE_ROCM)
#define NUM_WARPS (1024 / warpSize)
#define WARP_SIZE (warpSize)
#else
#define NUM_WARPS 32
#define WARP_SIZE 32
#endif

__device__ inline float reduce(float sum, int warp_id, int lane_id)
{
    // Shuffle to sum across lanes
    __shared__ float sums[NUM_WARPS];
    for(int offset = warpSize / 2; offset > 0; offset /= 2) sum += __shfl_xor_sync(0xffffffff, sum, offset);
    if (lane_id == 0) sums[warp_id] = sum;
    __syncthreads();

    // Load partial sums from across warps, shuffle again across lanes
    #if defined(USE_ROCM)
        sum = lane_id < NUM_WARPS ? sums[lane_id] : 0.0f;
    #else
        sum = sums[lane_id];
    #endif
    for(int offset = warpSize / 2; offset > 0; offset /= 2) sum += __shfl_xor_sync(0xffffffff, sum, offset);

    return sum;
}

__device__ inline void read_half4(float4& f4, const half4* addr, bool clamp)
{
    half4 h4;
    READ64(h4, addr);
    f4.x = LOW_TO_FLOAT(h4.x);
    f4.y = HIGH_TO_FLOAT(h4.x);
    f4.z = LOW_TO_FLOAT(h4.y);
    f4.w = HIGH_TO_FLOAT(h4.y);
    if (clamp)
    {
        f4.x = CLAMP_FP16(f4.x);
        f4.y = CLAMP_FP16(f4.y);
        f4.z = CLAMP_FP16(f4.z);
        f4.w = CLAMP_FP16(f4.w);
    }
}

__device__ inline void read_float4(float4& f4, const float4* addr)
{
    READ128(f4, addr);
}

__device__ inline void write_half4(const float4& f4, half4* addr)
{
    half4 h4
    (
        __halves2half2(__float2half_rn(f4.x), __float2half_rn(f4.y)),
        __halves2half2(__float2half_rn(f4.z), __float2half_rn(f4.w))
    );
    WRITE64(addr, h4);
}

__device__ inline void write_float4(const float4& f4, float4* addr)
{
    WRITE128(addr, f4);
}

__device__ inline float sum_sq4(float lsum, const float4& f4)
{
    lsum = fma(f4.x, f4.x, lsum);
    lsum = fma(f4.y, f4.y, lsum);
    lsum = fma(f4.z, f4.z, lsum);
    lsum = fma(f4.w, f4.w, lsum);
    return lsum;
}

__device__ inline void apply4(float4& x4, const float4& w4, const float rmf)
{
    x4.x = x4.x * w4.x * rmf;
    x4.y = x4.y * w4.y * rmf;
    x4.z = x4.z * w4.z * rmf;
    x4.w = x4.w * w4.w * rmf;
}

template <typename input_t, typename output_t>
__global__ __launch_bounds__(NUM_THREADS)
void rms_norm_kernel
(
    const input_t* __restrict__ x,
    const half* __restrict__ w,
    output_t* __restrict__ y,
    const float epsilon,
    const int rows,
    const int dim,
    float constant_bias
)
{
    constexpr bool input_fp32 = std::is_same_v<input_t, float>;
    constexpr bool output_fp32 = std::is_same_v<output_t, float>;
    constexpr bool input_fp16 = std::is_same_v<input_t, half>;
    constexpr bool output_fp16 = std::is_same_v<output_t, half>;
    static_assert(input_fp32 || input_fp16, "rms_norm_kernel: input must be float or half type");
    static_assert(output_fp32 || output_fp16, "rms_norm_kernel: output must be float or half type");

    int t = threadIdx.x;
    int warp_id = threadIdx.x / WARP_SIZE;
    int lane_id = threadIdx.x % WARP_SIZE;
    int row = blockIdx.x;

    int columns = dim / 4;

    // Compute sum of squares
    float sum = 0.0f;
    for (int column = t; column < columns; column += NUM_THREADS)
    {
        float4 x4;
        if constexpr (input_fp16) read_half4(x4, ((const half4*) (x + row * dim)) + column, true);
        if constexpr (input_fp32) read_float4(x4, ((const float4*) (x + row * dim)) + column);
        sum = sum_sq4(sum, x4);
    }
    sum = reduce(sum, warp_id, lane_id);

    // Get norm
    float rmf = rsqrtf(sum / (float)dim + epsilon);

    // Normalize x, scaling by w
    for (int column = t; column < columns; column += NUM_THREADS)
    {
        float4 x4;
        if constexpr (input_fp16) read_half4(x4, ((const half4*) (x + row * dim)) + column, true);
        if constexpr (input_fp32) read_float4(x4, ((const float4*) (x + row * dim)) + column);

        float4 w4;
        read_half4(w4, ((const half4*) w) + column, false);
        if (constant_bias != 0.0f)
        {
            w4.x += constant_bias;
            w4.y += constant_bias;
            w4.z += constant_bias;
            w4.w += constant_bias;
        }

        apply4(x4, w4, rmf);

        if constexpr (output_fp16) write_half4(x4, ((half4*) (y + row * dim)) + column);
        if constexpr (output_fp32) write_float4(x4, ((float4*) (y + row * dim)) + column);
    }
}

/*
Compute RMSNorm: y = x * w / sqrt(row_mean(x * x) + epsilon)
- Can operate in-place if y == x
- x can be either float or half dtype
- y can be either float or half dtype
- w must be half dtype
*/
void rms_norm
(
    at::Tensor x,
    at::Tensor w,
    at::Tensor y,
    float epsilon,
    float constant_bias
)
{
    TORCH_CHECK_DTYPE(w, kHalf);
    TORCH_CHECK_DIV(x, -1, 4);
    TORCH_CHECK_SHAPES(x, -1, w, 0, 1);
    TORCH_CHECK_SHAPES_FULL(x, y);

    const at::cuda::OptionalCUDAGuard device_guard(x.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    bool input_fp32 = x.dtype() == at::kFloat;
    bool output_fp32 = y.dtype() == at::kFloat;
    bool input_fp16 = !input_fp32;
    bool output_fp16 = !output_fp32;
    int rows = 1;
    for (int i = 0; i < x.dim() - 1; ++i) rows *= x.size(i);
    int dim = x.size(-1);
    dim3 blockDim(NUM_THREADS, 1, 1);
    dim3 gridDim(rows, 1, 1);

    if (input_fp16 && output_fp16)
        rms_norm_kernel<<<gridDim, blockDim, 0, stream>>>
        (
            (const half*) x.data_ptr(),
            (const half*) w.data_ptr(),
            (half*) y.data_ptr(),
            epsilon,
            rows,
            dim,
            constant_bias
        );
    else if (input_fp16 && output_fp32)
        rms_norm_kernel<<<gridDim, blockDim, 0, stream>>>
        (
            (const half*) x.data_ptr(),
            (const half*) w.data_ptr(),
            (float*) y.data_ptr(),
            epsilon,
            rows,
            dim,
            constant_bias
        );
    else if (input_fp32 && output_fp16)
        rms_norm_kernel<<<gridDim, blockDim, 0, stream>>>
        (
            (const float*) x.data_ptr(),
            (const half*) w.data_ptr(),
            (half*) y.data_ptr(),
            epsilon,
            rows,
            dim,
            constant_bias
        );
    else if (input_fp32 && output_fp32)
        rms_norm_kernel<<<gridDim, blockDim, 0, stream>>>
        (
            (const float*) x.data_ptr(),
            (const half*) w.data_ptr(),
            (float*) y.data_ptr(),
            epsilon,
            rows,
            dim,
            constant_bias
        );
    else
        TORCH_CHECK(false, "rms_norm: Invalid datatypes for input/output, must be half or float")
}
</content>

<content full_path="exllamav3/exllamav3_ext/compat.cuh">
#pragma once

// Approximate tanh

__forceinline__ __device__ float copysignf_pos(float a, float b)
{
    float r;
    r = __int_as_float(__float_as_int(a) | (__float_as_int(b) & 0x80000000));
    return r;
}

#if defined(USE_ROCM) || (defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 750 || CUDART_VERSION < 11000))

__inline__ __device__ float tanh_opt(float x)
{
    const float exp_val = -1.f * fabs(2 * x);
    return copysignf_pos((1.0f - __expf(exp_val)) / (__expf(exp_val) + 1.0f), x);
}

#else

__inline__ __device__ float tanh_opt(float x)
{
    float r;
    asm("tanh.approx.f32 %0,%1; \n\t" : "=f"(r) : "f"(x));
    return r;
}

#endif

</content>

<content full_path="exllamav3/exllamav3_ext/histogram.cu">
#include "histogram.cuh"
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include "util.h"
#include "util.cuh"
#include <cmath>

#define NUM_THREADS 1024
#define MAX_BINS 1024

template <typename T>
__global__ __launch_bounds__(NUM_THREADS)
void histogram_kernel
(
    const T* __restrict__ input_ptr,
    unsigned long long* __restrict__ output_ptr,
    uint64_t numel,
    uint64_t num_bins,
    float min_value,
    float max_value,
    bool exclusive
)
{
    __shared__ unsigned long long histogram[MAX_BINS];

    // Clear
    for (int i = threadIdx.x; i < num_bins; i += NUM_THREADS)
        histogram[i] = 0;
    __syncthreads();

    // Count
    for (size_t i = threadIdx.x; i < numel; i += NUM_THREADS)
    {
        float val;
        if constexpr (std::is_same_v<T, float>)
            val = input_ptr[i];
        else
            val = __half2float(input_ptr[i]);

        if (exclusive)
        {
            if (val < min_value) continue;
            if (val > max_value) continue;
        }

        val -= min_value;
        val /= (max_value - min_value);
        val *= (float) num_bins;
        int idx = (int) val;
        if (idx < 0) idx = 0;
        if (idx >= num_bins) idx = num_bins - 1;
        atomicAdd(&histogram[idx], 1);
    }
    __syncthreads();

    // Write
    for (int i = threadIdx.x; i < num_bins; i += NUM_THREADS)
        output_ptr[i] = histogram[i];
}

/*
Compare tensor distribution to codebook (not optimized)

input: tensor, float, any shape
output: (empty) output histogram, uint64_t, shape (num_bins,)
*/

void histogram
(
    at::Tensor& input,
    at::Tensor& output,
    float min_value,
    float max_value,
    bool exclusive
)
{
    const at::cuda::OptionalCUDAGuard device_guard(input.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    bool float32;
    if (input.dtype() == at::kFloat)
        float32 = true;
    else if (input.dtype() == at::kHalf)
        float32 = false;
    else TORCH_CHECK(false, "incorrect input datatype");

    TORCH_CHECK_DTYPE(output, kLong);

    uint64_t numel = input.numel();
    uint64_t num_bins = output.numel();
    TORCH_CHECK(num_bins <= MAX_BINS, "Too many bins");

    if (float32)
        histogram_kernel<float><<<1, NUM_THREADS, 0, stream>>>
        (
            (const float*) input.data_ptr(),
            (unsigned long long*) output.data_ptr(),
            numel,
            num_bins,
            min_value,
            max_value,
            exclusive
        );
    else
        histogram_kernel<half><<<1, NUM_THREADS, 0, stream>>>
        (
            (const half*) input.data_ptr(),
            (unsigned long long*) output.data_ptr(),
            numel,
            num_bins,
            min_value,
            max_value,
            exclusive
        );

    cuda_check(cudaPeekAtLastError());
}
</content>

<content full_path="exllamav3/exllamav3_ext/util.cuh">
#pragma once

typedef struct __align__(8) half4
{
    half2 x;
    half2 y;
    __host__ __device__ half4() = default;
    __host__ __device__ half4(half2 x_, half2 y_) : x(x_), y(y_) {}
    __host__ __device__ half4(half h0, half h1, half h2, half h3) :
         x(__halves2half2(h0, h1)),
         y(__halves2half2(h2, h3)) {}
}
half4;

typedef struct __align__(16) half8
{
    half2 x;
    half2 y;
    half2 z;
    half2 w;
    __host__ __device__ half8() = default;
    __host__ __device__ half8(half2 x_, half2 y_, half2 z_, half2 w_) : x(x_), y(y_), z(z_), w(w_) {}
    __host__ __device__ half8(half h0, half h1, half h2, half h3, half h4, half h5, half h6, half h7) :
         x(__halves2half2(h0, h1)),
         y(__halves2half2(h2, h3)),
         z(__halves2half2(h4, h5)),
         w(__halves2half2(h6, h7)) {}
}
half8;

struct Dim3
{
    int m;
    int k;
    int n;
    inline __device__ int numel_a() { return m * k; }
    inline __device__ int numel_b() { return k * n; }
    inline __device__ int numel_c() { return m * n; }
};

#define READ128(__x, __y) ((uint4*)&__x)[0] = ((uint4*)(__y))[0];
#define WRITE128(__x, __y) ((uint4*)__x)[0] = ((uint4*)(&__y))[0];
#define READ64(__x, __y) ((uint2*)&__x)[0] = ((uint2*)(__y))[0];
#define WRITE64(__x, __y) ((uint2*)__x)[0] = ((uint2*)(&__y))[0];

#define LOW_TO_FLOAT(__x) __half2float(__low2half(__x))
#define HIGH_TO_FLOAT(__x) __half2float(__high2half(__x))

#define CLAMP(__x, __min, __max) fmaxf(__min, fminf(__x, __max))
#define CLAMP_FP16(__x) CLAMP(__x, -65504.0f, 65504.0f)

#define SWAP16(__x) __byte_perm(__x, 0, 0x1032)

union half2_uint32
{
    uint32_t as_uint32;
    half2 as_half2;
    __device__ half2_uint32(uint32_t val) : as_uint32(val) {}
    __device__ half2_uint32(half2 val) : as_half2(val) {}
    __device__ half2_uint32() : as_uint32(0) {}
};

union half_uint16
{
    uint16_t as_uint16;
    half as_half;
    __device__ half_uint16(uint16_t val) : as_uint16(val) {}
    __device__ half_uint16(half val) : as_half(val) {}
    __device__ half_uint16() : as_uint16(0) {}
};

#define cuda_check(ans) { gpu_assert((ans), __FILE__, __LINE__); }
inline void gpu_assert(cudaError_t code, const char *file, int line, bool abort=true)
{
   if (code != cudaSuccess)
   {
      fprintf(stderr,"GPU assert: %s %s %d\n", cudaGetErrorString(code), file, line);
      if (abort) exit(code);
   }
}

__device__ inline float fxor(float v, uint32_t mask)
{
    uint32_t* vi = reinterpret_cast<uint32_t*>(&v);
    *vi ^= mask;
    return v;
}

__device__ inline half2 h2xor(half2 v, uint32_t mask)
{
    uint32_t* vi = reinterpret_cast<uint32_t*>(&v);
    *vi ^= mask;
    return v;
}

#define NEG_INF_F16 __ushort_as_half(0xFC00)
#define POS_INF_F16 __ushort_as_half(0x7C00)

</content>

<content full_path="exllamav3/exllamav3_ext/hgemm.cu">
#include "hgemm.cuh"
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include "util.h"
#include "util.cuh"

/*

Row-major matmul using cuBLAS, a @ b -> c
- if c is float16, operation is float16 @ float16 -> float16
- if c is float32, operation is float16 @ float16 -> float23
*/

void hgemm
(
    at::Tensor a,
    at::Tensor b,
    at::Tensor c
)
{
    const at::cuda::OptionalCUDAGuard device_guard(a.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    bool output_fp32 = c.dtype() == at::kFloat;
    if (!output_fp32)
        TORCH_CHECK_DTYPE(c, kHalf);

    TORCH_CHECK_DTYPE(a, kHalf);
    TORCH_CHECK_DTYPE(b, kHalf);
    TORCH_CHECK_DIM(a, 2);
    TORCH_CHECK_DIM(b, 2);
    TORCH_CHECK_DIM(c, 2);
    TORCH_CHECK_SHAPES(a, 0, c, 0, 1);
    TORCH_CHECK_SHAPES(a, 1, b, 0, 1);
    TORCH_CHECK_SHAPES(b, 1, c, 1, 1);

    const half* a_ptr = (const half*) a.data_ptr();
    const half* b_ptr = (const half*) b.data_ptr();

    int size_m = a.size(0);
    int size_k = a.size(1);
    int size_n = b.size(1);

    cublasHandle_t cublas_handle = at::cuda::getCurrentCUDABlasHandle();
    cublasSetStream(cublas_handle, stream);

    if (!output_fp32)
    {
        half alpha_ = __float2half(1.0f);
        half beta_ = __float2half(0.0f);

        half* c_ptr = (half*) c.data_ptr();
        cublasHgemm
        (
            cublas_handle,
            CUBLAS_OP_N,
            CUBLAS_OP_N,
            size_n, size_m, size_k,
            &alpha_, b_ptr, size_n,
                     a_ptr, size_k,
            &beta_,  c_ptr, size_n
        );
    }
    else
    {
        float alpha_ = 1.0f;
        float beta_ = 0.0f;

        float* c_ptr = (float*) c.data_ptr();
        cublasGemmEx
        (
            cublas_handle,
            CUBLAS_OP_N, CUBLAS_OP_N,
            size_n, size_m, size_k,
            &alpha_, b_ptr, CUDA_R_16F, size_n,
                     a_ptr, CUDA_R_16F, size_k,
            &beta_,  c_ptr, CUDA_R_32F, size_n,
            CUBLAS_COMPUTE_32F,
            CUBLAS_GEMM_DEFAULT_TENSOR_OP
        );
    }
}

</content>

<content full_path="exllamav3/exllamav3_ext/hadamard.cpp">
#include "hadamard.h"
#include "util.h"

#define HALF_P 0x3C00
#define HALF_N 0xBC00
#define HALF_PP 0x3C003C00
#define HALF_PN 0xBC003C00
#define HALF_NP 0x3C00BC00
#define HALF_NN 0xBC00BC00

inline int pmod(int a, int b)
{
    int ret = a % b;
    if (ret < 0 && b > 0) ret += b;
    return ret;
}

inline int modular_pow(int base, int exp, int mod)
{
    int result = 1;
    base = pmod(base, mod);
    while (exp > 0)
    {
        if (exp % 2 == 1) result = pmod((result * base), mod);
        exp = exp >> 1;
        base = pmod((base * base), mod);
    }
    return result;
}

inline bool is_quadratic_residue(int a, int p)
{
    return modular_pow(a, (p - 1) / 2, p) == 1;
}

// Paley construction

void had_paley
(
    at::Tensor h
)
{
    TORCH_CHECK_DTYPE(h, kHalf);
    TORCH_CHECK_SHAPES(h, 0, h, 1, 1);
    TORCH_CHECK(h.is_contiguous());
    int n = h.size(0);
    int p = n - 1;
    uint16_t* ptr = (uint16_t*) h.data_ptr();

    for (int j = 0; j < n; ++j)
        *ptr++ = HALF_P;

    for (int i = 0; i < p; ++i)
    {
        *ptr++ = HALF_N;
        for (int j = 0; j < p; ++j)
        {
            if (i == j) *ptr++ = HALF_P;
            else
            {
                int residue = pmod(i - j, p);
                if (is_quadratic_residue(residue, p))
                    *ptr++ = HALF_P;
                else
                    *ptr++ = HALF_N;
            }
        }
    }
}

// Paley construction, type 2

void had_paley2
(
    at::Tensor h
)
{
    TORCH_CHECK_DTYPE(h, kHalf);
    TORCH_CHECK_SHAPES(h, 0, h, 1, 1);
    int n = h.size(0);
    int p = n / 2 - 1;
    uint32_t* ptr0 = (uint32_t*) h.data_ptr();
    uint32_t* ptr1 = ptr0 + n / 2;

    for (int i = 0; i < n / 2; ++i)
    {
        for (int j = 0; j < n / 2; ++j)
        {
            if (i == j)
            {
                *ptr0++ = HALF_PN;
                *ptr1++ = HALF_NN;
            }
            else
            {
                int residue = pmod(i - j, p);
                if (i == 0 || j == 0 || is_quadratic_residue(residue, p))
                {
                    *ptr0++ = HALF_PP;
                    *ptr1++ = HALF_PN;
                }
                else
                {
                    *ptr0++ = HALF_NN;
                    *ptr1++ = HALF_NP;
                }
            }
        }
        ptr0 += n / 2;
        ptr1 += n / 2;
    }
}

</content>

<content full_path="exllamav3/exllamav3_ext/reduction.cuh">
#pragma once

struct ValIdx
{
    float val;
    int idx;
};

__device__ inline ValIdx warp_reduce_argmax(ValIdx v)
{
    for (int offset = 32 >> 1; offset > 0; offset >>= 1)
    {
        float other_val = __shfl_down_sync(0xffffffff, v.val, offset);
        int other_idx = __shfl_down_sync(0xffffffff, v.idx, offset);
        if (other_val > v.val)
        {
            v.val = other_val;
            v.idx = other_idx;
        }
    }
    return v;
}

__device__ inline ValIdx block_reduce_argmax(ValIdx v)
{
    __shared__ ValIdx shared[32];

    int lane_id = threadIdx.x % 32;
    int warp_id = threadIdx.x / 32;

    v = warp_reduce_argmax(v);

    if (lane_id == 0) shared[warp_id] = v;
    __syncthreads();

    if (warp_id == 0)
    {
        v = shared[lane_id];
        v = warp_reduce_argmax(v);
    }
    return v;
}

__device__ inline half warp_reduce_max_h(half v)
{
    for (int offset = 32 >> 1; offset > 0; offset >>= 1)
    {
        half2 other_v = __shfl_down_sync(0xffffffff, __half2half2(v), offset);
        v = __hmax(v, __low2half(other_v));
    }
    return v;
}

__device__ inline half block_reduce_max_h(half v, int num_threads)
{
    __shared__ half shared[32];

    int lane_id = threadIdx.x % 32;
    int warp_id = threadIdx.x / 32;

    v = warp_reduce_max_h(v);

    if (lane_id == 0) shared[warp_id] = v;
    __syncthreads();

    int max_warp_id = num_threads / 32;
    if (warp_id == 0)
    {
        v = lane_id < max_warp_id ? shared[lane_id] : NEG_INF_F16;
        v = warp_reduce_max_h(v);
    }
    return v;
}

__device__ inline float warp_reduce_sum_f(float v)
{
    for (int offset = 32 >> 1; offset > 0; offset >>= 1)
    {
        float other_v = __shfl_down_sync(0xffffffff, v, offset);
        v += other_v;
    }
    return v;
}

__device__ inline float block_reduce_sum_f(float v, int num_threads)
{
    __shared__ float shared[32];

    int lane_id = threadIdx.x % 32;
    int warp_id = threadIdx.x / 32;

    v = warp_reduce_sum_f(v);

    if (lane_id == 0) shared[warp_id] = v;
    __syncthreads();

    int max_warp_id = num_threads / 32;
    if (warp_id == 0)
    {
        v = lane_id < max_warp_id ? shared[lane_id] : 0.0f;
        v = warp_reduce_sum_f(v);
    }
    return v;
}
</content>

<content full_path="exllamav3/exllamav3_ext/norm.cuh">
#pragma once

#include <ATen/Tensor.h>

void rms_norm
(
    at::Tensor x,
    at::Tensor w,
    at::Tensor y,
    float epsilon,
    float constant_bias
);

</content>

<content full_path="exllamav3/exllamav3_ext/ptx.cuh">
#pragma once

// Tensor core fragments

template <typename T, int n>
struct Vec
{
    T elems[n];
    __device__ T& operator[](int i) { return elems[i]; }
};

using FragA = Vec<half2, 4>;
using FragB = Vec<half2, 2>;
using FragC = Vec<float, 4>;
using FragC_h = Vec<half2, 2>;

// m8n8k4 tensor core matmul (emulated on Ampere and later), don't use
//
// https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#matrix-fragments-for-mma-m8n8k4-with-f16-floating-point-type

__device__ inline void ptx_mma_m8n8k4
(
    const Vec<half2, 2>& frag_a,
    const Vec<half2, 2>& frag_b,
    Vec<float, 8>& frag_c
)
{
    const uint32_t* a = reinterpret_cast<const uint32_t*>(&frag_a);
    const uint32_t* b = reinterpret_cast<const uint32_t*>(&frag_b);
    float* c = reinterpret_cast<float*>(&frag_c);
    const float* d = reinterpret_cast<const float*>(&frag_c);

    asm volatile
    (
        "mma.sync.aligned.m8n8k4.row.col.f32.f16.f16.f32 "
        "{%0,%1,%2,%3,%4,%5,%6,%7}, {%8,%9}, {%10,%11}, {%12,%13,%14,%15,%16,%17,%18,%19};\n"

        : "=f"(c[0]), "=f"(c[1]), "=f"(c[2]), "=f"(c[3]),"=f"(c[4]), "=f"(c[5]), "=f"(c[6]), "=f"(c[7])

        :  "r"(a[0]), "r"(a[1]),
           "r"(b[0]), "r"(b[1]),
           "f"(d[0]), "f"(d[1]), "f"(d[2]), "f"(d[3]), "f"(d[4]), "f"(d[5]), "f"(d[6]), "f"(d[7])
    );
}

// m16n8k16 tensor core matmul
//
// https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#matrix-fragments-for-mma-m16n8k16-with-floating-point-type

// FP16 @ FP16 + FP32 -> FP32
__device__ inline void ptx_mma_m16n8k16
(
    const FragA& frag_a,
    const FragB& frag_b,
    FragC& frag_c
)
{
    const uint32_t* a = reinterpret_cast<const uint32_t*>(&frag_a);
    const uint32_t* b = reinterpret_cast<const uint32_t*>(&frag_b);
    float* c = reinterpret_cast<float*>(&frag_c);
    const float* d = reinterpret_cast<const float*>(&frag_c);

    asm volatile
    (
        "mma.sync.aligned.m16n8k16.row.col.f32.f16.f16.f32 "
        "{%0,%1,%2,%3}, {%4,%5,%6,%7}, {%8,%9}, {%10,%11,%12,%13};\n"

        : "=f"(c[0]), "=f"(c[1]), "=f"(c[2]), "=f"(c[3])
        :  "r"(a[0]), "r"(a[1]), "r"(a[2]), "r"(a[3]),
           "r"(b[0]), "r"(b[1]),
           "f"(d[0]), "f"(d[1]), "f"(d[2]), "f"(d[3])
    );
}

// FP16 @ FP16 + FP16 -> FP16
__device__ inline void ptx_mma_m16n8k16
(
    const FragA& frag_a,
    const FragB& frag_b,
    FragC_h& frag_c
)
{
    const uint32_t* a = reinterpret_cast<const uint32_t*>(&frag_a);
    const uint32_t* b = reinterpret_cast<const uint32_t*>(&frag_b);
    uint32_t* c = reinterpret_cast<uint32_t*>(&frag_c);
    const uint32_t* d = reinterpret_cast<const uint32_t*>(&frag_c);

    asm volatile
    (
        "mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 "
        "{%0,%1}, {%2,%3,%4,%5}, {%6,%7}, {%8,%9};\n"

        : "=r"(c[0]), "=r"(c[1])
        :  "r"(a[0]), "r"(a[1]), "r"(a[2]), "r"(a[3]),
           "r"(b[0]), "r"(b[1]),
           "r"(d[0]), "r"(d[1])
    );
}

// Global barrier

__device__ inline void barrier_acquire
(
    int* lock,
    int stage
)
{
    if (threadIdx.x == 0)
    {
        volatile int state = -1;
        do
        {
            asm volatile ("ld.global.acquire.gpu.b32 %0, [%1];\n" : "=r"(state) : "l"(lock));
        }
        while (state != stage);
    }
    __syncthreads();
}

__device__ inline void barrier_release
(
    int* lock,
    int val,
    bool reset
)
{
    __syncthreads();
    if (threadIdx.x == 0)
    {
        if (reset)
        {
            *lock = 0;
            return;
        }
        asm volatile ("fence.acq_rel.gpu;\n");
        asm volatile ("red.relaxed.gpu.global.add.s32 [%0], %1;\n" : : "l"(lock), "r"(val));
    }
}

// Load global to shared memory, predicated. Seems to produce incorrect code when compiling for Blackwell, but
// `if (...) cp_async(...)` compiles to a predicated instruction anyway

__device__ inline void cp_async_pred(void* smem_ptr, const void* glob_ptr, bool pred = true)
{
    const int bytes = 16;
    uint32_t smem = static_cast<uint32_t>(__cvta_generic_to_shared(smem_ptr));
    asm volatile(
        "{\n"
        "   .reg .pred p;\n"
        "   setp.ne.b32 p, %0, 0;\n"
        "   @p cp.async.cg.shared.global [%1], [%2], %3;\n"
        "}\n" :: "r"((int) pred), "r"(smem), "l"(glob_ptr), "n"(bytes)
    );
}

// Load global to shared memory

__device__ inline void cp_async(void* smem_ptr, const void* glob_ptr)
{
    const int bytes = 16;
    uint32_t smem = static_cast<uint32_t>(__cvta_generic_to_shared(smem_ptr));
    asm volatile(
        "{\n"
        "   cp.async.cg.shared.global [%0], [%1], %2;\n"
        "}\n" :: "r"(smem), "l"(glob_ptr), "n"(bytes)
    );
}

// Load global to shared memory with cache hint to evict data from L2 ASAP

__device__ inline void cp_async_stream(void* smem_ptr, const void* glob_ptr)
{
    uint32_t smem = static_cast<uint32_t>(__cvta_generic_to_shared(smem_ptr));
    const int bytes = 16;
    asm volatile
    (
        "{\n"
        "   .reg .b64 p;\n"
        "   createpolicy.fractional.L2::evict_first.b64 p, 1.0;\n"
        "   cp.async.cg.shared.global.L2::cache_hint [%0], [%1], %2, p;\n"
        "}\n" :: "r"(smem), "l"(glob_ptr), "n"(bytes)
    );
}

// Async copy fence, commit all pending async copies

__device__ inline void cp_async_fence()
{
    asm volatile("cp.async.commit_group;\n" ::);
}

// Wait until at most n async groups are still pending.

template <int n>
__device__ inline void cp_async_wait()
{
    asm volatile("cp.async.wait_group %0;\n" :: "n"(n));
}

// Load 16x16 matrix fragment from shared memory, directly in tensor core layout

__device__ inline void ldsm4(FragA& frag_a, const void* smem_ptr)
{
    uint32_t* a = reinterpret_cast<uint32_t*>(&frag_a);
    uint32_t smem = static_cast<uint32_t>(__cvta_generic_to_shared(smem_ptr));
    asm volatile
    (
        "ldmatrix.sync.aligned.m8n8.x4.shared.b16 {%0,%1,%2,%3}, [%4];\n"
        : "=r"(a[0]), "=r"(a[1]), "=r"(a[2]), "=r"(a[3]) : "r"(smem)
    );
}

__device__ inline uint32_t mul_lo_u32(uint32_t x, uint32_t y)
{
    uint32_t w;
    asm volatile
    (
        "mul.lo.u32 %0, %1, %2;"
        : "=r"(w)
        :  "r"(x), "r"(y)
    );
    return w;
}

__device__ inline uint32_t mul_hi_u32(uint32_t x, uint32_t y)
{
    uint32_t w;
    asm volatile
    (
        "mul.hi.u32 %0, %1, %2;"
        : "=r"(w)
        :  "r"(x), "r"(y)
    );
    return w;
}

static __forceinline__ __device__ uint32_t bfe64(uint32_t lo, uint32_t hi, int offset, int length)
{
    uint64_t value = (static_cast<uint64_t>(hi) << 32) | static_cast<uint64_t>(lo);
    uint64_t result64;
    asm volatile ("bfe.u64 %0, %1, %2, %3;"
                  : "=l"(result64)
                  : "l"(value), "r"(offset), "r"(length));

    return static_cast<uint32_t>(result64);
}
</content>

<content full_path="exllamav3/exllamav3_ext/softcap.cuh">
#pragma once

#include <ATen/Tensor.h>

void softcap
(
    at::Tensor x,
    at::Tensor y,
    float softcap_factor
);

</content>

<content full_path="exllamav3/exllamav3_ext/histogram.cuh">
#pragma once

#include <ATen/Tensor.h>

void histogram
(
    at::Tensor& input,
    at::Tensor& output,
    float min_value,
    float max_value,
    bool exclusive
);
</content>

<content full_path="exllamav3/exllamav3_ext/hadamard.h">
#pragma once

#include <ATen/Tensor.h>

void had_paley
(
    at::Tensor h
);

void had_paley2
(
    at::Tensor h
);
</content>

<content full_path="exllamav3/exllamav3_ext/util.h">
#pragma once

#include <chrono>

#define CEIL_DIVIDE(x, size) (((x) + (size) - 1) / (size))
#define MIN(x, y) ((x) < (y) ? (x) : (y))
#define MAX(x, y) ((x) > (y) ? (x) : (y))

// Some decluttering macros
//
// TORCH_CHECK_DTYPE(x, T):                     assert x is dtype T
// TORCH_CHECK_DTYPE_OPT(x, T):                 assert x is dtype T, unless x is None
// TORCH_CHECK_FLOAT_HALF(x):                   assert x is either kFloat or kHalf
// TORCH_CHECK_SHAPES(x, i, y, j, scale):       assert x.size(i) == y.size(j) * scale
// TORCH_CHECK_SHAPES_OPT(x, i, y, j, scale):   assert x.size(i) == y.size(j) * scale, unless x is None
// TORCH_CHECK_SHAPES_FULL(x, y):               assert x and y are same shape
// TORCH_CHECK_NUMEL(x, y):                     assert x and y have same number of elements
// TORCH_CHECK_DIV(x, i, divisor):              assert x.size(i) is divisible by divisor
// TORCH_CHECK_DIM(x, D):                       assert x has D dimensions
// TORCH_CHECK_DIM_OPT(x, D):                   assert x has D dimensions, unless x is None
// TORCH_CHECK_SIZE(x, i, s):                   assert x.size(i) == s
// OPTPTR(x):                                   x.data_ptr() or nullptr if x is None

#define TORCH_CHECK_DTYPE(__x, __dtype) TORCH_CHECK((__x).dtype() == at::__dtype, #__x " is incorrect datatype, must be " #__dtype)
#define TORCH_CHECK_DTYPE_OPT(__x, __dtype) TORCH_CHECK((!__x.has_value()) || (__x).value().dtype() == at::__dtype, #__x " is incorrect datatype, must be " #__dtype)
#define TORCH_CHECK_FLOAT_HALF(__x) TORCH_CHECK((__x).dtype() == at::kHalf || (__x).dtype() == at::kFloat,  #__x " is incorrect datatype, must be kHalf or kFloat")
#define TORCH_CHECK_SHAPES(__x, __dim_x, __y, __dim_y, __scale_y) TORCH_CHECK((__x).size(__dim_x) == (__y).size(__dim_y) * __scale_y, #__x " and " #__y " have incompatible shapes")
#define TORCH_CHECK_SHAPES_OPT(__x, __dim_x, __y, __dim_y, __scale_y) TORCH_CHECK((!(__x).has_value()) || (__x).value().size(__dim_x) == (__y).size(__dim_y) * __scale_y, #__x " and " #__y " have incompatible shapes")
#define TORCH_CHECK_SHAPES_FULL(__x, __y) TORCH_CHECK((__x).sizes() == (__y).sizes(), #__x " and " #__y " have incompatible shapes")
#define TORCH_CHECK_NUMEL(__x, __y) TORCH_CHECK((__x).numel() == (__y).numel(), #__x " and " #__y " have incompatible shapes")
#define TORCH_CHECK_DIV(__x, __dim_x, __div) TORCH_CHECK((__x).size(__dim_x) % __div == 0, #__x " dimension " #__dim_x " must be divisible by " #__div)
#define TORCH_CHECK_DIM(__x, __dims) TORCH_CHECK((__x).dim() == __dims, #__x " must have " #__dims " dimensions")
#define TORCH_CHECK_DIM_OPT(__x, __dims) TORCH_CHECK((!__x.has_value()) || (__x).value().dim() == __dims, #__x " must have " #__dims " dimensions")
#define TORCH_CHECK_SIZE(__x, __dim_x, __s) TORCH_CHECK((__x).size(__dim_x) == (__s), #__x " dimension " #__dim_x " is incorrect size")
#define OPTPTR(__x) (__x.has_value() ? __x.value().data_ptr() : nullptr)

// Debug stuff

#define DBGS(__x) printf("%s\n", __x)
#define DBGI(__x) \
    printf("%s: %i\n", #__x, __x)
#define DBGI2(__x, __y) \
    printf("%s, %s: %i, %i\n", #__x, #__y, __x, __y)
#define DBGI3(__x, __y, __z) \
    printf("%s, %s, %s: %i, %i, %i\n", #__x, #__y, #__z, __x, __y, __z)
#define DBGI4(__x, __y, __z, __w) \
    printf("%s, %s, %s, %s: %i, %i, %i, %i\n", #__x, #__y, #__z, #__w, __x, __y, __z, __w)
#define DBGI5(__x, __y, __z, __w, __v) \
    printf("%s, %s, %s, %s, %s: %i, %i, %i, %i, %i\n", #__x, #__y, #__z, #__w, #__v, __x, __y, __z, __w, __v)
#define DBGI6(__x, __y, __z, __w, __v, __u) \
    printf("%s, %s, %s, %s, %s, %s: %i, %i, %i, %i, %i, %i\n", #__x, #__y, #__z, #__w, #__v, #__u, __x, __y, __z, __w, __v, __u)
#define DBGI7(__x, __y, __z, __w, __v, __u, __t) \
    printf("%s, %s, %s, %s, %s, %s, %s: %i, %i, %i, %i, %i, %i, %i\n", #__x, #__y, #__z, #__w, #__v, #__u, #__t, __x, __y, __z, __w, __v, __u, __t)
#define DBGI8(__x, __y, __z, __w, __v, __u, __t, __s) \
    printf("%s, %s, %s, %s, %s, %s, %s, %s: %i, %i, %i, %i, %i, %i, %i, %i\n", #__x, #__y, #__z, #__w, #__v, #__u, #__t, #__s, __x, __y, __z, __w, __v, __u, __t, __s)
#define DBGI9(__x, __y, __z, __w, __v, __u, __t, __s, __r) \
    printf("%s, %s, %s, %s, %s, %s, %s, %s, %s: %i, %i, %i, %i, %i, %i, %i, %i, %i\n", #__x, #__y, #__z, #__w, #__v, #__u, #__t, #__s, #__r, __x, __y, __z, __w, __v, __u, __t, __s, __r)
#define DBGI10(__x, __y, __z, __w, __v, __u, __t, __s, __r, __q) \
    printf("%s, %s, %s, %s, %s, %s, %s, %s, %s, %s: %i, %i, %i, %i, %i, %i, %i, %i, %i, %i\n", #__x, #__y, #__z, #__w, #__v, #__u, #__t, #__s, #__r, #__q, __x, __y, __z, __w, __v, __u, __t, __s, __r, __q)
#define DBGX(__x) printf("%s: %x\n", #__x, __x)
#define DBGX2(__x, __y) printf("%s, %s: %x, %x\n", #__x, #__y, __x, __y)
#define DBGX3(__x, __y, __z) printf("%s, %s, %s: %x, %x, %x\n", #__x, #__y, #__z, __x, __y, __z)
#define DBGIX(__x, __y) printf("%s, %s: %i, %x\n", #__x, #__y, __x, __y)
#define DBGIX2(__x, __y, __z) printf("%s, %s, %s: %i, %x, %x\n", #__x, #__y, #__z, __x, __y, __z)
#define DBGIF(__x, __y) printf("%s, %s: %i, %f\n", #__x, #__y, __x, __y)
#define DBGF(__x) printf("%s: %f\n", #__x, __x)
#define DBGF2(__x, __y) printf("%s, %s: %f, %f\n", #__x, #__y, __x, __y)
#define DBGF3(__x, __y, __z) printf("%s, %s, %s: %f, %f, %f\n", #__x, #__y, #__z, __x, __y, __z)
#define DBGF4(__x, __y, __z, __w) printf("%s, %s, %s, %s: %f, %f, %f, %f\n", #__x, #__y, #__z, #__w, __x, __y, __z, __w)
#define DBGH(__x) printf("%s: %f\n", #__x, __half2float(__x))
#define DBGH2(__x, __y) printf("%s, %s: %f, %f\n", #__x, #__y, __half2float(__x), __half2float(__y))
#define DBGH3(__x, __y, __z) printf("%s, %s, %s: %f, %f, %f\n", #__x, #__y, #__z, __half2float(__x), __half2float(__y), __half2float(__z))
#define DBGIH(__x, __y) printf("%s, %s: %i, %f\n", #__x, #__y, __x, __half2float(__y))
#define DBGIH2(__x, __y, __z) printf("%s, %s, %s: %i, %f, %f\n", #__x, #__y, #__z, __x, __half2float(__y), __half2float(__z))
#define DBGI2H2(__x, __y, __z, __w) printf("%s, %s, %s, %s: %i, %i, %f, %f\n", #__x, #__y, #__z, #__w, __x, __y, __half2float(__z), __half2float(__w))
#define DBGIH3(__x, __y, __z, __w) printf("%s, %s, %s, %s: %i, %f, %f, %f\n", #__x, #__y, #__z, #__w, __x, __half2float(__y), __half2float(__z), __half2float(__w))
#define DBGIH4(__x, __y, __z, __w, __v) printf("%s, %s, %s, %s, %s: %i, %f, %f, %f, %f\n", #__x, #__y, #__z, #__w, #__v, __x, __half2float(__y), __half2float(__z), __half2float(__w), __half2float(__v))

#define TIME_START \
    auto start = std::chrono::high_resolution_clock::now()

#define TIME_STOP \
    do { \
        auto stop = std::chrono::high_resolution_clock::now(); \
        auto duration_us = std::chrono::duration_cast<std::chrono::microseconds>(stop - start); \
        DBGI(duration_us); \
    } while (false)

/*
Compile-time for loop. Supports template instancing. Example usage:

int kernel_arg = select_kernel_somehow();

// Not nice
if (kernel_arg == 2)
    launch_kernel_instance<2><<< ... >>>( ... )
if (kernel_arg == 3)
    launch_kernel_instance<3><<< ... >>>( ... )
if (kernel_arg == 4)
    launch_kernel_instance<4><<< ... >>>( ... )
if (kernel_arg == 6)
    launch_kernel_instance<6><<< ... >>>( ... )
if (kernel_arg == 8)
    launch_kernel_instance<8><<< ... >>>( ... )

// Nice?
static_for_pack<2, 3, 4, 6, 8>([&](auto ic)
{
    constexpr int i = decltype(ic)::value;
    if (kernel_arg == i)
        launch_kernel_instance<i><<< ... >>>( ... )
});

// Ultimately much cleaner
#define __(i, j) quant_cache_paged_kernel<i, j>
constexpr auto quant_cache_paged_kernel_instances = std::array
{
    std::array{ __(2, 2), __(2, 3), __(2, 4), __(2, 5), __(2, 6), __(2, 7), __(2, 8) },
    std::array{ __(3, 2), __(3, 3), __(3, 4), __(3, 5), __(3, 6), __(3, 7), __(3, 8) },
    std::array{ __(4, 2), __(4, 3), __(4, 4), __(4, 5), __(4, 6), __(4, 7), __(4, 8) },
    std::array{ __(5, 2), __(5, 3), __(5, 4), __(5, 5), __(5, 6), __(5, 7), __(5, 8) },
    std::array{ __(6, 2), __(6, 3), __(6, 4), __(6, 5), __(6, 6), __(6, 7), __(6, 8) },
    std::array{ __(7, 2), __(7, 3), __(7, 4), __(7, 5), __(7, 6), __(7, 7), __(7, 8) },
    std::array{ __(8, 2), __(8, 3), __(8, 4), __(8, 5), __(8, 6), __(8, 7), __(8, 8) }
};
#undef __
*/

// This breaks with nesting on VC++ older than 17.13 (late 2024 preview)
template <int... Values, class F>
constexpr void static_for_pack(F&& f)
{
    (f(std::integral_constant<int, Values>{}), ...);
}

</content>

<content full_path="exllamav3/exllamav3_ext/stloader.h">
#pragma once

#include <ATen/Tensor.h>
#include <vector>

#define STLOADER_BLOCK_SIZE (512*1024)
#define STLOADER_THREADS 8

void stloader_read
(
    std::vector<uintptr_t> handles,
    size_t offset,
    size_t size,
    at::Tensor target
);

std::vector<uintptr_t> stloader_open_file(const char* filename);
void stloader_close_file(std::vector<uintptr_t> handles);

struct TensorLoadJob {
    std::vector<uintptr_t> handles;
    size_t file_offset;
    size_t bytesize;
    uintptr_t destination;
    bool bf16_to_fp16;
    bool fp32_to_fp16;
    bool cuda;
    int device_id;
};

void stloader_deferred_cpu(std::vector<TensorLoadJob> const& jobs);
void stloader_deferred_cuda(std::vector<TensorLoadJob> const& jobs, size_t max_chunk_size);

</content>

<content full_path="exllamav3/exllamav3_ext/stloader_cu.cu">
#include "stloader_cu.cuh"
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include "util.h"
#include "util.cuh"

void inplace_bf16_to_fp16_cpu
(
    void* buffer,
    size_t numel
)
{
    const __nv_bfloat16* rd = (const __nv_bfloat16*) buffer;
    __half* wr = (__half*) buffer;

    for (size_t i = 0; i < numel; ++i)
    {
        float f32 = __bfloat162float(rd[i]);
        wr[i] = __float2half_rn(f32);
    }
}

#define NUM_THREADS 1024

__global__ __launch_bounds__(NUM_THREADS)
void inplace_bf16_to_fp16_kernel
(
    void* __restrict__ buffer,
    size_t numel2
)
{
    size_t i = blockIdx.x * NUM_THREADS + threadIdx.x;
    if (i >= numel2) return;

     const __nv_bfloat162* rd2 = (const __nv_bfloat162*) buffer;
    __half2* wr2 = (__half2*) buffer;

    __nv_bfloat162 b2 = rd2[i];
    float2 f2 = __bfloat1622float2(b2);
    __half2 h2 = __floats2half2_rn(f2.x, f2.y);
    wr2[i] = h2;
}

void inplace_bf16_to_fp16_cuda
(
    void* buffer,
    size_t numel
)
{
    size_t blocks = CEIL_DIVIDE(numel / 2, NUM_THREADS);
    inplace_bf16_to_fp16_kernel<<<blocks, NUM_THREADS>>>(buffer, numel / 2);
}
</content>

<content full_path="exllamav3/exllamav3_ext/activation.cuh">
#pragma once

#include <ATen/Tensor.h>

void silu_mul
(
    const at::Tensor& x,
    const at::Tensor& y,
    at::Tensor& z
);

void gelu_mul
(
    const at::Tensor& x,
    const at::Tensor& y,
    at::Tensor& z
);
</content>

<content full_path="exllamav3/exllamav3_ext/activation.cu">
#include "activation.cuh"
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include "util.h"
#include "util.cuh"
#include "compat.cuh"

#define NUM_THREADS 256
#define ACT_SILU 0
#define ACT_GELU 1

__device__ __forceinline__ half _silu(half x)
{
    half one = __float2half(1.0f);
    half neg_x = __hneg(x);
    half e = hexp(neg_x);
    half sum = __hadd(one, e);
    half r = hrcp(sum);
    half result = __hmul(x, r);
    return result;
}

__device__ __forceinline__ half2 _silu(half2 x)
{
    half2 one = __float2half2_rn(1.0f);
    half2 neg_x = __hneg2(x);
    half2 e = h2exp(neg_x);
    half2 sum = __hadd2(one, e);
    half2 r = h2rcp(sum);
    half2 result = __hmul2(x, r);
    return result;
}

__device__ __forceinline__ half _gelu(half x)
{
    float xf = __half2float(x);
    const float c = 0.797884560803f;  // sqrt(2/Pi)
    float tanh_arg = c * (xf + 0.044715f * xf * xf * xf);
    xf = 0.5f * xf * (1.0 + tanh_opt(tanh_arg));
    return __float2half_rn(xf);
}

__device__ __forceinline__ half2 _gelu(half2 x)
{
    return __halves2half2(_gelu(__low2half(x)), _gelu(__high2half(x)));
}

template <int activation_type>
__global__ __launch_bounds__(NUM_THREADS)
void act_mul_kernel
(
    const half* __restrict__ x,
    const half* __restrict__ y,
    half* __restrict__ z,
    int numel
)
{
    int idx = (blockIdx.x * NUM_THREADS + threadIdx.x);
    if (idx >= numel / 2) return;

    half2 x2 = ((const half2*) x)[idx];
    half2 y2 = ((const half2*) y)[idx];

    if constexpr (activation_type == ACT_SILU)
        x2 = _silu(x2);
    else if constexpr (activation_type == ACT_GELU)
        x2 = _gelu(x2);

    ((half2*) z)[idx] = __hmul2(x2, y2);
}

// silu(x) * y -> z, in-place if z == x or z == y

void silu_mul
(
    const at::Tensor& x,
    const at::Tensor& y,
    at::Tensor& z
)
{
    const at::cuda::OptionalCUDAGuard device_guard(x.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    int numel = x.numel();
    int blocks = CEIL_DIVIDE(numel, 2 * NUM_THREADS);
    act_mul_kernel<ACT_SILU><<<blocks, NUM_THREADS, 0, stream>>>
    (
        (const half*) x.data_ptr(),
        (const half*) y.data_ptr(),
        (half*) z.data_ptr(),
        numel
    );
}

// silu(x) * y -> z, in-place if z == x or z == y

void gelu_mul
(
    const at::Tensor& x,
    const at::Tensor& y,
    at::Tensor& z
)
{
    const at::cuda::OptionalCUDAGuard device_guard(x.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    int numel = x.numel();
    int blocks = CEIL_DIVIDE(numel, 2 * NUM_THREADS);
    act_mul_kernel<ACT_GELU><<<blocks, NUM_THREADS, 0, stream>>>
    (
        (const half*) x.data_ptr(),
        (const half*) y.data_ptr(),
        (half*) z.data_ptr(),
        numel
    );
}
</content>

<content full_path="exllamav3/exllamav3_ext/bindings.cpp">
#include <torch/extension.h>
#include <pybind11/pybind11.h>
#include <pybind11/stl.h>

#include <cuda_fp16.h>

#include "stloader.h"
#include "hadamard.h"

#include "norm.cuh"
#include "hgemm.cuh"
#include "rope.cuh"
#include "activation.cuh"
#include "softcap.cuh"

#include "quant/quantize.cuh"
#include "quant/pack.cuh"
#include "quant/reconstruct.cuh"
#include "quant/hadamard.cuh"
#include "quant/exl3_gemm.cuh"
#include "quant/exl3_kernel_map.cuh"
#include "quant/util.cuh"

#include "generator/strings.h"
#include "generator/sampling_basic.cuh"
#include "generator/gumbel.cuh"
#include "generator/rep_pen.cuh"
#include "generator/cache.cuh"

#include "cache/q_cache.cuh"

#include "histogram.cuh"

#include "libtorch/blocksparse_mlp.h"

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m)
{
    m.def("stloader_read", &stloader_read, "stloader_read");
    m.def("stloader_open_file", &stloader_open_file, "stloader_open_file");
    m.def("stloader_close_file", &stloader_close_file, "stloader_close_file");
    py::class_<TensorLoadJob>(m, "TensorLoadJob")
        .def(py::init<std::vector<uintptr_t>, size_t, size_t, uintptr_t, bool, bool, bool, int>());
    m.def("stloader_deferred_cpu", &stloader_deferred_cpu, py::arg("jobs"));
    m.def("stloader_deferred_cuda", &stloader_deferred_cuda, py::arg("jobs"), py::arg("max_chunk_size"));

    m.def("rms_norm", &rms_norm, "rms_norm");
    m.def("softcap", &softcap, "softcap");

    m.def("had_paley", &had_paley, "had_paley");
    m.def("had_paley2", &had_paley2, "had_paley2");

    m.def("quantize_tiles", &quantize_tiles, "quantize_tiles");
    m.def("test_distribution", &test_distribution, "test_distribution");
    m.def("decode", &decode, "decode");
    m.def("pack_trellis", &pack_trellis, "pack_trellis");
    m.def("unpack_trellis", &unpack_trellis, "unpack_trellis");
    m.def("pack_signs", &pack_signs, "pack_signs");
    m.def("reconstruct", &reconstruct, "reconstruct");
    m.def("had_r_128", &had_r_128, "had_r_128");
    m.def("exl3_gemm", &exl3_gemm, "exl3_gemm");
    m.def("exl3_gemm_num_kernel_shapes", &exl3_gemm_num_kernel_shapes, "exl3_gemm_num_kernel_shapes");
    m.def("exl3_gemm_shape_compat", &exl3_gemm_shape_compat, "exl3_gemm_shape_compat");
    m.def("exl3_mgemm", &exl3_mgemm, "exl3_mgemm");
    m.def("hgemm", &hgemm, "hgemm");
    m.def("rope", &rope, "rope");
    m.def("silu_mul", &silu_mul, "silu_mul");
    m.def("gelu_mul", &gelu_mul, "gelu_mul");

    m.def("argmax_sample", &argmax_sample, "argmax_sample");
    m.def("gumbel_sample", &gumbel_sample, "gumbel_sample");
    m.def("gumbel_noise_f16", &gumbel_noise_f16, "gumbel_noise_f16");
    m.def("gumbel_noise_f32", &gumbel_noise_f32, "gumbel_noise_f32");
    m.def("gumbel_noise_log", &gumbel_noise_log, "gumbel_noise_log");
    m.def("apply_rep_pens", &apply_rep_pens, "apply_rep_pens");
    m.def("apply_pres_freq_pens", &apply_pres_freq_pens, "apply_pres_freq_pens");

    m.def("cache_rotate", &cache_rotate, "cache_rotate");

    m.def("partial_strings_match", &partial_strings_match, "partial_strings_match");
    m.def("count_match_tensor", &count_match_tensor, "count_match_tensor");

    m.def("quant_cache_cont", &quant_cache_cont, "quant_cache_cont");
    m.def("dequant_cache_cont", &dequant_cache_cont, "dequant_cache_cont");
    m.def("quant_cache_paged", &quant_cache_paged, "quant_cache_paged");
    m.def("dequant_cache_paged", &dequant_cache_paged, "dequant_cache_paged");

    m.def("count_inf_nan", &count_inf_nan, "count_inf_nan");
    m.def("histogram", &histogram, "histogram");

    m.def("blocksparse_mlp_routing", &blocksparse_mlp_routing, "blocksparse_mlp_routing");
}
</content>

<content full_path="exllamav3/exllamav3_ext/softcap.cu">
#include "softcap.cuh"
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include "util.h"
#include "util.cuh"

#define NUM_THREADS 1024

__launch_bounds__(NUM_THREADS)
__global__ void softcap_kernel
(
    const float* __restrict__ x,
    float* __restrict__ y,
    const uint64_t numel,
    const float scale
)
{
    uint64_t idx = (uint64_t)blockIdx.x * NUM_THREADS + (uint64_t)threadIdx.x;
    if (idx >= numel) return;

    float v = x[idx];
    v /= scale;
    v = tanhf(v);
    v *= scale;
    y[idx] = v;
}

__launch_bounds__(NUM_THREADS)
__global__ void softcap_h_kernel
(
    const half* __restrict__ x,
    half* __restrict__ y,
    const uint64_t numel,
    const float scale
)
{
    uint64_t idx = ((uint64_t)blockIdx.x * NUM_THREADS + (uint64_t)threadIdx.x) * 2;
    if (idx >= numel) return;

    half2 v01 = *((half2*)(x + idx));
    float v0 = __low2float(v01);
    float v1 = __high2float(v01);
    v0 /= scale;
    v1 /= scale;
    v0 = tanhf(v0);
    v1 = tanhf(v1);
    v0 *= scale;
    v1 *= scale;
    v01 = __floats2half2_rn(v0, v1);
    *((half2*)(y + idx)) = v01;
}

/*
Apply softcapping: y <-scale * tanh(x/scale)
Works inplace if x == y
*/

void softcap
(
    at::Tensor x,
    at::Tensor y,
    float scale
)
{
    const at::cuda::OptionalCUDAGuard device_guard(x.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    uint64_t numel = x.numel();

    if (x.dtype() == at::kFloat)
    {
        softcap_kernel<<<CEIL_DIVIDE(numel, NUM_THREADS), NUM_THREADS, 0, stream>>>
        (
            (const float*) x.data_ptr(),
            (float*) y.data_ptr(),
            numel,
            scale
        );
    }
    else if (x.dtype() == at::kHalf)
    {
        softcap_h_kernel<<<CEIL_DIVIDE(numel / 2, NUM_THREADS), NUM_THREADS, 0, stream>>>
        (
            (const half*) x.data_ptr(),
            (half*) y.data_ptr(),
            numel,
            scale
        );
    }
    else
    {
        TORCH_CHECK(false, "softcap wrong dtype");
    }
}

</content>

<content full_path="exllamav3/exllamav3_ext/stloader_cu.cuh">
#pragma once

void inplace_bf16_to_fp16_cpu
(
    void* buffer,
    size_t numel
);

void inplace_bf16_to_fp16_cuda
(
    void* buffer,
    size_t numel
 );

</content>

<content full_path="exllamav3/exllamav3_ext/hgemm.cuh">
#pragma once

#include <ATen/Tensor.h>

void hgemm
(
    at::Tensor a,
    at::Tensor b,
    at::Tensor c
);
</content>

<content full_path="exllamav3/exllamav3_ext/rope.cu">
#include "rope.cuh"

#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include "util.h"
#include "util.cuh"
#include "reduction.cuh"

#define ROPESTYLE_NONE 0
#define ROPESTYLE_GPTJ 1
#define ROPESTYLE_NEOX 2
#define MAX_NUM_THREADS 1024

template <int rope_mode>
__global__
void rope_kernel
(
    const half* __restrict__ q,
    half* __restrict__ out_q,
    const half* __restrict__ k,
    half* __restrict__ out_k,
    const float* __restrict__ inv_freq,
    int bsz,
    int seq_len,
    int num_heads_q,
    int num_heads_k,
    int head_dim,
    int partial_head_dim,
    int position,
    const uint32_t* __restrict__ positions,
    const uint32_t* __restrict__ position_ids,
    float attn_factor,
    const half* __restrict__ q_norm,
    const half* __restrict__ k_norm,
    float norm_eps,
    float norm_constant_bias
)
{
    // Get position
    int batch = blockIdx.y;
    int token_pos = blockIdx.x;
    int pos = token_pos + position;
    if (positions)
        pos = token_pos + positions[batch];
    else if (position_ids)
        pos = position_ids[batch * seq_len + token_pos];

    // Load inv_freq, compute sin/cos
    int t = threadIdx.x;
    int t_head = threadIdx.y;
    float fr = inv_freq[t];
    float pf = __int2float_rn(pos);
    float sin = __sinf(fr * pf) * attn_factor;
    float cos = __cosf(fr * pf) * attn_factor;

    // Shared buffer
    __shared__ half norm_head[MAX_NUM_THREADS * 2];
    __shared__ float sums[MAX_NUM_THREADS / 32];

    // Prep
    half2 norm_constant_bias_h2 = __float2half2_rn(norm_constant_bias);
    int head_dim_pad = (head_dim + 63) / 64 * 64;

    // Loop over heads
    for (int head_idx = threadIdx.y; head_idx < num_heads_q + num_heads_k; head_idx += blockDim.y)
    {
        const half* g_head_in_ptr;
        half* g_head_out_ptr;
        const half* norm_weight;
        if (head_idx < num_heads_q)
        {
            g_head_in_ptr = q + ((batch * seq_len + token_pos) * num_heads_q + head_idx) * head_dim;
            g_head_out_ptr = out_q + ((batch * seq_len + token_pos) * num_heads_q + head_idx) * head_dim;
            norm_weight = q_norm;
        }
        else if (head_idx < num_heads_q + num_heads_k)
        {
            g_head_in_ptr = k + ((batch * seq_len + token_pos) * num_heads_k + head_idx - num_heads_q) * head_dim;
            g_head_out_ptr = out_k + ((batch * seq_len + token_pos) * num_heads_k + head_idx - num_heads_q) * head_dim;
            norm_weight = k_norm;
        }

        // Temp storage
        half* sh_head = norm_head + t_head * head_dim_pad;
        auto load_head = [&] ()
        {
            if (t < head_dim / 2)
                ((half2*) sh_head)[t] = ((half2*)g_head_in_ptr)[t];
            else
                ((half2*) sh_head)[t] = {};
            __syncthreads();
        };
        auto store_head = [&] ()
        {
            if (t < head_dim / 2)
                ((half2*) g_head_out_ptr)[t] = ((half2*) sh_head)[t];
            __syncthreads();
        };

        // Apply embeddings
        auto apply_rope = [&] ()
        {
            if (t < partial_head_dim / 2)
            {
                if constexpr (rope_mode == ROPESTYLE_NEOX)
                {
                    float v1 = __half2float(sh_head[t]);
                    float v2 = __half2float(sh_head[t + partial_head_dim / 2]);
                    float r1 = v1 * cos - v2 * sin;
                    float r2 = v2 * cos + v1 * sin;
                    sh_head[t] = __float2half_rn(r1);
                    sh_head[t + partial_head_dim / 2] = __float2half_rn(r2);
                }
                else if constexpr (rope_mode == ROPESTYLE_GPTJ)
                {
                    half2 *tptr = (half2*)(sh_head + t * 2);
                    half2 v = *tptr;
                    float v1 = __low2float(v);
                    float v2 = __high2float(v);
                    float r1 = v1 * cos - v2 * sin;
                    float r2 = v2 * cos + v1 * sin;
                    v = __floats2half2_rn(r1, r2);
                    *tptr = v;
                }
            }
            __syncthreads();
        };

        // RMS Norm
        auto apply_norm = [&] ()
        {
            half2 *tptr = (half2*)(sh_head + t * 2);
            half2 *wptr = (half2*)(norm_weight + t * 2);
            // int lane_id = threadIdx.x % 32;
            int warp_id = threadIdx.x / 32;
            int warps = blockDim.x / 32;

            // Sum of squares
            half2 v = *tptr;
            float v1 = __low2float(v);
            float v2 = __high2float(v);
            float sum = v1 * v1 + v2 * v2;
            sums[warps * t_head + warp_id] = warp_reduce_sum_f(sum);
            __syncthreads();

            sum = sums[warps * t_head];
            for (int i = 1; i < warps; ++i) sum += sums[warps * t_head + i];

            // Normalize and downcast
            float rmf = rsqrtf(sum / (float) head_dim + norm_eps);
            v1 *= rmf;
            v2 *= rmf;
            v = __floats2half2_rn(v1, v2);

            // Apply weight and store
            half2 w = __hadd2(*wptr, norm_constant_bias_h2);
            v = __hmul2(w, v);
            *tptr = v;
            __syncthreads();
        };

        // Do the things
        load_head();
        if (q_norm) apply_norm();
        apply_rope();
        store_head();
    }
}

/*

Apply position embeddings, works in-place

- q: tensor of shape (bsz, seq_len, num_heads_q, head_dim), float16
- k: tensor of shape (bsz, seq_len, num_heads_k, head_dim), float16, optional
- out_q: output for queries, may be == q
- out_k: output for keys, may be == k
- inv_freq: tensor of shape (head_dim / 2), float32
- position: int, constant position offset (position ID of first token across batch)
- positions: tensor of shape (bsz), (position ID of first token per seq), int, optional
- position_ids: tensor of shape (bsz, seq_len), int, optional
- rope_mode: ROPESTYLE_NEOX
- attn_factor: scale for sin/cos factors
- q_norm: optional RMS norm weight, must be supplied with k_norm
- k_norm: optional RMS norm weight, must be supplied with q_norm
- norm_eps
- norm_constant_bias

Either positions or position_ids overrides position
*/

void rope
(
    const at::Tensor& q,
    at::Tensor& out_q,
    const c10::optional<at::Tensor>& k,
    c10::optional<at::Tensor>& out_k,
    const at::Tensor& inv_freq,
    uint32_t position,
    const c10::optional<at::Tensor>& positions,
    const c10::optional<at::Tensor>& position_ids,
    int rope_mode,
    float attn_factor,
    const c10::optional<at::Tensor>& q_norm,
    const c10::optional<at::Tensor>& k_norm,
    float norm_eps,
    float norm_constant_bias
)
{
    const at::cuda::OptionalCUDAGuard device_guard(q.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    int bsz = q.size(0);
    int seq_len = q.size(1);
    int num_heads_q = q.size(2);
    int num_heads_k = 0;
    int head_dim = q.size(3);
    int partial_head_dim = inv_freq.size(0) * 2;

    const half* q_ptr = (half*) q.data_ptr();
    half* out_q_ptr = (half*) out_q.data_ptr();
    const half* k_ptr = (const half*) OPTPTR(k);
    half* out_k_ptr = (half*) OPTPTR(out_k);
    TORCH_CHECK_DTYPE(q, kHalf);
    TORCH_CHECK_DTYPE_OPT(k, kHalf);
    TORCH_CHECK_DIM(q, 4);
    TORCH_CHECK_DIM_OPT(k, 4);

    if (k_ptr)
    {
        num_heads_k = k.value().size(2);
        TORCH_CHECK(k.value().size(0) == bsz, "k is incorrect shape");
        TORCH_CHECK(k.value().size(1) == seq_len, "k is incorrect shape");
        TORCH_CHECK(k.value().size(3) == head_dim, "k is incorrect shape");
    }

    const float* inv_freq_ptr = (const float*) inv_freq.data_ptr();
    TORCH_CHECK_DTYPE(inv_freq, kFloat);
    TORCH_CHECK_DIM(inv_freq, 1);

    uint32_t* positions_ptr = (uint32_t*) OPTPTR(positions);
    uint32_t* position_ids_ptr = (uint32_t*) OPTPTR(position_ids);
    TORCH_CHECK_DTYPE_OPT(positions, kInt);
    TORCH_CHECK_DTYPE_OPT(position_ids, kInt);
    TORCH_CHECK((positions_ptr != nullptr) + (position_ids_ptr != nullptr) <= 1, "invalid arguments")

    if (positions_ptr)
    {
        TORCH_CHECK_DIM(positions.value(), 1)
        TORCH_CHECK(positions.value().size(0) == bsz, "positions is incorrect shape");
    }

    if (position_ids_ptr)
    {
        TORCH_CHECK_DIM(position_ids.value(), 2)
        TORCH_CHECK(position_ids.value().size(0) == bsz, "position_ids is incorrect shape");
        TORCH_CHECK(position_ids.value().size(1) == seq_len, "position_ids is incorrect shape");
    }

    half* q_norm_ptr = (half*) OPTPTR(q_norm);
    half* k_norm_ptr = (half*) OPTPTR(k_norm);
    TORCH_CHECK_DTYPE_OPT(q_norm, kHalf);
    TORCH_CHECK_DTYPE_OPT(k_norm, kHalf);
    if (q_norm_ptr)
    {
        TORCH_CHECK_DIM(q_norm.value(), 1);
        TORCH_CHECK(q_norm.value().size(0) == head_dim, "q_norm is incorrect size");
    }

    dim3 blocks(seq_len, bsz);
    int warps = CEIL_DIVIDE(head_dim / 2, 32);
    int thr = warps * 32;
    int parallel_heads = MIN((MAX_NUM_THREADS / thr), num_heads_q + num_heads_k);
    dim3 threads(thr, parallel_heads);

    #define ARGS q_ptr, out_q_ptr, k_ptr, out_k_ptr, inv_freq_ptr, bsz, seq_len, num_heads_q, num_heads_k, \
                 head_dim, partial_head_dim, position, positions_ptr, position_ids_ptr, attn_factor, \
                 q_norm_ptr, k_norm_ptr, norm_eps, norm_constant_bias

    if      (rope_mode == ROPESTYLE_GPTJ) rope_kernel<ROPESTYLE_GPTJ><<<blocks, threads, 0, stream>>>(ARGS);
    else if (rope_mode == ROPESTYLE_NEOX) rope_kernel<ROPESTYLE_NEOX><<<blocks, threads, 0, stream>>>(ARGS);
}

</content>

<content full_path="exllamav3/exllamav3_ext/stloader.cpp">
#include "stloader.h"
#include <iostream>
#include <thread>
#include <mutex>
#include <deque>
#include <condition_variable>
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <Python.h>
#include "util.h"
#include "stloader_cu.cuh"
#include <cerrno>
#include <cstring>

uint8_t* host_buffers[STLOADER_THREADS] = {nullptr};

void stloader_read
(
    std::vector<uintptr_t> handles,
    size_t offset,
    size_t size,
    at::Tensor target
)
{
    at::Device device = target.device();
    bool target_cpu = device.is_cpu();
    cudaStream_t stream;

    // Buffers

    uint8_t* load_buffer;
    uint8_t* cuda_buffer;
    if (target_cpu)
    {
        load_buffer = (uint8_t*) target.data_ptr();
        cuda_buffer = nullptr;
    }
    else
    {
        load_buffer = (uint8_t*) malloc(size);
        TORCH_CHECK(load_buffer, "Can't allocate buffer for tensor");
        cuda_buffer = (uint8_t*) target.data_ptr();
        cudaSetDevice(device.index());
        stream = at::cuda::getCurrentCUDAStream(device.index()).stream();
    }

    // Synchronization

    Py_BEGIN_ALLOW_THREADS

    volatile bool load_failed = false;
    volatile int load_errnum = 0;
    std::mutex mtx;
    std::deque<std::pair<size_t, size_t>> dq;
    std::condition_variable cv;

    // Load chunks

    auto load_worker = [&] (size_t pos_a, int thread_idx)
    {
        FILE* file = reinterpret_cast<FILE*>(handles[thread_idx]);

        while (pos_a < size && !load_failed)
        {
            size_t pos_b = pos_a + STLOADER_BLOCK_SIZE;
            if (pos_b > size) pos_b = size;

            #ifdef __linux__
                ssize_t br = pread(fileno(file), load_buffer + pos_a, pos_b - pos_a, offset + pos_a);
                if (br != pos_b - pos_a) goto error;
            #else
                int sr = _fseeki64(file, static_cast<__int64>(offset + pos_a), SEEK_SET);
                if (sr) goto error;
                size_t br = fread(load_buffer + pos_a, 1, pos_b - pos_a, file);
                if (br != pos_b - pos_a) goto error;
            #endif

            {
                std::lock_guard<std::mutex> lock(mtx);
                dq.push_back(std::pair<size_t, size_t>(pos_a, pos_b));
                cv.notify_one();
            }

            pos_a += STLOADER_THREADS * STLOADER_BLOCK_SIZE;
        }

        return;

        error:
        if (file && ferror(file))
            printf(" ## Error reading file: %s (errno: %d)\n", strerror(errno), errno);
        load_errnum = errno;
        load_failed = true;
    };

    // Copy chunks to device

    auto copy_worker = [&] ()
    {
        cudaSetDevice(device.index());

        size_t total_blocks = CEIL_DIVIDE(size, STLOADER_BLOCK_SIZE);
        while (total_blocks && !load_failed)
        {
            size_t pos_a, pos_b;
            {
                std::unique_lock<std::mutex> lock(mtx);
                cv.wait(lock, [&dq] { return !dq.empty(); });

                auto pop = dq.front();
                dq.pop_front();
                total_blocks--;
                pos_a = std::get<0>(pop);
                pos_b = std::get<1>(pop);

                while (!dq.empty() && std::get<0>(dq.front()) == pos_b)
                {
                    pop = dq.front();
                    dq.pop_front();
                    pos_b = std::get<1>(pop);
                    total_blocks--;
                }
            }

            cudaError_t cr = cudaMemcpyAsync
            (
                cuda_buffer + pos_a,
                load_buffer + pos_a,
                pos_b - pos_a,
                cudaMemcpyHostToDevice,
                stream
            );

            if (cr != cudaSuccess)
            {
                fprintf(stderr, " ## GPUassert: %s\n", cudaGetErrorString(cr));
                goto error;
            }
        }
        return;

        error:
        load_errnum = errno;
        load_failed = true;
    };

    std::vector<std::thread> threads;
    if (cuda_buffer)
        threads.emplace_back(copy_worker);
    for (size_t i = 0; i < STLOADER_THREADS && i * STLOADER_BLOCK_SIZE < size; ++i)
        threads.emplace_back(load_worker, i * STLOADER_BLOCK_SIZE, i);
    for (auto& thread : threads)
        thread.join();

    if (load_failed)
        TORCH_CHECK
        (
            false, " ## Error reading file: ", std::strerror(load_errnum),
            " (errno=", load_errnum, ")"
        );

    if (!target_cpu)
    {
        cudaDeviceSynchronize();
        free(load_buffer);
    }

    Py_END_ALLOW_THREADS
}

std::vector<uintptr_t> stloader_open_file(const char* filename)
{
    std::vector<uintptr_t> handles;
    for (int i = 0; i < STLOADER_THREADS; ++i)
    {
        FILE* file = fopen(filename, "rb");
        if (!file)
        {
            int errnum = errno;
            TORCH_CHECK(
                false, " ## Error opening file '", filename, "': ",
                std::strerror(errnum), " (errno=", errnum, ")"
            );
        }
        handles.push_back(reinterpret_cast<uintptr_t>(file));
    }
    return handles;
}

void stloader_close_file(std::vector<uintptr_t> handles)
{
    for (int i = 0; i < handles.size(); ++i)
    {
        FILE* file = reinterpret_cast<FILE*>(handles[i]);
        fclose(file);

        if (host_buffers[i])
        {
            cudaFreeHost(host_buffers[i]);
            host_buffers[i] = nullptr;
        }
    }

}

void stloader_deferred_cpu(std::vector<TensorLoadJob> const& jobs)
{
    Py_BEGIN_ALLOW_THREADS
    volatile bool load_failed = false;
    volatile int load_errnum = 0;

    auto load_worker = [&] (int base_index)
    {
        int index = base_index;
        while (index < jobs.size())
        {
            TensorLoadJob const& job = jobs[index];
            FILE* file = reinterpret_cast<FILE*>(job.handles[base_index]);
            uint8_t* dest = reinterpret_cast<uint8_t*>(job.destination);

            #ifdef __linux__
                ssize_t br = pread(fileno(file), dest, job.bytesize, job.file_offset);
                if (br != job.bytesize) goto error;
            #else
                int sr = _fseeki64(file, static_cast<__int64>(job.file_offset), SEEK_SET);
                if (sr) goto error;
                size_t br = fread(dest, 1, job.bytesize, file);
                if (br != job.bytesize) goto error;
            #endif

            if (job.bf16_to_fp16)
                inplace_bf16_to_fp16_cpu(dest, job.bytesize / 2);

            index += STLOADER_THREADS;
        }

        return;

        error:
        load_errnum = errno;
        load_failed = true;
    };

    std::vector<std::thread> threads;
    for (int i = 0; i < STLOADER_THREADS && i < jobs.size(); ++i)
        threads.emplace_back(load_worker, i);
    for (auto& thread : threads)
        thread.join();

    TORCH_CHECK(!load_failed, "I/O error reading tensor");

    Py_END_ALLOW_THREADS
}

// TODO: GPUDirect option
void stloader_deferred_cuda(std::vector<TensorLoadJob> const& jobs, size_t max_chunk_size)
{
    Py_BEGIN_ALLOW_THREADS
    volatile bool load_failed = false;
    volatile int load_errnum = 0;

    auto load_worker = [&] (int base_index)
    {
        cudaError_t cr;

        if (!host_buffers[base_index])
            cudaMallocHost((void **) &host_buffers[base_index], max_chunk_size);
        uint8_t* temp = host_buffers[base_index];

        int index = base_index;
        while (index < jobs.size())
        {
            TensorLoadJob const& job = jobs[index];
            FILE* file = reinterpret_cast<FILE*>(job.handles[base_index]);
            uint8_t* dest = reinterpret_cast<uint8_t*>(job.destination);

            #ifdef __linux__
                ssize_t br = pread(fileno(file), temp, job.bytesize, job.file_offset);
                if (br != job.bytesize) goto error;
            #else
                int sr = _fseeki64(file, static_cast<__int64>(job.file_offset), SEEK_SET);
                if (sr) goto error;
                size_t br = fread(temp, 1, job.bytesize, file);
                if (br != job.bytesize) goto error;
            #endif

            cudaSetDevice(job.device_id);

            cudaError_t cr = cudaMemcpyAsync(dest, temp, job.bytesize, cudaMemcpyDefault);
            cudaDeviceSynchronize();
            if (cr != cudaSuccess)
            {
                fprintf(stderr, "GPUassert: %s\n", cudaGetErrorString(cr));
                goto error;
            }

            if (job.bf16_to_fp16)
                inplace_bf16_to_fp16_cuda(dest, job.bytesize / 2);

            index += STLOADER_THREADS;
        }

        return;

        error:
        load_failed = true;
    };

    std::vector<std::thread> threads;
    for (int i = 0; i < STLOADER_THREADS && i < jobs.size(); ++i)
        threads.emplace_back(load_worker, i);
    for (auto& thread : threads)
        thread.join();

    if (load_failed)
        TORCH_CHECK
        (
            false, " ## Error reading file: ", std::strerror(load_errnum),
            " (errno=", load_errnum, ")"
        );

    Py_END_ALLOW_THREADS
}


</content>

<content full_path="exllamav3/exllamav3_ext/quant/util.cu">
#include "util.cuh"
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include "../util.h"
#include "../util.cuh"

#define NUM_THREADS 1024
#define BLOCK_SIZE 32768

#define uint64_cu unsigned long long int

__device__ inline uint64_cu warp_reduce_sum(uint64_cu v)
{
    for (int offset = 32 >> 1; offset > 0; offset >>= 1)
    {
        uint64_cu other_v = __shfl_down_sync(0xffffffff, v, offset);
        v += other_v;
    }
    return v;
}

__device__ inline uint64_cu block_reduce_sum(uint64_cu v)
{
    __shared__ uint64_cu shared[NUM_THREADS / 32];

    int lane_id = threadIdx.x % 32;
    int warp_id = threadIdx.x / 32;

    v = warp_reduce_sum(v);

    if (lane_id == 0) shared[warp_id] = v;
    __syncthreads();

    int max_warp_id = NUM_THREADS / 32;
    if (warp_id == 0)
    {
        v = lane_id < max_warp_id ? shared[lane_id] : 0;
        v = warp_reduce_sum(v);
    }
    __syncthreads();
    return v;
}

__device__ inline bool isinf(half v)
{
    return isinf(__half2float(v));
}

__device__ inline bool isnan(half v)
{
    return isnan(__half2float(v));
}

template <typename T>
__global__ __launch_bounds__(NUM_THREADS)
void count_inf_nan_kernel
(
    const T* __restrict__ x,
    uint64_cu* __restrict__ y,
    uint64_cu numel
)
{
    uint64_cu idx = blockIdx.x * BLOCK_SIZE + threadIdx.x;
    uint64_cu max_idx = MIN(blockIdx.x * BLOCK_SIZE + BLOCK_SIZE, numel);
    uint64_cu thread_inf = 0;
    uint64_cu thread_nan = 0;
    for (; idx < max_idx; idx += NUM_THREADS)
    {
        T val = x[idx];
        if (isinf(val)) thread_inf++;
        if (isnan(val)) thread_nan++;
    }

    thread_inf = block_reduce_sum(thread_inf);
    thread_nan = block_reduce_sum(thread_nan);

    if (threadIdx.x == 0)
    {
        atomicAdd(y + 0, thread_inf);
        atomicAdd(y + 1, thread_nan);
    }
}

/*
Count number of inf and NaN values in tensor

x: Tensor to test
y: Output, dtype kLong, shape (2,)
*/

void count_inf_nan
(
    at::Tensor x,
    at::Tensor y
)
{
    const at::cuda::OptionalCUDAGuard device_guard(x.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();
    TORCH_CHECK_DTYPE(y, kLong);

    uint64_cu numel = x.numel();
    uint64_cu num_blocks = CEIL_DIVIDE(numel, BLOCK_SIZE);

    if (x.dtype() == at::kHalf)
        count_inf_nan_kernel<half><<<num_blocks, NUM_THREADS, 0, stream>>>
        (
            (const half*) x.data_ptr(),
            (uint64_cu*) y.data_ptr(),
            numel
        );
    else if (x.dtype() == at::kFloat)
        count_inf_nan_kernel<float><<<num_blocks, NUM_THREADS, 0, stream>>>
        (
            (const float*) x.data_ptr(),
            (uint64_cu*) y.data_ptr(),
            numel
        );
    else
        TORCH_CHECK(false, "Unsupported dtype");
}
</content>

<content full_path="exllamav3/exllamav3_ext/quant/exl3_devctx.cuh">
#pragma once

#include <tuple>
#include <mutex>

// Max allowable output size, in tiles. Used to allocate global lock buffer per device for sync across threadblocks
#define MAX_TILES_C (1024 * 1024)

#define MAX_DEVICES 32
#define CC_OLD        1
#define CC_AMPERE     2
#define CC_ADA        3
#define CC_HOPPER     4
#define CC_BLACKWELL  5

// Singleton to manage context for each device. Stores device attributes and a large-enough lock buffer per device
class DevCtx
{
private:
    int num_sms[MAX_DEVICES] = {};
    int cc[MAX_DEVICES] = {};
    void* locks[MAX_DEVICES] = {};
    std::mutex mtx;

public:
    static DevCtx& instance();
    int get_num_sms(int device);
    int get_cc(int device);
    int* get_locks(int device);

private:
    DevCtx() = default;
    DevCtx(const DevCtx&) = delete;
    DevCtx& operator=(const DevCtx&) = delete;
};
</content>

<content full_path="exllamav3/exllamav3_ext/quant/util.cuh">
#pragma once

#include <ATen/Tensor.h>
#include <tuple>

void count_inf_nan
(
    at::Tensor x,
    at::Tensor y
);
</content>

<content full_path="exllamav3/exllamav3_ext/quant/hadamard.cu">
#include "quantize.cuh"
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include "../util.h"
#include "../util.cuh"
#include "hadamard_inner.cuh"

__global__ __launch_bounds__(32)
void had_hf_r_128_kernel
(
    const half* __restrict__ input_ptr,
    half* __restrict__ output_ptr,
    const half* __restrict__ pre_scale,
    const half* __restrict__ post_scale,
    float r_scale
)
{
    input_ptr += gridDim.y * 128 * blockIdx.x + blockIdx.y * 128;
    output_ptr += gridDim.y * 128 * blockIdx.x + blockIdx.y * 128;
    had_hf_r_128_inner(input_ptr, output_ptr, pre_scale, post_scale, r_scale);
}

__global__ __launch_bounds__(32)
void had_ff_r_128_kernel
(
    const float* __restrict__ input_ptr,
    float* __restrict__ output_ptr,
    const half* __restrict__ pre_scale,
    const half* __restrict__ post_scale,
    float r_scale
)
{
    input_ptr += gridDim.y * 128 * blockIdx.x + blockIdx.y * 128;
    output_ptr += gridDim.y * 128 * blockIdx.x + blockIdx.y * 128;
    had_ff_r_128_inner(input_ptr, output_ptr, pre_scale, post_scale, r_scale);
}

/*
Compute y = (x.view(-1, 128) @ had_128).view(x.shape)
Works inplace if y == x
x and y must be same dtype, either float16 or float32
*/
void had_r_128
(
    const at::Tensor& input,
    const at::Tensor& output,
    const c10::optional<at::Tensor>& pre_scale,
    const c10::optional<at::Tensor>& post_scale,
    float scale
)
{
    const at::cuda::OptionalCUDAGuard device_guard(input.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    TORCH_CHECK_SHAPES_FULL(input, output);
    TORCH_CHECK_DIM(input, 2);
    TORCH_CHECK_DIV(input, 1, 128);
    int rows = input.size(0);
    int cols = input.size(1);

    int blocks = cols / 128;
    float r_scale = scale * 0.088388347648f; // scale / sqrt(128)

    dim3 blockDim(32);
    dim3 gridDim(rows, blocks);

    if (input.dtype() == at::kHalf)
    {
        TORCH_CHECK_DTYPE(output, kHalf);
        had_hf_r_128_kernel<<<gridDim, blockDim, 0, stream>>>
        (
            (const half*) input.data_ptr(),
            (half*) output.data_ptr(),
            (const half*) OPTPTR(pre_scale),
            (const half*) OPTPTR(post_scale),
            r_scale
        );
    }

    else if (input.dtype() == at::kFloat)
    {
        TORCH_CHECK_DTYPE(output, kFloat);
        had_ff_r_128_kernel<<<gridDim, blockDim, 0, stream>>>
        (
            (const float*) input.data_ptr(),
            (float*) output.data_ptr(),
            (const half*) OPTPTR(pre_scale),
            (const half*) OPTPTR(post_scale),
            r_scale
        );
    }

    else TORCH_CHECK(false, "unsupported datatype");
}
</content>

<content full_path="exllamav3/exllamav3_ext/quant/hadamard.cuh">
#pragma once

#include <ATen/Tensor.h>

void had_r_128
(
    const at::Tensor& input,
    const at::Tensor& output,
    const c10::optional<at::Tensor>& pre_scale,
    const c10::optional<at::Tensor>& post_scale,
    float scale
);

</content>

<content full_path="exllamav3/exllamav3_ext/quant/pack.cu">
#include "quantize.cuh"
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include "../util.h"
#include "../util.cuh"
#include "codebook.cuh"

template <int K>
__global__ __launch_bounds__(128)
void pack_trellis_kernel
(
    uint16_t* __restrict__ g_packed,
    const uint16_t* __restrict__ g_unpacked
)
{
    constexpr int packed_size = 256 * K / 16;
    __shared__ uint16_t s_unpacked[256];
    __shared__ uint16_t s_packed[packed_size];

    int t = threadIdx.x;
    g_packed += (gridDim.x * blockIdx.y + blockIdx.x) * packed_size;
    g_unpacked += (gridDim.x * blockIdx.y + blockIdx.x) * 256;

    ((uint32_t*) s_unpacked)[t] = ((uint32_t*) g_unpacked)[t];
    __syncthreads();

    // 16 spans of 16 weights to guarantee alignment for any K
    const int spans = 16;
    const int len = 256 / spans;
    if (t < spans)
    {
        int i = len * t;
        int j = K * t;
        int k = 32;
        uint32_t buf = 0;
        for (int n = 0; n < len; ++n)
        {
            uint32_t v = (uint32_t) s_unpacked[i];
            v &= ((1 << K) - 1);
            k -= K;
            buf |= (v << k);
            if (k <= 16)
            {
                s_packed[j] = (uint16_t) (buf >> 16);
                buf <<= 16;
                k += 16;
                j++;
            }
            i++;
        }
    }
    __syncthreads();

    if (t < packed_size / 2)
        ((uint32_t*) g_packed)[t] = SWAP16(((uint32_t*) s_packed)[t]);;
}

#define __(i) pack_trellis_kernel<i>
constexpr auto pack_trellis_kernel_instances = std::array
{
    __(1), __(2), __(3), __(4), __(5), __(6), __(7), __(8)
};
#undef __

void pack_trellis
(
    at::Tensor packed,
    at::Tensor unpacked,
    int K
)
{
    const at::cuda::OptionalCUDAGuard device_guard(unpacked.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    TORCH_CHECK_SHAPES(packed, 0, unpacked, 0, 1);
    TORCH_CHECK_SHAPES(packed, 1, unpacked, 1, 1);
    TORCH_CHECK_SIZE(unpacked, 2, 256);
    TORCH_CHECK_SIZE(packed, 2, 256 * K / 16);

    int rows = packed.size(0);
    int cols = packed.size(1);

    dim3 blockDim(128);
    dim3 gridDim(rows, cols);

    pack_trellis_kernel_instances[K - 1]<<<gridDim, blockDim, 0, stream>>>
    (
        (uint16_t*) packed.data_ptr(),
        (const uint16_t*) unpacked.data_ptr()
    );
    cuda_check(cudaPeekAtLastError());
}

template <int K>
__global__ __launch_bounds__(128)
void unpack_trellis_kernel
(
    uint16_t* __restrict__ g_unpacked,
    const uint16_t* __restrict__ g_packed
)
{
    constexpr int packed_size = 256 * K / 16;
    __shared__ uint16_t s_packed[packed_size];

    int t = threadIdx.x;
    g_packed += (gridDim.x * blockIdx.y + blockIdx.x) * packed_size;
    g_unpacked += (gridDim.x * blockIdx.y + blockIdx.x) * 256;

    // Read packed tile
    if (t < packed_size / 2)
        ((uint32_t*) s_packed)[t] = ((uint32_t*) g_packed)[t];
    __syncthreads();

    // Index two words
    int b0 = t * 2 * K + K - 16 + 256 * K;          // start of word0
    int b1 = b0 + K;                                // start of word1
    int b2 = b1 + 16;                               // end of word1
    int i0 = b0 / 32;                               // uint32 containing first bit of word0
    int i1 = (b2 - 1) / 32;                         // uint32 containing last bit of word1, may be == i0
    int s1 = (i1 + 1) * 32 - b2;                    // shift to align word1 to 32-bit boundary

    // Load 32-64 bits containing word0 and word1, overlapping by 16-K bits, correct for endianness
    uint32_t a = ((uint32_t*) s_packed)[i0 % (K * 256 / 32)];
    uint32_t b = ((uint32_t*) s_packed)[i1 % (K * 256 / 32)];
//    a = SWAP16(a);
//    b = SWAP16(b);

    // Shift into place
    uint32_t w1 = __funnelshift_r(b, a, s1);
    uint32_t w0 = w1 >> K;
    w0 &= 0xffff;
    w1 &= 0xffff;

    // Store
    uint32_t word01 = (w1 << 16) | w0;
    ((uint32_t*)g_unpacked)[t] = word01;
}

#define __(i) unpack_trellis_kernel<i>
constexpr auto unpack_trellis_kernel_instances = std::array
{
    __(1), __(2), __(3), __(4), __(5), __(6), __(7), __(8)
};
#undef __

void unpack_trellis
(
    at::Tensor unpacked,
    at::Tensor packed,
    int K
)
{
    const at::cuda::OptionalCUDAGuard device_guard(unpacked.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    TORCH_CHECK_SHAPES(packed, 0, unpacked, 0, 1);
    TORCH_CHECK_SHAPES(packed, 1, unpacked, 1, 1);
    TORCH_CHECK_SIZE(unpacked, 2, 256);
    TORCH_CHECK_SIZE(packed, 2, 256 * K / 16);

    int rows = packed.size(0);
    int cols = packed.size(1);

    dim3 blockDim(128);
    dim3 gridDim(cols, rows);

    unpack_trellis_kernel_instances[K - 1]<<<gridDim, blockDim, 0, stream>>>
    (
        (uint16_t*) unpacked.data_ptr(),
        (const uint16_t*) packed.data_ptr()
    );
    cuda_check(cudaPeekAtLastError());
}

__global__ __launch_bounds__(32)
void pack_signs_kernel
(
    uint16_t* __restrict__ g_packed,
    const uint16_t* __restrict__ g_unpacked,
    int cols
)
{
    int t = threadIdx.x;
    int idx = 32 * blockIdx.x + t;
    if (idx >= cols) return;
    g_unpacked += 16 * idx;
    g_packed += idx;

    // Not efficient but whatever
    uint16_t out = 0;
    for (int i = 0; i < 16; ++i)
    {
        uint16_t v = *g_unpacked++;
        v &= 0x8000;
        out >>= 1;
        out |= v;
    }

    *g_packed = out;
}

void pack_signs
(
    at::Tensor packed,
    at::Tensor unpacked
)
{
    const at::cuda::OptionalCUDAGuard device_guard(unpacked.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    TORCH_CHECK_DTYPE(unpacked, kHalf);
    TORCH_CHECK_DTYPE(packed, kShort);

    int cols = packed.size(0);
    dim3 blockDim(32);
    dim3 gridDim(CEIL_DIVIDE(cols, 32));

    pack_signs_kernel<<<gridDim, blockDim, 0, stream>>>
    (
        (uint16_t*) packed.data_ptr(),
        (const uint16_t*) unpacked.data_ptr(),
        cols
    );
    cuda_check(cudaPeekAtLastError());
}


</content>

<content full_path="exllamav3/exllamav3_ext/quant/pack.cuh">
#pragma once

#include <ATen/Tensor.h>

void pack_trellis
(
    at::Tensor packed,
    at::Tensor unpacked,
    int K
);

void unpack_trellis
(
    at::Tensor unpacked,
    at::Tensor packed,
    int K
);

void pack_signs
(
    at::Tensor packed,
    at::Tensor unpacked
);

</content>

<content full_path="exllamav3/exllamav3_ext/quant/exl3_kernel_map.cu">
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include <cooperative_groups.h>
namespace cg = cooperative_groups;
#include "../util.h"
#include "../util.cuh"
#include "../ptx.cuh"
#include <tuple>
#include <mutex>
#include "exl3_kernel_map.cuh"
#include "exl3_devctx.cuh"
#include "comp_units/exl3_comp_unit_1.cuh"
#include "comp_units/exl3_comp_unit_2.cuh"
#include "comp_units/exl3_comp_unit_3.cuh"
#include "comp_units/exl3_comp_unit_4.cuh"
#include "comp_units/exl3_comp_unit_5.cuh"
#include "comp_units/exl3_comp_unit_6.cuh"
#include "comp_units/exl3_comp_unit_7.cuh"
#include "comp_units/exl3_comp_unit_8.cuh"

int select_gemm_shape(int cc, int size_m, int size_k, int size_n, int bits, bool multi)
{
    bool mod_256 = (size_n % 256 == 0);
    bool mod_512 = (size_n % 512 == 0);

    switch(cc)
    {
        case CC_OLD:
        case CC_AMPERE:
            if (bits <= 4)
            {
                if (size_n <= 2048 || size_k <= 2048) return 2;
                return 3;
            }
            if (size_n < 4096) return size_k > 8192 ? 3 : 2;
            if (mod_512 && (size_n * size_k) > (4096 * 4096) && bits <= 6) return 4;
            if (mod_256) return 3;
            return 2;

        case CC_ADA:
            if (bits <= 3)
            {
                if (size_k <= 2048 && !multi) return 2;
                if (size_n < 4096 && size_k <= 12288) return 2;
                return 3;
            }
            if (size_n <= 16384) return 2;
            if (mod_512 && size_n >= 32768) return 4;
            if (mod_256) return 3;
            return 2;

        case CC_HOPPER:
        case CC_BLACKWELL:
            if ((bits == 4 || bits == 2) && !multi)
            {
                if (size_k <= 2048) return 1;
            }
            if (bits >= 7)
            {
                if (size_n <= 8192) return size_k > 32768 ? 3 : 2;
                if (mod_512 && size_n > 32768) return 4;
                return 2;
            }
            if (size_n <= 4096) return (size_k && bits >= 3) > 8192 ? 3 : 2;
            if (mod_512 && size_n > 16384) return 4;
            if (mod_256) return 3;
            return 2;
    }
    return 0;
}

int exl3_gemm_num_kernel_shapes()
{
    return EXL3_GEMM_NUM_SHAPES;
}

int exl3_gemm_tilesize_k[] = {EXL3_GEMM_TILESIZE_K};
int exl3_gemm_tilesize_n[] = {EXL3_GEMM_TILESIZE_N};
int exl3_gemm_blockdim[] = {EXL3_GEMM_BLOCKDIM};

bool exl3_gemm_shape_compat(int shape_idx, int size_m, int size_k, int size_n, int bits)
{
    int tilesize_k = exl3_gemm_tilesize_k[shape_idx];
    int tilesize_n = exl3_gemm_tilesize_n[shape_idx];
    return (size_k % tilesize_k == 0) && (size_n % tilesize_n == 0);
}

fp_exl3_gemm_kernel select_exl3_gemm_kernel
(
    int cc,
    int size_m,
    int size_k,
    int size_n,
    int bits,
    bool c_fp32,
    int force_shape_idx,
    int* out_block_dim,
    int* out_shape_idx,
    int* num_sms,
    int cb
)
{
    int shape_idx = force_shape_idx <= 0 ? select_gemm_shape(cc, size_m, size_k, size_n, bits, false) : force_shape_idx;
    TORCH_CHECK(shape_idx > 0, "exl3_gemm: no compatible kernel");
    if (out_shape_idx) *out_shape_idx = shape_idx;
    if (out_block_dim) *out_block_dim = exl3_gemm_blockdim[shape_idx];

    // Avoid empty blocks
    if (num_sms)
    {
        int tilesize_k = exl3_gemm_tilesize_k[shape_idx];
        int tilesize_n = exl3_gemm_tilesize_n[shape_idx];
        int max_slices = size_k / tilesize_k * size_n / tilesize_n;
        *num_sms = MAX(MIN(max_slices, *num_sms), 1);
    }

    int kernel_idx = shape_idx + (EXL3_GEMM_NUM_SHAPES + 1) * cb;

    if (c_fp32)
    {
        switch (bits)
        {
            case 1: return tfp_exl3_gemm_kernel_fp32_b1[kernel_idx];
            case 2: return tfp_exl3_gemm_kernel_fp32_b2[kernel_idx];
            case 3: return tfp_exl3_gemm_kernel_fp32_b3[kernel_idx];
            case 4: return tfp_exl3_gemm_kernel_fp32_b4[kernel_idx];
            case 5: return tfp_exl3_gemm_kernel_fp32_b5[kernel_idx];
            case 6: return tfp_exl3_gemm_kernel_fp32_b6[kernel_idx];
            case 7: return tfp_exl3_gemm_kernel_fp32_b7[kernel_idx];
            case 8: return tfp_exl3_gemm_kernel_fp32_b8[kernel_idx];
            default: TORCH_CHECK(false, "No kernel for GEMM shape");
        }
    }
    else
    {
        switch (bits)
        {
            case 1: return tfp_exl3_gemm_kernel_fp16_b1[kernel_idx];
            case 2: return tfp_exl3_gemm_kernel_fp16_b2[kernel_idx];
            case 3: return tfp_exl3_gemm_kernel_fp16_b3[kernel_idx];
            case 4: return tfp_exl3_gemm_kernel_fp16_b4[kernel_idx];
            case 5: return tfp_exl3_gemm_kernel_fp16_b5[kernel_idx];
            case 6: return tfp_exl3_gemm_kernel_fp16_b6[kernel_idx];
            case 7: return tfp_exl3_gemm_kernel_fp16_b7[kernel_idx];
            case 8: return tfp_exl3_gemm_kernel_fp16_b8[kernel_idx];
            default: TORCH_CHECK(false, "No kernel for GEMM shape");
        }
    }
}

fp_exl3_mgemm_kernel select_exl3_mgemm_kernel
(
    int cc,
    int size_m,
    int size_k,
    int size_n,
    int bits,
    bool c_fp32,
    int force_shape_idx,
    int* out_block_dim,
    int* out_shape_idx,
    int* num_sms,
    int cb
)
{
    int shape_idx = force_shape_idx <= 0 ? select_gemm_shape(cc, size_m, size_k, size_n, bits, true) : force_shape_idx;
    TORCH_CHECK(shape_idx > 0, "exl3_mgemm: no compatible kernel");
    if (out_shape_idx) *out_shape_idx = shape_idx;
    if (out_block_dim) *out_block_dim = exl3_gemm_blockdim[shape_idx];

    // Avoid empty blocks
    if (num_sms)
    {
        int tilesize_k = exl3_gemm_tilesize_k[shape_idx];
        int tilesize_n = exl3_gemm_tilesize_n[shape_idx];
        int max_slices = size_k / tilesize_k * size_n / tilesize_n / 24;
        *num_sms = MIN(max_slices, *num_sms);
    }

    int kernel_idx = shape_idx + (EXL3_GEMM_NUM_SHAPES + 1) * cb;

    if (c_fp32)
    {
        switch (bits)
        {
            case 1: return tfp_exl3_mgemm_kernel_fp32_b1[kernel_idx];
            case 2: return tfp_exl3_mgemm_kernel_fp32_b2[kernel_idx];
            case 3: return tfp_exl3_mgemm_kernel_fp32_b3[kernel_idx];
            case 4: return tfp_exl3_mgemm_kernel_fp32_b4[kernel_idx];
            case 5: return tfp_exl3_mgemm_kernel_fp32_b5[kernel_idx];
            case 6: return tfp_exl3_mgemm_kernel_fp32_b6[kernel_idx];
            case 7: return tfp_exl3_mgemm_kernel_fp32_b7[kernel_idx];
            case 8: return tfp_exl3_mgemm_kernel_fp32_b8[kernel_idx];
            default: TORCH_CHECK(false, "No kernel for GEMM shape");
        }
    }
    else
    {
        switch (bits)
        {
            case 1: return tfp_exl3_mgemm_kernel_fp16_b1[kernel_idx];
            case 2: return tfp_exl3_mgemm_kernel_fp16_b2[kernel_idx];
            case 3: return tfp_exl3_mgemm_kernel_fp16_b3[kernel_idx];
            case 4: return tfp_exl3_mgemm_kernel_fp16_b4[kernel_idx];
            case 5: return tfp_exl3_mgemm_kernel_fp16_b5[kernel_idx];
            case 6: return tfp_exl3_mgemm_kernel_fp16_b6[kernel_idx];
            case 7: return tfp_exl3_mgemm_kernel_fp16_b7[kernel_idx];
            case 8: return tfp_exl3_mgemm_kernel_fp16_b8[kernel_idx];
            default: TORCH_CHECK(false, "No kernel for GEMM shape");
        }
    }
}
</content>

<content full_path="exllamav3/exllamav3_ext/quant/quantize.cu">
#include "quantize.cuh"
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include "../util.h"
#include "../util.cuh"
#include "codebook.cuh"
#include <cmath>

#define NUM_THREADS 1024

template <int K, int cb>
__global__ __launch_bounds__(1024)
void quantize_tiles_kernel
(
    const float* __restrict__ input_tiles_ptr,
    float* __restrict__ output_tiles_ptr,
    uint16_t* __restrict__ output_indices_ptr,
    float* __restrict__ temp_costs_ptr,
    uint16_t* __restrict__ temp_edges_ptr,
    uint32_t mult
)
{
    int tile_idx = blockIdx.x;

    constexpr int Kr = 16 - K;
    constexpr int max_q = 1 << K;
    constexpr int edges = 65536 >> K;

    const float* input_tile = input_tiles_ptr + 256 * tile_idx;
    float* output_tile = output_tiles_ptr + 256 * tile_idx;
    uint16_t* output_indices = output_indices_ptr + 256 * tile_idx;
    float* temp_costs = temp_costs_ptr + 2 * edges * tile_idx;
    float* temp_costs_inc = temp_costs + edges;
    uint16_t* temp_edges = temp_edges_ptr + 256 * edges * tile_idx;

    auto forward = [&](int roll, int pre_state)
    {
        // Each thread iterates over all weights in the tile
        for (int i = 0; i < 256; ++i)
        {
            int ri = (i + roll) % 256;

            // Swap buffers.
            // temp_costs_inc[z] is the cost/cumulative error of an incoming edge from state (z & edge_mask)
            float* t = temp_costs;
            temp_costs = temp_costs_inc;
            temp_costs_inc = t;

            for (int out_edge_idx = threadIdx.x; out_edge_idx < edges; out_edge_idx += NUM_THREADS)
            {
                float w = input_tile[ri];

                float min_err = INFINITY;
                int min_in_edge = 0;

                #pragma unroll
                for (int k = 0; k < max_q; ++k)
                {
                    int state = (k << Kr) | out_edge_idx;

                    float err = decode_3inst_f_diff<cb>(state, w, mult);
                    err = err * err;

                    int in_edge_idx = state >> K;
                    if (i > 0)
                        err += temp_costs_inc[in_edge_idx];
                    else if (pre_state >= 0 && in_edge_idx != pre_state)
                        err = 1e30f;

                    if (err < min_err)
                    {
                        min_err = err;
                        min_in_edge = in_edge_idx;
                    }
                }

                temp_costs[out_edge_idx] = min_err;
                temp_edges[edges * ri + out_edge_idx] = (uint16_t) min_in_edge;
            }

            // Next iteration depends on costs computed by current iteration
            __syncthreads();
        }
    };

    auto argmin_cost = [&]()
    {
        // Find the final state with the lowest total cost. Return value is only valid in thread 0

        float local_min = 1e30f;
        int local_idx = -1;
        for (int e = threadIdx.x; e < edges; e += NUM_THREADS)
        {
            float v = temp_costs_inc[e];
            if (v < local_min)
            {
                local_min = v;
                local_idx = e;
            }
        }

        // Shuffle reduction
        int lane_id = threadIdx.x % 32;
        int warp_id = threadIdx.x / 32;

        #pragma unroll
        for (int offset = 16; offset > 0; offset >>= 1)
        {
            float other_min = __shfl_down_sync(0xffffffff, local_min, offset, 32);
            int other_idx = __shfl_down_sync(0xffffffff, local_idx, offset, 32);
            if (other_min < local_min)
            {
                local_min = other_min;
                local_idx = other_idx;
            }
        }

        __shared__ float s_min[32];
        __shared__ int s_idx[32];

        s_min[warp_id] = local_min;
        s_idx[warp_id] = local_idx;
        __syncthreads();

        if (warp_id == 0)
        {
            local_min = lane_id * 32 < edges ? s_min[lane_id] : 1e31f;
            local_idx = s_idx[lane_id];

            #pragma unroll
            for (int offset = 16; offset > 0; offset >>= 1)
            {
                float other_min = __shfl_down_sync(0xffffffff, local_min, offset, 32);
                int other_idx = __shfl_down_sync(0xffffffff, local_idx, offset, 32);
                if (other_min < local_min)
                {
                    local_min = other_min;
                    local_idx = other_idx;
                }
            }
        }

        return local_idx;
    };

    auto backward = [&](int roll, bool write, int edge)
    {
        // Construct output tile. Since the graph has to be walked, this will run in a single thread per block.
        // Profiling says this is not a bottleneck

        if (threadIdx.x == 0)
        {
            for (int i = 255; i >= 0; --i)
            {
                int ri = (i + roll) % 256;

                int prev_edge = (int) temp_edges[edges * ri + edge];
                int encoded = (prev_edge << K) | edge;
                edge = prev_edge;

                if (write)
                {
                    output_indices[ri] = (uint16_t) encoded;
                    output_tile[ri] = __half2float(decode_3inst<cb>(encoded, mult));
                }
                else if (ri == 0) break;
            }
        }

        // Broadcast to block
        __shared__ int broadcast;
        if (threadIdx.x == 0) broadcast = edge;
        __syncthreads();
        edge = broadcast;

        return edge;
    };

    // Solve starting at position 128 find initial state for second pass
    forward(128, -1);
    int end_state = argmin_cost();
    end_state = backward(128, false, end_state);

    // Solve again from position 0 with tail-biting constraint
    forward(0, end_state);
    backward(0, true, end_state);
}

#define __(i, cb) quantize_tiles_kernel<i, cb>
constexpr auto quantize_tiles_kernel_instances = std::array
{
    __(1, 0), __(2, 0), __(3, 0), __(4, 0), __(5, 0), __(6, 0), __(7, 0), __(8, 0),
    __(1, 1), __(2, 1), __(3, 1), __(4, 1), __(5, 1), __(6, 1), __(7, 1), __(8, 1),
    __(1, 2), __(2, 2), __(3, 2), __(4, 2), __(5, 2), __(6, 2), __(7, 2), __(8, 2)
};
#undef __

/*
Quantize batch of tiles

input_tiles: shape (n, 256), float
output_tiles: shape (n, 256), float
output_indices: shape (n, 256), uint16_t (unpacked)
temp_costs: shape (max_bsz, 2, 65536 >> K), float (scratch space for Viterbi algorithm)
temp_edges: shape (max_bsz, 256, 65536 >> K), uint16_t (scratch space for Viterbi algorithm)
K: number of bits per weight (1..8)
*/

void quantize_tiles
(
    at::Tensor input_tiles,
    at::Tensor output_tiles,
    at::Tensor output_indices,
    at::Tensor temp_costs,
    at::Tensor temp_edges,
    int K,
    uint32_t mcg_mult,
    uint32_t mul1_mult
)
{
    const at::cuda::OptionalCUDAGuard device_guard(input_tiles.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    TORCH_CHECK_DIM(input_tiles, 2);
    TORCH_CHECK_SIZE(input_tiles, 1, 256);
    TORCH_CHECK_SHAPES_FULL(input_tiles, output_indices);
    TORCH_CHECK_DTYPE(input_tiles, kFloat);
    TORCH_CHECK_DTYPE(output_tiles, kFloat);
    TORCH_CHECK_DTYPE(output_indices, kShort);

    int edges = 65536 >> K;
    int threads = MIN(NUM_THREADS, edges);

    int num_tiles = input_tiles.size(0);

    TORCH_CHECK_DTYPE(temp_costs, kFloat);
    TORCH_CHECK_DIM(temp_costs, 3);
    TORCH_CHECK_SIZE(temp_costs, 1, 2);
    TORCH_CHECK_SIZE(temp_costs, 2, edges);

    TORCH_CHECK_DTYPE(temp_edges, kShort);
    TORCH_CHECK_DIM(temp_edges, 3);
    TORCH_CHECK_SIZE(temp_edges, 1, 256);
    TORCH_CHECK_SIZE(temp_edges, 2, edges);

    int max_batch_size = temp_costs.size(0);

    int cb = 0;
    uint32_t mult = 0;
    if (mcg_mult) { cb = 1; mult = mcg_mult; }
    if (mul1_mult) { cb = 2; mult = mul1_mult; }

    int batch_i = 0;
    do
    {
        int batch_j = MIN(batch_i + max_batch_size, num_tiles);

        const float* input_tiles_ptr = ((const float*) input_tiles.data_ptr()) + 256 * batch_i;
        float* output_tiles_ptr = ((float*) output_tiles.data_ptr()) + 256 * batch_i;
        uint16_t* output_indices_ptr = ((uint16_t*) output_indices.data_ptr()) + 256 * batch_i;
        float* temp_costs_ptr = (float*) temp_costs.data_ptr();
        uint16_t* temp_edges_ptr = (uint16_t*) temp_edges.data_ptr();

        int bsz = batch_j - batch_i;
        int kernel_idx = K - 1 + 8 * cb;

        quantize_tiles_kernel_instances[kernel_idx]<<<bsz, threads, 0, stream>>>
        (
            input_tiles_ptr,
            output_tiles_ptr,
            output_indices_ptr,
            temp_costs_ptr,
            temp_edges_ptr,
            mult
        );
        cuda_check(cudaPeekAtLastError());

        batch_i = batch_j;
    }
    while (batch_i < num_tiles);
}

template <typename T>
__global__ //__launch_bounds__(64)
void decode_kernel
(
    const uint16_t* __restrict__ input_tiles_ptr,
    T* __restrict__ output_tiles_ptr,
    int cols,
    uint32_t mcg_mult,
    uint32_t mul1_mult
)
{
    int col = threadIdx.x + blockIdx.x * 64;
    if (col >= cols) return;
    int row = blockIdx.y;
    int idx = row * cols + col;

    uint32_t enc = (uint32_t) input_tiles_ptr[idx];
    half w;
    if (mcg_mult)
        w = decode_3inst<1>(enc, mcg_mult);
    else if (mul1_mult)
        w = decode_3inst<2>(enc, mul1_mult);
    else
        w = decode_3inst<0>(enc, 0);

    if constexpr (std::is_same_v<T, float>)
        output_tiles_ptr[idx] = __half2float(w);
    else
        output_tiles_ptr[idx] = w;
}

/*
Decode tensor

input_indices: uint16_t
output_tiles: float or half
mcg_mult: MCG multiplier, or 0 to use default LCG
*/

void decode
(
    at::Tensor input_indices,
    at::Tensor output_tiles,
    uint32_t mcg_mult,
    uint32_t mul1_mult
)
{
    const at::cuda::OptionalCUDAGuard device_guard(input_indices.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    TORCH_CHECK_DIM(input_indices, 2);
    TORCH_CHECK_SHAPES_FULL(input_indices, output_tiles);
    TORCH_CHECK_DTYPE(input_indices, kShort);

    int rows = input_indices.size(0);
    int cols = input_indices.size(1);

    dim3 blockDim(64);
    dim3 gridDim(cols / 64, rows);

    if (output_tiles.dtype() == at::kFloat)
        decode_kernel<<<gridDim, blockDim, 0, stream>>>
        (
            (const uint16_t*) input_indices.data_ptr(),
            (float*) output_tiles.data_ptr(),
            cols,
            mcg_mult,
            mul1_mult
        );
    else if (output_tiles.dtype() == at::kHalf)
        decode_kernel<<<gridDim, blockDim, 0, stream>>>
        (
            (const uint16_t*) input_indices.data_ptr(),
            (half*) output_tiles.data_ptr(),
            cols,
            mcg_mult,
            mul1_mult
        );
}


#define NUM_THREADS_TD 1024
#define MAX_BINS 1024

__global__ __launch_bounds__(NUM_THREADS_TD)
void test_distribution_kernel
(
    const float* __restrict__ input_ptr,
    float* __restrict__ dist_output_ptr,
    float* __restrict__ ref_output_ptr,
    uint64_t numel,
    uint64_t num_bins,
    float min_value,
    float max_value,
    uint32_t mcg_mult,
    uint32_t mul1_mult
)
{
    __shared__ int histogram[MAX_BINS];
    auto reset_histogram = [&]()
    {
        for (int i = threadIdx.x; i < num_bins; i += NUM_THREADS_TD)
            histogram[i] = 0;
        __syncthreads();
    };

    auto write_histogram = [&](float* output_ptr, uint64_t sc)
    {
        float scf = (float) sc;
        for (int i = threadIdx.x; i < num_bins; i += NUM_THREADS_TD)
            output_ptr[i] = ((float) histogram[i]) / scf;
        __syncthreads();
    };

    auto count = [&](float val)
    {
        val -= min_value;
        val /= (max_value - min_value);
        val *= (float) num_bins;
        int idx = (int) val;
        if (idx < 0) idx = 0;
        if (idx > num_bins - 1) idx = num_bins - 1;
        atomicAdd(&histogram[idx], 1);
    };

    if (ref_output_ptr)
    {
        reset_histogram();
        for (uint64_t i = threadIdx.x; i < 65536; i += NUM_THREADS_TD)
        {
            if (mcg_mult)
                count(decode_3inst_f<1>((uint16_t) (i & 0xffff), mcg_mult));
            else if (mul1_mult)
                count(decode_3inst_f<2>((uint16_t) (i & 0xffff), mul1_mult));
            else
                count(decode_3inst_f<0>((uint16_t) (i & 0xffff), 0));
        }
        __syncthreads();
        write_histogram(ref_output_ptr, 65536);
    }

    reset_histogram();
    for (uint64_t i = threadIdx.x; i < numel; i += NUM_THREADS_TD)
        count(input_ptr[i]);
    __syncthreads();
    write_histogram(dist_output_ptr, numel);
}

/*
Compare tensor distribution to codebook (not optimized)

input: tensor, float, any shape
dist_output: (empty) output histogram, float, shape (num_bins,)
ref_output, optional: (empty) output codebook histogram, float, shape (num_bins,)
*/

void test_distribution
(
    at::Tensor& input,
    at::Tensor& dist_output,
    const c10::optional<at::Tensor>& ref_output,
    float min_value,
    float max_value,
    uint32_t mcg_mult,
    uint32_t mul1_mult
)
{
    const at::cuda::OptionalCUDAGuard device_guard(input.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    TORCH_CHECK_DTYPE(input, kFloat);

    uint64_t numel = input.numel();
    float* ref_output_ptr = (float*) OPTPTR(ref_output);
    uint64_t num_bins = dist_output.numel();
    TORCH_CHECK(num_bins <= MAX_BINS, "Too many bins");
    if (ref_output_ptr)
        TORCH_CHECK(num_bins == ref_output.value().numel());

    test_distribution_kernel<<<1, NUM_THREADS_TD, 0, stream>>>
    (
        (const float*) input.data_ptr(),
        (float*) dist_output.data_ptr(),
        (float*) ref_output_ptr,
        numel,
        num_bins,
        min_value,
        max_value,
        mcg_mult,
        mul1_mult
    );
}
</content>

<content full_path="exllamav3/exllamav3_ext/quant/reconstruct.cu">
#include "reconstruct.cuh"
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include "../util.h"
#include "../util.cuh"
#include "../ptx.cuh"
#include "exl3_dq.cuh"

// TODO: Benchmark, profile, unit test

template <int K, int cb>
__global__ __launch_bounds__(256)
void reconstruct_kernel
(
    half* __restrict__ g_unpacked,
    const uint16_t* __restrict__ g_packed,
    uint32_t mult
)
{
    constexpr int packed_size = 256 * K / 16;  // in uint16s

    int t = threadIdx.x;
    int lane_id = t % 32;
    int warp_id = t / 32;
    int k = blockIdx.y;
    int n = blockIdx.x * 8;
    int tiles_n = gridDim.x;
    int blocks_n = tiles_n * 8;

    // Load packed 16*128 tile
    __shared__ uint32_t s_packed[8][packed_size / 2];
    g_packed += (k * blocks_n + n) * packed_size;
    for (int s = t; s < packed_size * 8 / 8; s += 256)
        ((int4*) s_packed)[t] = ((int4*) g_packed)[t];
    __syncthreads();

    // Dequant
    register FragB frag[2];
    dq_dispatch<K, cb>(s_packed[warp_id], lane_id * 8, frag[0], frag[1], mult);

    // Shuffle from tensor core layout to row major tile
//    __shared__ half tile[16 * 8 * 16];
    __shared__ half2 tile[16][8][8];

    half2 n0 = __shfl_down_sync(0xFFFFFFFF, frag[0][0], 4, 32);
    half2 n1 = __shfl_down_sync(0xFFFFFFFF, frag[0][1], 4, 32);
    half2 n2 = __shfl_down_sync(0xFFFFFFFF, frag[1][0], 4, 32);
    half2 n3 = __shfl_down_sync(0xFFFFFFFF, frag[1][1], 4, 32);
    __syncwarp();

    if (!(lane_id & 4))
    {
        half2 m0 = __halves2half2(__low2half(frag[0][0]), __low2half(n0));
        half2 m1 = __halves2half2(__high2half(frag[0][0]), __high2half(n0));
        half2 m2 = __halves2half2(__low2half(frag[0][1]), __low2half(n1));
        half2 m3 = __halves2half2(__high2half(frag[0][1]), __high2half(n1));
        half2 m4 = __halves2half2(__low2half(frag[1][0]), __low2half(n2));
        half2 m5 = __halves2half2(__high2half(frag[1][0]), __high2half(n2));
        half2 m6 = __halves2half2(__low2half(frag[1][1]), __low2half(n3));
        half2 m7 = __halves2half2(__high2half(frag[1][1]), __high2half(n3));
        int r0 = (lane_id % 4) * 2;
        int r1 = r0 + 1;
        int r2 = r0 + 8;
        int r3 = r0 + 9;
        int c0 = lane_id / 8;
        int c1 = c0 + 4;
        tile[r0][warp_id][c0] = m0;
        tile[r1][warp_id][c0] = m1;
        tile[r2][warp_id][c0] = m2;
        tile[r3][warp_id][c0] = m3;
        tile[r0][warp_id][c1] = m4;
        tile[r1][warp_id][c1] = m5;
        tile[r2][warp_id][c1] = m6;
        tile[r3][warp_id][c1] = m7;
    }
    __syncthreads();

    // Store unpacked tile
    int r = t / 16;
    int c = t % 16;
    int4* tile_int4 = (reinterpret_cast<int4*> (tile));
    int4* out_int4 = ((int4*) g_unpacked) + (k * 16 + r) * 2 * blocks_n + n * 2 + c;
    *out_int4 = tile_int4[t];
}

#define __(i, cb) reconstruct_kernel<i, cb>
constexpr auto reconstruct_kernel_instances = std::array
{
    __(1, 0), __(2, 0), __(3, 0), __(4, 0), __(5, 0), __(6, 0), __(7, 0), __(8, 0),
    __(1, 1), __(2, 1), __(3, 1), __(4, 1), __(5, 1), __(6, 1), __(7, 1), __(8, 1),
    __(1, 2), __(2, 2), __(3, 2), __(4, 2), __(5, 2), __(6, 2), __(7, 2), __(8, 2)
};
#undef __

/*
Reconstruct encoded+packed tensor
*/
void reconstruct
(
    at::Tensor unpacked,
    at::Tensor packed,
    int K,
    uint32_t mcg_mult,
    uint32_t mul1_mult
)
{
    const at::cuda::OptionalCUDAGuard device_guard(unpacked.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    TORCH_CHECK_SHAPES(unpacked, 0, packed, 0, 16);
    TORCH_CHECK_SHAPES(unpacked, 1, packed, 1, 16);
    TORCH_CHECK_SIZE(packed, 2, 256 * K / 16);
    TORCH_CHECK_DTYPE(unpacked, kHalf);

    int rows = packed.size(0);
    int cols = packed.size(1);

    dim3 blockDim(256);
    dim3 gridDim(cols / 8, rows);

    int cbi = K - 1;
    uint32_t mult = 0;
    if (mcg_mult)
    {
        mult = mcg_mult;
        cbi += 8;
    }
    else if (mul1_mult)
    {
        mult = mul1_mult;
        cbi += 16;
    }

    reconstruct_kernel_instances[cbi]<<<gridDim, blockDim, 0, stream>>>
    (
        (half*) unpacked.data_ptr(),
        (const uint16_t*) packed.data_ptr(),
        mult
    );
    cuda_check(cudaPeekAtLastError());
}

</content>

<content full_path="exllamav3/exllamav3_ext/quant/codebook.cuh">
#pragma once

// "3INST" procedural codebook

template <int cb>
__device__ inline half decode_3inst(uint32_t x, uint32_t mult)
{
    if constexpr (cb == 0)
    {
        x *= 89226354u;
        x += 64248484u;
        asm volatile ("lop3.b32 %0, %0, 0x8fff8fff, 0x3b603b60, 0x6a;" : "+r"(x));
        half2_uint32 xu(x);
        return __hadd(__low2half(xu.as_half2), __high2half(xu.as_half2));
    }
    if constexpr (cb == 1)
    {
        x *= mult;
        asm volatile ("lop3.b32 %0, %0, 0x8fff8fff, 0x3b603b60, 0x6a;" : "+r"(x));
        half2_uint32 xu(x);
        return __hadd(__low2half(xu.as_half2), __high2half(xu.as_half2));
    }
    if constexpr (cb == 2)
    {
        x *= mult;
        uint32_t sum;
        const uint32_t acc = 0x6400u;  // 0x6400 -> 1024.0 ..  0x67FF -> 2047.0
        asm volatile ("vabsdiff4.u32.u32.u32.add %0, %1, %2, %3;" : "=r"(sum) : "r"(x), "r"(0), "r"(acc) : );
        const __half k_inv_h = __ushort_as_half(0x1eee);  //  0.00677 = 1/147.7
        const __half k_bias_h = __ushort_as_half(0xc931);  // -10.39 = (-1024.0 - 510.0) * k_inv_h
        half_uint16 h((uint16_t) sum);
        return __hfma(h.as_half, k_inv_h, k_bias_h);
    }
}

template <int cb>
__device__ inline half2 decode_3inst_2(uint32_t x0, uint32_t x1, uint32_t mult)
{
    if constexpr (cb == 0)
    {
        x0 *= 89226354u;
        x1 *= 89226354u;
        x0 += 64248484u;
        x1 += 64248484u;
        asm volatile ("lop3.b32 %0, %0, 0x8fff8fff, 0x3b603b60, 0x6a;" : "+r"(x0));
        asm volatile ("lop3.b32 %0, %0, 0x8fff8fff, 0x3b603b60, 0x6a;" : "+r"(x1));
        half2_uint32 xu0(x0);
        half2_uint32 xu1(x1);
        half2 d0 = __lows2half2(xu0.as_half2, xu1.as_half2);
        half2 d1 = __highs2half2(xu0.as_half2, xu1.as_half2);
        return __hadd2(d0, d1);
    }
    if constexpr (cb == 1)
    {
        x0 *= mult;
        x1 *= mult;
        asm volatile ("lop3.b32 %0, %0, 0x8fff8fff, 0x3b603b60, 0x6a;" : "+r"(x0));
        asm volatile ("lop3.b32 %0, %0, 0x8fff8fff, 0x3b603b60, 0x6a;" : "+r"(x1));
        half2_uint32 xu0(x0);
        half2_uint32 xu1(x1);
        half2 d0 = __lows2half2(xu0.as_half2, xu1.as_half2);
        half2 d1 = __highs2half2(xu0.as_half2, xu1.as_half2);
        return __hadd2(d0, d1);
    }
    if constexpr (cb == 2)
    {
        x0 *= mult;
        x1 *= mult;
        uint32_t sum0;
        uint32_t sum1;
        const uint32_t acc = 0x6400u;  // 0x6400 -> 1024.0 ..  0x67FF -> 2047.0
        asm volatile ("vabsdiff4.u32.u32.u32.add %0, %1, %2, %3;" : "=r"(sum0) : "r"(x0), "r"(0), "r"(acc) : );
        asm volatile ("vabsdiff4.u32.u32.u32.add %0, %1, %2, %3;" : "=r"(sum1) : "r"(x1), "r"(0), "r"(acc) : );
        half2 k_inv_h2 = __half2half2(__ushort_as_half(0x1eee));  //  0.00677 = 1/147.7
        half2 k_bias_h2 = __half2half2(__ushort_as_half(0xc931));  // -10.39 = (-1024.0 - 510.0) * k_inv_h
        half_uint16 h0((uint16_t) sum0);
        half_uint16 h1((uint16_t) sum1);
        return __hfma2(__halves2half2(h0.as_half, h1.as_half), k_inv_h2, k_bias_h2);
    }
}

template <int cb>
__device__ inline float decode_3inst_f(uint64_t x, uint32_t mult)
{
    return __half2float(decode_3inst<cb>(x, mult));
}

template <int cb>
__device__ inline float decode_3inst_f_diff(uint64_t x, float d, uint32_t mult)
{
    return __half2float(decode_3inst<cb>(x, mult)) - d;
}

// "2MAD" procedural codebook, much more overhead than 3INST, slightly better distribution at 2bpw
// Not used currently

__device__ inline half decode_2mad(uint64_t x)
{
    x = x * 264435761u + 1013904223u;
    x = ((x * 1664525u) >> 32) + x;
    int32_t c = (int32_t) __dp4a((uint32_t) x, 0x01010101u, 0xFFFFFE02u);
    half y = __hmul(__int2half_rn(c), __float2half_rn(0.008415));
    return y;
}

__device__ inline float decode_2mad_f(uint64_t x)
{
    x = x * 264435761u + 1013904223u;
    x = ((x * 1664525u) >> 32) + x;
    int32_t c = (int32_t) __dp4a((uint32_t) x, 0x01010101u, 0xFFFFFE02u);
    float y = __int2float_rn(c) * 0.008415f;
    return y;
}

__device__ inline float decode_2mad_f_diff(uint64_t x, float d)
{
    x = x * 264435761u + 1013904223u;
    x = ((x * 1664525u) >> 32) + x;
    int32_t c = (int32_t) __dp4a((uint32_t) x, 0x01010101u, 0xFFFFFE02u);
    float y = fma(__int2float_rn(c), 0.008415f, -d);
    return y;
}

</content>

<content full_path="exllamav3/exllamav3_ext/quant/exl3_gemm.cuh">
#pragma once

#include <ATen/Tensor.h>

int exl3_gemm
(
    const at::Tensor& A,
    const at::Tensor& B,
    at::Tensor& C,
    const c10::optional<at::Tensor>& suh,
    const c10::optional<at::Tensor>& A_had,
    const c10::optional<at::Tensor>& svh,
    int force_shape_idx,
    uint32_t mcg_mult,
    uint32_t mul1_mult
);

int exl3_mgemm
(
    const at::Tensor& A,
    const at::Tensor& B,
    at::Tensor& C,
    const at::Tensor& suh,
    const at::Tensor& A_had,
    const at::Tensor& svh,
    const c10::optional<at::Tensor>& indices,
    const c10::optional<at::Tensor>& weights,
    int K,
    int force_shape_idx,
    uint32_t mcg_mult,
    uint32_t mul1_mult
);

</content>

<content full_path="exllamav3/exllamav3_ext/quant/quantize.cuh">
#pragma once

#include <ATen/Tensor.h>

void quantize_tiles
(
    at::Tensor input_tiles,
    at::Tensor output_tiles,
    at::Tensor output_indices,
    at::Tensor temp_costs,
    at::Tensor temp_edges,
    int K,
    uint32_t mcg_mult,
    uint32_t mul1_mult
);

void decode
(
    at::Tensor input_indices,
    at::Tensor output_tiles,
    uint32_t mcg_mult,
    uint32_t mul1_mult
);

void test_distribution
(
    at::Tensor& input,
    at::Tensor& dist_output,
    const c10::optional<at::Tensor>& ref_output,
    float min_value,
    float max_value,
    uint32_t mcg_mult,
    uint32_t mul1_mult
);
</content>

<content full_path="exllamav3/exllamav3_ext/quant/hadamard_inner.cuh">
#pragma once

// Hadamard transform 128-element vector across one warp, with optional pre and post scales

__device__ inline half hreduce(half2 x)
{
    return __hadd(__low2half(x), __high2half(x));
}

__device__ inline float shuffle_had_fx32(float v, int lane_id)
{
    for (int i = 1; i < 32; i <<= 1)
    {
        float pv = __shfl_xor_sync(0xffffffff, v, i);
        uint32_t* vi = reinterpret_cast<uint32_t*>(&v);
        int32_t sfm = -static_cast<int16_t>(lane_id & i) >> 31;
        *vi ^= (sfm & 0x80000000);
        v = v + pv;
    }
    return v;
}

__device__ inline half2 shuffle_had_h2x32(half2 v, int lane_id)
{
    for (int i = 1; i < 32; i <<= 1)
    {
        half2 pv = __shfl_xor_sync(0xffffffff, v, i);
        uint32_t* vi = reinterpret_cast<uint32_t*>(&v);
        int32_t sfm = -static_cast<int16_t>(lane_id & i) >> 31;
        *vi ^= (sfm & 0x80008000);
        v = __hadd2(v, pv);
    }
    return v;
}

// Half vector, half scales

inline __device__
void had_hf_r_128_inner
(
    const half* __restrict__ input_ptr,
    half* __restrict__ output_ptr,
    const half* __restrict__ pre_scale,
    const half* __restrict__ post_scale,
    float r_scale
)
{
    int t = threadIdx.x % 32;

    // Load
    half4 v = ((half4*) input_ptr)[t];

    // Pre scale
    if (pre_scale)
    {
        int i = blockIdx.y * 32 + t;
        half4 scales = ((half4*) pre_scale)[i];
        v.x = __hmul2(v.x, scales.x);
        v.y = __hmul2(v.y, scales.y);
    }

    // 4 element had
    float v0 = __half2float(__low2half(v.x));
    float v1 = __half2float(__high2half(v.x));
    float v2 = __half2float(__low2half(v.y));
    float v3 = __half2float(__high2half(v.y));
    float h0 = v0 + v1 + v2 + v3;
    float h1 = v0 - v1 + v2 - v3;
    float h2 = v0 + v1 - v2 - v3;
    float h3 = v0 - v1 - v2 + v3;

    // 32 element had, warp shuffle
    h0 = shuffle_had_fx32(h0, t) * r_scale;
    h1 = shuffle_had_fx32(h1, t) * r_scale;
    h2 = shuffle_had_fx32(h2, t) * r_scale;
    h3 = shuffle_had_fx32(h3, t) * r_scale;
    v.x = __floats2half2_rn(h0, h1);
    v.y = __floats2half2_rn(h2, h3);

    // Post scale
    if (post_scale)
    {
        int i = blockIdx.y * 32 + t;
        half4 scales = ((half4*) post_scale)[i];
        v.x = __hmul2(v.x, scales.x);
        v.y = __hmul2(v.y, scales.y);
    }

    // Store
    ((half4*) output_ptr)[t] = v;
}

// Float vector, half scales

inline __device__
void had_ff_r_128_inner
(
    const float* __restrict__ input_ptr,
    float* __restrict__ output_ptr,
    const half* __restrict__ pre_scale,
    const half* __restrict__ post_scale,
    float r_scale
)
{
    int t = threadIdx.x % 32;

    // Load
    float4 v = ((float4*) input_ptr)[t];

    // Pre scale
    if (pre_scale)
    {
        int i = blockIdx.y * 32 + t;
        half4 scales = ((half4*) pre_scale)[i];
        v.x *= __low2float(scales.x);
        v.y *= __high2float(scales.x);
        v.z *= __low2float(scales.y);
        v.w *= __high2float(scales.y);
    }

    // 4 element had
    float v0 = v.x;
    float v1 = v.y;
    float v2 = v.z;
    float v3 = v.w;
    float h0 = v0 + v1 + v2 + v3;
    float h1 = v0 - v1 + v2 - v3;
    float h2 = v0 + v1 - v2 - v3;
    float h3 = v0 - v1 - v2 + v3;

    // 32 element had, warp shuffle
    v.x = shuffle_had_fx32(h0, t) * r_scale;
    v.y = shuffle_had_fx32(h1, t) * r_scale;
    v.z = shuffle_had_fx32(h2, t) * r_scale;
    v.w = shuffle_had_fx32(h3, t) * r_scale;

    // Post scale
    if (post_scale)
    {
        int i = blockIdx.y * 32 + t;
        half4 scales = ((half4*) post_scale)[i];
        v.x *= __low2float(scales.x);
        v.y *= __high2float(scales.x);
        v.z *= __low2float(scales.y);
        v.w *= __high2float(scales.y);
    }

    // Store
    ((float4*) output_ptr)[t] = v;
}
</content>

<content full_path="exllamav3/exllamav3_ext/quant/exl3_gemm_inner.cuh">
#pragma once

#include "../ptx.cuh"

// Constants
#define EXL3_GEMM_BASE_THREADS 256
#define SMEM_MAX (90 * 1024)  // max shared memory on compute capability 8.6

#include "exl3_dq.cuh"

template<EXL3_GEMM_T_ARGS>
inline __device__
void exl3_gemm_kernel_inner
(
    const half* __restrict__  A,
    const uint16_t* __restrict__ B,
    void* __restrict__ C,
    int size_m,
    int size_k,
    int size_n,
    int* __restrict__ locks,
    uint32_t mult
)
{
    const int TILEBLOCKS_M = TILESIZE_M / 16;
    const int TILEBLOCKS_K = TILESIZE_K / 16;
    const int TILEBLOCKS_N = TILESIZE_N / 16;
    const int FRAGS_M = TILEBLOCKS_M;
    const int FRAGS_N_PER_WARP = 2 * TILEBLOCKS_N / (EXL3_GEMM_BASE_THREADS / 32);

    const int sh_a_stage_size = TILESIZE_M * TILESIZE_K;                         // in halfs
    const int sh_b_stage_size = TILEBLOCKS_K * TILEBLOCKS_N * 256 / 16 * bits;   // in uint16s
    const int sh_c_size = 4 * EXL3_GEMM_BASE_THREADS;                            // in floats

    // Sanity checks
    static_assert(EXL3_GEMM_BASE_THREADS == 256);
    static_assert(TILESIZE_M % 16 == 0, "Invalid kernel params");
    static_assert(TILESIZE_K % 16 == 0, "Invalid kernel params");
    static_assert(TILESIZE_N % 128 == 0, "Invalid kernel params");
    static_assert
    (
        SMEM_MAX >= SH_STAGES * (2 * sh_a_stage_size + 2 * sh_b_stage_size) + 4 * sh_c_size,
        "Invalid kernel params (insufficient shared memory for shape)"
    );

    // Shared memory
    extern __shared__ half shared[];
    half* sh_a = shared;
    uint16_t* sh_b = (uint16_t*) (sh_a + SH_STAGES * sh_a_stage_size);
    float* sh_c = (float*) (sh_b + sh_b_stage_size * SH_STAGES);

    // Thread index
    int t = threadIdx.x % EXL3_GEMM_BASE_THREADS;
    int sub_k = threadIdx.x / EXL3_GEMM_BASE_THREADS;
    int warp_id = t / 32;
    int lane_id = t % 32;

    // Dimensions
    Dim3 size = { size_m, size_k, size_n };
    Dim3 tiles = { CEIL_DIVIDE(size_m, TILESIZE_M), size_k / TILESIZE_K, size_n / TILESIZE_N };
    Dim3 blocks = { 1, tiles.k * TILEBLOCKS_K, tiles.n * TILEBLOCKS_N };

    // Start and end index of current slice, must span at least one tile
    int num_slices = gridDim.x;
    int slice_beg = tiles.numel_b() * blockIdx.x / num_slices;
    int slice_end = tiles.numel_b() * (blockIdx.x + 1) / num_slices;
    int slice_len = slice_end - slice_beg;
    if (slice_len < 1) return;

    auto index_m = [&] (int slice_i) { return 0; }; //blockIdx.y; };
    auto index_k = [&] (int slice_i) { return (slice_i % tiles.k); };
    auto index_n = [&] (int slice_i) { return (slice_i / tiles.k); };

    // Batch dimension
    int slice_m = index_m(slice_beg);
    int max_m = MIN(size_m - slice_m * TILESIZE_M, TILESIZE_M);

    // Pipe 0, global A, B tile and shared A, B tile
    int slice0_k = index_k(slice_beg);
    int slice0_n = index_n(slice_beg);
    int slice0_iters = slice_len;

    int gl_a_stride_m = TILESIZE_M * size_k;
    const int gl_a_stride_k = TILESIZE_K;
    const int sh0_a_stride_m = TILESIZE_M * TILESIZE_K;
    const half* gl_a_ptr = A + slice_m * gl_a_stride_m + slice0_k * gl_a_stride_k;
    half* sh0_a_ptr = sh_a + (slice0_iters % SH_STAGES) * sh_a_stage_size;

    const int load_a_iters = CEIL_DIVIDE(sh0_a_stride_m / 8, EXL3_GEMM_BASE_THREADS);
    bool pred_a_gl[load_a_iters];
    int load_a_gl[load_a_iters];
    for (int i = 0; i < load_a_iters; ++i)
    {
        int k = (i * EXL3_GEMM_BASE_THREADS + t) % (gl_a_stride_k / 8);
        int m = (i * EXL3_GEMM_BASE_THREADS + t) / (gl_a_stride_k / 8);
        load_a_gl[i] = m * size_k / 8 + k;
        pred_a_gl[i] = m < max_m;
    }

    int gl_b_stride_k = blocks.n * TILEBLOCKS_K * 256 / 16 * bits;
    const int gl_b_stride_n = TILEBLOCKS_N * 256 / 16 * bits;
    const int sh0_b_stride_k = TILEBLOCKS_K * TILEBLOCKS_N * 256 / 16 * bits;
    const uint16_t* gl_b_ptr = B + slice0_k * gl_b_stride_k + slice0_n * gl_b_stride_n;
    uint16_t* sh0_b_ptr = sh_b + (slice0_iters % SH_STAGES) * sh_b_stage_size;

    const int load_b_iters = CEIL_DIVIDE(sh0_b_stride_k / 8, EXL3_GEMM_BASE_THREADS);
    bool pred_b_gl[load_b_iters];
    int load_b_gl[load_b_iters];
    for (int i = 0; i < load_b_iters; ++i)
    {
        int n = (i * EXL3_GEMM_BASE_THREADS + t) % (gl_b_stride_n / 8);
        int k = (i * EXL3_GEMM_BASE_THREADS + t) / (gl_b_stride_n / 8);
        load_b_gl[i] = k * blocks.n * 256 / 16 * bits / 8 * k + n;
        pred_b_gl[i] = i * EXL3_GEMM_BASE_THREADS + t < sh0_b_stride_k / 8;
    }

    auto advance0 = [&] ()
    {
        slice0_k++;
        slice0_iters--;

        int stage = slice0_iters % SH_STAGES;
        sh0_a_ptr = sh_a + stage * sh_a_stage_size;
        sh0_b_ptr = sh_b + stage * sh_b_stage_size;

        if (slice0_k >= tiles.k)
        {
            slice0_k = 0;
            slice0_n++;
            gl_a_ptr = A + slice_m * gl_a_stride_m + slice0_k * gl_a_stride_k;
            gl_b_ptr = B + slice0_k * gl_b_stride_k + slice0_n * gl_b_stride_n;
        }
        else
        {
            gl_a_ptr += gl_a_stride_k;
            gl_b_ptr += gl_b_stride_k;
        }
    };

    // Pipe 1, shared A, B tile and registers
    int slice1_k = slice0_k;
    int slice1_n = slice0_n;
    int slice1_iters = slice0_iters;

    half* sh1_a_ptr = sh_a + (slice1_iters % SH_STAGES) * sh_a_stage_size;
    uint16_t* sh1_b_ptr = sh_b + (slice1_iters % SH_STAGES) * sh_b_stage_size;

    auto advance1 = [&] ()
    {
        slice1_k++;
        slice1_iters--;

        int stage = slice1_iters % SH_STAGES;
        sh1_a_ptr = sh_a + stage * sh_a_stage_size;
        sh1_b_ptr = sh_b + stage * sh_b_stage_size;

        if (slice1_k >= tiles.k)
        {
            slice1_k = 0;
            slice1_n++;
        }
    };

    // Pipe 2
    int slice2_k = slice0_k;
    int slice2_k0 = slice0_k;
    int slice2_n = slice0_n;
    int slice2_iters = slice0_iters;

    int gl_c_stride_n = TILESIZE_N;
    int gl_c_stride_m = TILESIZE_M * size_n;

    half* gl_c_ptr_16 = ((half*) C) + slice_m * gl_c_stride_m + slice2_n * gl_c_stride_n;
    float* gl_c_ptr_32 = ((float*) C) + slice_m * gl_c_stride_m + slice2_n * gl_c_stride_n;

    register FragA frag_a[FRAG_STAGES][FRAGS_M];
    register FragB frag_b[FRAG_STAGES][FRAGS_N_PER_WARP];
    register FragC frag_c[FRAGS_M][FRAGS_N_PER_WARP];

    auto advance2 = [&] ()
    {
        slice2_k++;
        slice2_iters--;

        if (slice2_k >= tiles.k)
        {
            slice2_k = 0;
            slice2_k0 = 0;
            slice2_n++;
            if constexpr (c_fp32)
                gl_c_ptr_32 += gl_c_stride_n;
            else
                gl_c_ptr_16 += gl_c_stride_n;
        }
    };

    // Schedule load of the next A, B tiles to shared memory and advance the pipeline
    auto async_load_gl = [&] ()
    {
        if (sub_k)
        {
            cp_async_fence();
            return;
        }

        if (slice0_iters)
        {
            // Copy tile from row-major A matrix
            {
                const int4* gl = (const int4*) gl_a_ptr;
                int4* sh = (int4*) sh0_a_ptr;
                #pragma unroll
                for (int i = 0; i < load_a_iters; ++i)
                {
                    // TODO: Rearrange into ldmatrix friendly layout while loading?
                    // cp_async_pred(sh + EXL3_GEMM_BASE_THREADS * i + t, gl + load_a_gl[i], pred_a_gl[i]);
                    if (pred_a_gl[i]) cp_async(sh + EXL3_GEMM_BASE_THREADS * i + t, gl + load_a_gl[i]);
                }
            }

            // Copy tile of 256-element blocks from quantized B matrix
            {
                const int4* gl = (const int4*) gl_b_ptr;
                int4* sh = (int4*) sh0_b_ptr;
                #pragma unroll
                for (int i = 0; i < load_b_iters; ++i)
                {
                    // cp_async_pred(sh + EXL3_GEMM_BASE_THREADS * i + t, gl + load_b_gl[i], pred_b_gl[i]);
                    if (pred_b_gl[i]) cp_async(sh + EXL3_GEMM_BASE_THREADS * i + t, gl + load_b_gl[i]);
                }
            }
            advance0();
        }

        // Sync and advance
        cp_async_fence();
    };

    // Load fragments
    // Ref. for fragment layout:
    // https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#matrix-fragments-for-mma-m16n8k16-with-floating-point-type
    auto load_frags = [&] (int buf)
    {
        if (!slice1_iters) return;

        // A fragments
        {
            // TODO: Resolve bank conflicts
            int r = (lane_id % 8) + 8 * ((lane_id / 8) % 2);
            int c = lane_id / 16;
            int4* sha = (int4*) sh1_a_ptr + r * TILESIZE_K / 8 + c;
            #pragma unroll
            for (int m = 0; m < TILEBLOCKS_M; ++m)
                ldsm4(frag_a[buf][m], sha + (m * 16) * TILESIZE_K / 8 + sub_k * 16 / 8);
        }

        // B fragments
        int r0 = lane_id / 2;
        int c0 = (lane_id % 2) * 8;

        #pragma unroll
        for (int n2 = 0; n2 < FRAGS_N_PER_WARP; n2 += 2)
        {
            int sub_n2 = warp_id * FRAGS_N_PER_WARP / 2 + n2 / 2;
            const uint32_t* shb = (const uint32_t*) (sh1_b_ptr + (sub_k * TILEBLOCKS_N + sub_n2) * 256 / 16 * bits);

            dq_dispatch<bits, cb>(shb, r0 * 16 + c0, frag_b[buf][n2], frag_b[buf][n2 + 1], mult);
        }

        __syncthreads();
        advance1();
    };

    // Clear C fragments
    auto clear_frag_c = [&] ()
    {
        #pragma unroll
        for (int m = 0; m < FRAGS_M; ++m)
            #pragma unroll
            for (int n = 0; n < FRAGS_N_PER_WARP; ++n)
                frag_c[m][n] = {};
    };

    // Threadblock reduction
    auto threadblock_reduce = [&] ()
    {
        auto store = [&] (int i, int m, int n)
        {
            // TODO: Shuffle to avoid bank conflicts here? Doesn't seem to be a bottleneck
            // TODO: Always accumulates entire C fragment, could be limited when size_m < 16
            if (sub_k == i)
            {
                float* sh_red = sh_c + (FRAGS_N_PER_WARP * 4) * t;
                for (int i = 0; i < 4; ++i)
                    *sh_red++ = frag_c[m][n][i];
            }
            __syncthreads();
        };

        auto add = [&] (int i, int m, int n)
        {
            if (sub_k == i)
            {
                float* sh_red = sh_c + (FRAGS_N_PER_WARP * 4) * t;
                for (int i = 0; i < 4; ++i)
                    frag_c[m][n][i] += *sh_red++;
            }
            __syncthreads();
        };

        for (int m = 0; m < FRAGS_M; ++m)
        for (int n = 0; n < FRAGS_N_PER_WARP; ++n)
        {
            if constexpr (TILEBLOCKS_K == 2)
            {
                store(1, m, n);
                add(0, m, n);
            }
            if constexpr (TILEBLOCKS_K == 3)
            {
                store(1, m, n);
                add(0, m, n);
                store(2, m, n);
                add(0, m, n);
            }
            if constexpr (TILEBLOCKS_K == 4)
            {
                store(3, m, n);
                add(2, m, n);
                store(1, m, n);
                add(0, m, n);
                store(2, m, n);
                add(0, m, n);
            }
        }
    };

    // Output reduction
    auto reduce = [&] ()
    {
        // First reduce all partial sums along k for the current slice
        threadblock_reduce();

        // Process (partial) slices within column in reverse order so the threadblock doing the bottom slice is
        // free to proceed to the next column right away
        int lock_i = tiles.k - slice2_k - 1;
        int lock_d = slice2_k - slice2_k0 + 1;
        int* lock = &locks[slice_m * blocks.n + slice2_n];

        barrier_acquire(lock, lock_i);

        bool first = lock_i == 0;
        bool last = lock_i + lock_d == tiles.k;

        int n0 = warp_id * FRAGS_N_PER_WARP;

        // Second and subsequent threadblocks in column read back the intermediate sum from global memory
        // TODO: Use an intermediate layout to make these writes coalesce
        if (!sub_k && !first)
        {
            for (int n = 0; n < FRAGS_N_PER_WARP; ++n)
            {
                for (int m = 0; m < FRAGS_M; ++m)
                {
                    int r0 = lane_id / 4 + 16 * m;
                    int r1 = r0 + 8;
                    int c = (lane_id % 4) * 2;
                    if (r0 < max_m)
                    {
                        if constexpr (c_fp32)
                        {
                            float* c_ptr = gl_c_ptr_32 + r0 * size_n + (n0 + n) * 8 + c;
                            frag_c[m][n][0] += *c_ptr++;
                            frag_c[m][n][1] += *c_ptr++;
                        }
                        else
                        {
                            half2* c_ptr = (half2*) (gl_c_ptr_16 + r0 * size_n + (n0 + n) * 8 + c);
                            float2 interm = __half22float2(*c_ptr);
                            frag_c[m][n][0] += interm.x;
                            frag_c[m][n][1] += interm.y;
                        }
                    }
                    if (r1 < max_m)
                    {
                        if constexpr (c_fp32)
                        {
                            float* c_ptr = gl_c_ptr_32 + r1 * size_n + (n0 + n) * 8 + c;
                            frag_c[m][n][2] += *c_ptr++;
                            frag_c[m][n][3] += *c_ptr++;
                        }
                        else
                        {
                            half2* c_ptr = (half2*) (gl_c_ptr_16 + r1 * size_n + (n0 + n) * 8 + c);
                            float2 interm = __half22float2(*c_ptr);
                            frag_c[m][n][2] += interm.x;
                            frag_c[m][n][3] += interm.y;
                        }
                    }
                }
            }
        }

        // All but last threadblock in column threadblocks write the intermediate result to global memory
        if (!sub_k && !last)
        {
            for (int n = 0; n < FRAGS_N_PER_WARP; ++n)
            {
                for (int m = 0; m < FRAGS_M; ++m)
                {
                    int r0 = lane_id / 4 + 16 * m;
                    int r1 = r0 + 8;
                    int c = (lane_id % 4) * 2;
                    if (r0 < max_m)
                    {
                        if constexpr (c_fp32)
                        {
                            float* c_ptr = gl_c_ptr_32 + r0 * size_n + (n0 + n) * 8 + c;
                            *c_ptr++ = frag_c[m][n][0];
                            *c_ptr++ = frag_c[m][n][1];
                        }
                        else
                        {
                            half2* c_ptr = (half2*) (gl_c_ptr_16 + r0 * size_n + (n0 + n) * 8 + c);
                            half2 sum = __floats2half2_rn(frag_c[m][n][0], frag_c[m][n][1]);
                            *c_ptr = sum;
                        }
                    }
                    if (r1 < max_m)
                    {
                        if constexpr (c_fp32)
                        {
                            float* c_ptr = gl_c_ptr_32 + r1 * size_n + (n0 + n) * 8 + c;
                            *c_ptr++ = frag_c[m][n][2];
                            *c_ptr++ = frag_c[m][n][3];
                        }
                        else
                        {
                            half2* c_ptr = (half2*) (gl_c_ptr_16 + r1 * size_n + (n0 + n) * 8 + c);
                            half2 sum = __floats2half2_rn(frag_c[m][n][2], frag_c[m][n][3]);
                            *c_ptr = sum;
                        }
                    }
                }
            }
        }

        // Last block writes in row-major format
        if (!sub_k && last)
        {
            for (int n = 0; n < FRAGS_N_PER_WARP; ++n)
            {
                for (int m = 0; m < FRAGS_M; ++m)
                {
                    int r0 = lane_id / 4 + 16 * m;
                    int r1 = r0 + 8;
                    int c = (lane_id % 4) * 2;
                    if (r0 < max_m)
                    {
                        if constexpr (c_fp32)
                        {
                            float* c_ptr = gl_c_ptr_32 + r0 * size_n + (n0 + n) * 8 + c;
                            *c_ptr++ = frag_c[m][n][0];
                            *c_ptr++ = frag_c[m][n][1];
                        }
                        else
                        {
                            half2* c_ptr = (half2*) (gl_c_ptr_16 + r0 * size_n + (n0 + n) * 8 + c);
                            half2 sum = __floats2half2_rn(frag_c[m][n][0], frag_c[m][n][1]);
                            *c_ptr = sum;
                        }
                    }
                    if (r1 < max_m)
                    {
                        if constexpr (c_fp32)
                        {
                            float* c_ptr = gl_c_ptr_32 + r1 * size_n + (n0 + n) * 8 + c;
                            *c_ptr++ = frag_c[m][n][2];
                            *c_ptr++ = frag_c[m][n][3];
                        }
                        else
                        {
                            half2* c_ptr = (half2*) (gl_c_ptr_16 + r1 * size_n + (n0 + n) * 8 + c);
                            half2 sum = __floats2half2_rn(frag_c[m][n][2], frag_c[m][n][3]);
                            *c_ptr = sum;
                        }
                    }
                }
            }
        }

        barrier_release(lock, lock_d, last);

        clear_frag_c();
    };

    // Wait until there are at most SH_STAGES - 2 async copies pending, i.e. at least one stage has finished loading
    auto wait_stage = [&] ()
    {
        cp_async_wait<SH_STAGES - 2>();
        __syncthreads();
    };

    // Perform tensor core matmul on current tile
    auto matmul = [&] (int buf)
    {
        for (int m = 0; m < FRAGS_M; ++m)
            for (int n = 0; n < FRAGS_N_PER_WARP; ++n)
                ptx_mma_m16n8k16(frag_a[buf][m], frag_b[buf][n], frag_c[m][n]);
    };

    // Start global to shared pipeline
    for (int i = 0; i < SH_STAGES - 1; ++i)
        async_load_gl();
    wait_stage();

    // Start shared to register pipeline.
    clear_frag_c();
    if constexpr (FRAG_STAGES > 1)
        load_frags(0);

    // Main loop. Fragments are double buffered to allow more interleaving. This is especially important to hide the
    // dequantization overhead, but we need two different iterations of the main loop to avoid confusing the compiler
    // and making it (sometimes) place the fragment arrays in local memory

    #define FSTAGE(_load, _mul) \
        async_load_gl(); \
        wait_stage(); \
        load_frags(_load); \
        matmul(_mul); \
        if (slice2_k == tiles.k - 1 || slice2_iters == 1) { reduce(); slice2_k0 = slice2_k + 1; } \
        advance2(); \
        if (!slice2_iters) break; \

    if constexpr (FRAG_STAGES == 1)
    {
        while (true)
        {
            FSTAGE(0, 0);
        }
    }

    if constexpr (FRAG_STAGES == 2)
    {
        while (true)
        {
            FSTAGE(1, 0);
            FSTAGE(0, 1);
        }
    }

    if constexpr (FRAG_STAGES == 3)
    {
        while (true)
        {
            FSTAGE(1, 0);
            FSTAGE(2, 1);
            FSTAGE(0, 2);
        }
    }

    if constexpr (FRAG_STAGES == 4)
    {
        while (true)
        {
            FSTAGE(1, 0);
            FSTAGE(2, 1);
            FSTAGE(3, 2);
            FSTAGE(0, 3);
        }
    }

    if constexpr (FRAG_STAGES == 5)
    {
        while (true)
        {
            FSTAGE(1, 0);
            FSTAGE(2, 1);
            FSTAGE(3, 2);
            FSTAGE(4, 3);
            FSTAGE(0, 4);
        }
    }
}

</content>

<content full_path="exllamav3/exllamav3_ext/quant/exl3_dq.cuh">
#pragma once

#include "codebook.cuh"

__device__ __forceinline__ uint32_t fshift(uint32_t b, uint32_t a, int shift)
{
     uint64_t merged = ((uint64_t)a << 32) | (uint64_t) b;
     return (uint32_t)(merged >> shift);

    // Conditional funnel shift is somehow no longer faster
    // if (shift < 32) return __funnelshift_r(b, a, shift);
    // return a >> (shift - 32);
}

template <int bits, int cb>
__device__ __forceinline__ half dq(const uint32_t* ptr, int t_offset, uint32_t mult)
{
    int b0 = t_offset * bits + bits - 16 + 256 * bits;  // bit index, start of word0
    int b1 = b0 + 16;                                   // bit index, end of word0
    int i0 = b0 / 32;                                   // uint32 containing first bit of word0
    int i1 = (b1 - 1) / 32;                             // uint32 containing last bit of word0, may be == i0
    int s0 = (i1 + 1) * 32 - b1;                        // shift value to align word1 to 32-bit boundary

    // Load 32 or 64 bits containing word0
    uint32_t a = ptr[i0 % (bits * 256 / 32)];
    uint32_t b = ptr[i1 % (bits * 256 / 32)];

    // Shift into place
    uint32_t w0 = __funnelshift_r(b, a, s0) & 0xffff;
    return decode_3inst<cb>(w0, mult);
}

template <int bits, int cb>
__device__ __forceinline__ half2 dq2(const uint32_t* ptr, int t_offset, uint32_t mult)
{
    int b0 = t_offset * bits + bits - 16 + 256 * bits;  // bit index, start of word0
    int b1 = b0 + 16;                                   // bit index, end of word0
    int i0 = b0 / 32;                                   // uint32 containing first bit of word0
    int i1 = (b1 - 1) / 32;                             // uint32 containing last bit of word0, may be == i0
    int s0 = (i1 + 1) * 32 - b1;                        // shift value to align word1 to 32-bit boundary

    // Load 32 or 64 bits containing word0
    uint32_t a = ptr[i0 % (bits * 256 / 32)];
    uint32_t b = ptr[i1 % (bits * 256 / 32)];

    // Shift into place
    uint32_t w1 = __funnelshift_r(b, a, s0)        & 0xffff;
    uint32_t w0 = __funnelshift_r(b, a, s0 + bits) & 0xffff;
    return decode_3inst_2<cb>(w0, w1, mult);
}

template <int bits, int cb>
__device__ __forceinline__ void dq4(const uint32_t* ptr, int t_offset, FragB& frag, uint32_t mult)
{
    int b0 = (t_offset + 257) * bits - 16;      // start of first word
    int b1 = b0 + 3 * bits;                     // start of last word
    int b2 = b1 + 16;                           // end of last word
    int i0 = b0 / 32;                           // uint32 containing first bit of first word
    int i2 = (b2 - 1) / 32;                     // uint32 containing last bit of last word, may be == i0
    int s2 = (i2 + 1) * 32 - b2;                // shift value to align last word to 32-bit boundary

    uint32_t a = ptr[i0 % (bits * 256 / 32)];
    uint32_t b = ptr[i2 % (bits * 256 / 32)];
    uint32_t w3 = fshift(b, a, s2)            & 0xffff;
    uint32_t w2 = fshift(b, a, s2 + bits)     & 0xffff;
    uint32_t w1 = fshift(b, a, s2 + bits * 2) & 0xffff;
    uint32_t w0 = fshift(b, a, s2 + bits * 3) & 0xffff;
    half2 d0d1 = decode_3inst_2<cb>(w0, w1, mult);
    half2 d2d3 = decode_3inst_2<cb>(w2, w3, mult);
    frag[0] = d0d1;
    frag[1] = d2d3;
}

template <int bits, int cb>
__device__ __forceinline__ void dq2x2(const uint32_t* ptr, int t_offset, FragB& frag, uint32_t mult)
{
    #pragma unroll
    for (int i = 0; i < 2; ++i)
    {
        int b0 = (t_offset + 2 * i + 257) * bits - 16;  // start of first word
        int b1 = b0 + 1 * bits;                         // start of last word
        int b2 = b1 + 16;                               // end of last word
        int i0 = b0 / 32;                               // uint32 containing first bit of first word
        int i2 = (b2 - 1) / 32;                         // uint32 containing last bit of last word, may be == i0
        int s2 = (i2 + 1) * 32 - b2;                    // shift value to align last word to 32-bit boundary

        uint32_t a = ptr[i0 % (bits * 256 / 32)];
        uint32_t b = ptr[i2 % (bits * 256 / 32)];
        uint32_t w1 = fshift(b, a, s2)        & 0xffff;
        uint32_t w0 = fshift(b, a, s2 + bits) & 0xffff;
        half2 d0d1 = decode_3inst_2<cb>(w0, w1, mult);
        frag[i] = d0d1;
    }
}

template <int bits, int cb, int align>
__device__ __forceinline__ void dq8(const uint32_t* ptr, int t_offset, FragB& frag0, FragB& frag1, uint32_t mult)
{
    int b1 = (t_offset + 257) * bits;               // end of first word
    int b0 = b1 - 16;                               // start of first word
    int b2 = b1 + bits * 7;
    int i0 = b0 / 32;                               // uint32 containing first bit of word0
    int i2 = (b2 - 1) / 32;                         // uint32 containing last bit of word0, may be == i0
    int s2 = (i2 + 1) * 32 - b2;                    // shift value to align last word to 32-bit boundary

    uint32_t a = ptr[i0 % (bits * 256 / 32)];
    uint32_t b = ptr[i2 % (bits * 256 / 32)];
    uint32_t w0, w1, w2, w3, w4, w5, w6, w7;
    if constexpr (align == 1)
    {
        w7 = fshift(b, a, s2);
        w6 = fshift(b, a, s2 + bits);
        w5 = fshift(b, a, s2 + bits * 2);
        w4 = fshift(b, a, s2 + bits * 3);
        w3 = fshift(b, a, s2 + bits * 4);
        w2 = fshift(b, a, s2 + bits * 5);
        w1 = fshift(b, a, s2 + bits * 6);
        w0 = fshift(b, a, s2 + bits * 7);
    }
    if constexpr (align == 2)
    {
        w7 = fshift(b, a, s2);
        w6 = w7 >> bits;
        w5 = fshift(b, a, s2 + bits * 2);
        w4 = w5 >> bits;
        w3 = fshift(b, a, s2 + bits * 4);
        w2 = w3 >> bits;
        w1 = fshift(b, a, s2 + bits * 6);
        w0 = w1 >> bits;
    }
    if constexpr (align == 4)
    {
        w7 = fshift(b, a, s2);
        w6 = w7 >> bits;
        w5 = w6 >> bits;
        w4 = w5 >> bits;
        w3 = fshift(b, a, s2 + bits * 4);
        w2 = w3 >> bits;
        w1 = w2 >> bits;
        w0 = w1 >> bits;
    }
    if constexpr (align == 8)
    {
        w7 = fshift(b, a, s2);
        w6 = w7 >> bits;
        w5 = w6 >> bits;
        w4 = w5 >> bits;
        w3 = w4 >> bits;
        w2 = w3 >> bits;
        w1 = w2 >> bits;
        w0 = w1 >> bits;
    }
    half2 d0d1 = decode_3inst_2<cb>(w0 & 0xffff, w1 & 0xffff, mult);
    half2 d2d3 = decode_3inst_2<cb>(w2 & 0xffff, w3 & 0xffff, mult);
    half2 d4d5 = decode_3inst_2<cb>(w4 & 0xffff, w5 & 0xffff, mult);
    half2 d6d7 = decode_3inst_2<cb>(w6 & 0xffff, w7 & 0xffff, mult);
    frag0[0] = d0d1;
    frag0[1] = d2d3;
    frag1[0] = d4d5;
    frag1[1] = d6d7;
}

template <int cb>
__device__ __forceinline__ void dq8_aligned_4bits(const uint32_t* ptr, int t_offset, FragB& frag0, FragB& frag1, uint32_t mult)
{
    int i1 = t_offset / 8;
    int i0 = (i1 + 31) % 32;

    uint32_t a = ptr[i0];
    uint32_t b = ptr[i1];
    uint32_t w7 = b & 0xffff;
    uint32_t w6 = (b >> 4) & 0xffff;
    uint32_t w5 = (b >> 8) & 0xffff;
    uint32_t w4 = (b >> 12) & 0xffff;
    uint32_t w3 = (b >> 16) & 0xffff;
    uint32_t w2 = __funnelshift_r(b, a, 20);
    uint32_t w1 = w2 >> 4;
    uint32_t w0 = w2 >> 8;
    w2 = w2 & 0xffff;
    w1 = w1 & 0xffff;
    w0 = w0 & 0xffff;
    half2 d0d1 = decode_3inst_2<cb>(w0, w1, mult);
    half2 d2d3 = decode_3inst_2<cb>(w2, w3, mult);
    half2 d4d5 = decode_3inst_2<cb>(w4, w5, mult);
    half2 d6d7 = decode_3inst_2<cb>(w6, w7, mult);
    frag0[0] = d0d1;
    frag0[1] = d2d3;
    frag1[0] = d4d5;
    frag1[1] = d6d7;
}

template <int cb>
__device__ __forceinline__ void dq8_aligned_4bits_bfe(const uint32_t* ptr, int t_offset, FragB& frag0, FragB& frag1, uint32_t mult)
{
    int i1 = t_offset / 8;
    int i0 = (i1 + 31) % 32;

    uint32_t a = ptr[i0];
    uint32_t b = ptr[i1];
    uint32_t w7 = bfe64(b, a, 0, 16);
    uint32_t w6 = bfe64(b, a, 4, 16);
    uint32_t w5 = bfe64(b, a, 8, 16);
    uint32_t w4 = bfe64(b, a, 12, 16);
    uint32_t w3 = bfe64(b, a, 16, 16);
    uint32_t w2 = bfe64(b, a, 20, 16);
    uint32_t w1 = bfe64(b, a, 24, 16);
    uint32_t w0 = bfe64(b, a, 28, 16);
    half2 d0d1 = decode_3inst_2<cb>(w0, w1, mult);
    half2 d2d3 = decode_3inst_2<cb>(w2, w3, mult);
    half2 d4d5 = decode_3inst_2<cb>(w4, w5, mult);
    half2 d6d7 = decode_3inst_2<cb>(w6, w7, mult);
    frag0[0] = d0d1;
    frag0[1] = d2d3;
    frag1[0] = d4d5;
    frag1[1] = d6d7;
}

template <int bits, int cb>
__device__ __forceinline__ void dq_dispatch(const uint32_t* ptr, int idx, FragB& frag0, FragB& frag1, uint32_t mult)
{
    if constexpr (bits == 1)
    {
        dq8<bits, cb, 4>(ptr, idx, frag0, frag1, mult);
    }
    else if constexpr (bits == 2)
    {
        dq8<bits, cb, 4>(ptr, idx, frag0, frag1, mult);
    }
    else if constexpr (bits == 3)
    {
        dq8<bits, cb, 2>(ptr, idx, frag0, frag1, mult);
    }
    else if constexpr (bits == 4)
    {
        dq8_aligned_4bits<cb>(ptr, idx, frag0, frag1, mult);
    }
    else if constexpr (bits == 5)
    {
        dq4<bits, cb>(ptr, idx, frag0, mult);
        dq4<bits, cb>(ptr, idx + 4, frag1, mult);
    }
    else if constexpr (bits == 6)
    {
        dq4<bits, cb>(ptr, idx, frag0, mult);
        dq4<bits, cb>(ptr, idx + 4, frag1, mult);
    }
    else if constexpr (bits == 7)
    {
        dq2x2<bits, cb>(ptr, idx, frag0, mult);
        dq2x2<bits, cb>(ptr, idx + 4, frag1, mult);
    }
    else if constexpr (bits == 8)
    {
        dq4<bits, cb>(ptr, idx, frag0, mult);
        dq4<bits, cb>(ptr, idx + 4, frag1, mult);
    }
}
</content>

<content full_path="exllamav3/exllamav3_ext/quant/exl3_gemm.cu">
#include "exl3_gemm.cuh"

#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include <cooperative_groups.h>
namespace cg = cooperative_groups;
#include "../util.h"
#include "../util.cuh"
#include "exl3_gemm_kernel.cuh"
#include "exl3_kernel_map.cuh"
#include "exl3_devctx.cuh"
#include <set>

/*
EXL3 matmul, A @ B -> C

- A: row-major A tensor, shape (m, k), dtype float16, contiguous
- B: EXL3-quantized B tensor, shape (k//16, n//16, 16*bits), dtype uint16
- C: empty row-major C tensor, shape (m, n), dtype float16 or float23, contiguous. Does not need to be zero-initialized
- suh: optional, packed input scales/flips, shape (k//16), dtype float16
- A_had: required if suh given, may be reference to A, temporary storage for input transform, size and dtype as A
- svh: optional, packed output scales/flips, shape (n//16), dtype float16

limitations:
- k % 16 == 0
- n % 128 == 0
*/

std::set<void*> kernel_attr_set[MAX_DEVICES] = {};

int exl3_gemm
(
    const at::Tensor& A,
    const at::Tensor& B,
    at::Tensor& C,
    const c10::optional<at::Tensor>& suh,
    const c10::optional<at::Tensor>& A_had,
    const c10::optional<at::Tensor>& svh,
    int force_shape_idx,
    uint32_t mcg_mult,
    uint32_t mul1_mult
)
{
    const at::cuda::OptionalCUDAGuard device_guard(A.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    TORCH_CHECK_DIM(B, 3);
    TORCH_CHECK_SHAPES(A, 1, B, 0, 16);
    TORCH_CHECK_SHAPES(C, -1, B, 1, 16);
//    TORCH_CHECK_SHAPES(A, 0, C, 0, 1);
    TORCH_CHECK_DTYPE(A, kHalf);
    TORCH_CHECK_DTYPE(B, kShort);
    bool c_fp32 = C.dtype() == at::kFloat;
    if (!c_fp32) TORCH_CHECK_DTYPE(C, kHalf);

    // Get SU, optionally
    const half* suh_ptr = (const half*) OPTPTR(suh);
    half* A_had_ptr = nullptr;
    if (suh_ptr)
    {
//        TORCH_CHECK_SHAPES(suh.value(), 0, A, 1, 1);
        A_had_ptr = (half*) OPTPTR(A_had);
//        TORCH_CHECK(A_had_ptr, "Must supply A_had with suh");
//        TORCH_CHECK_SHAPES_FULL(A_had.value(), A);
    }

    // Get SV, optionally
    const half* svh_ptr = (const half*) OPTPTR(svh);
//    if (svh_ptr)
//        TORCH_CHECK_SHAPES(svh.value(), 0, B, 1, 16);

    // Device properties
    int device;
    cudaGetDevice(&device);
    int num_sms = DevCtx::instance().get_num_sms(device);
    int cc = DevCtx::instance().get_cc(device);
    int* locks = DevCtx::instance().get_locks(device);

    // Dispatch
    int bits = B.size(2) / 16;
    const half* A_ptr = (const half*) A.data_ptr();
    const uint16_t* B_ptr = (const uint16_t*) B.data_ptr();
    void* C_ptr = (void*) C.data_ptr();
    int size_m = A.size(0);
    int size_k = A.size(1);
    int size_n = B.size(1) * 16;

    // Select kernel
    TORCH_CHECK(!(mcg_mult && mul1_mult), "Specified both mcg_mult and mul1_mult")
    int cb = 0;
    uint32_t mult = 0;
    if (mcg_mult) { cb = 1; mult = mcg_mult; }
    if (mul1_mult) { cb = 2; mult = mul1_mult; }

    int selected_shape;
    int block_dim;
    fp_exl3_gemm_kernel kernel = select_exl3_gemm_kernel
    (
        cc, size_m, size_k, size_n, bits, c_fp32,
        force_shape_idx, &block_dim, &selected_shape,
        &num_sms, cb
    );
    if (!kernel) return 0;

    // Launch
    if (kernel_attr_set[device].find((void*)kernel) == kernel_attr_set[device].end())
    {
        cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, SMEM_MAX);
        kernel_attr_set[device].insert((void*)kernel);
    }
    void* kernelArgs[] =
    {
        (void*)& A_ptr,
        (void*)& B_ptr,
        (void*)& C_ptr,
        (void*)& size_m,
        (void*)& size_k,
        (void*)& size_n,
        (void*)& locks,
        (void*)& suh_ptr,
        (void*)& A_had_ptr,
        (void*)& svh_ptr,
        (void*)& mult
    };
    cudaLaunchCooperativeKernel
    (
        (void*)kernel,
        num_sms,
        block_dim,
        kernelArgs,
        SMEM_MAX,
        stream
    );
    cuda_check(cudaPeekAtLastError());
    return selected_shape;
}

/*
EXL3 multi matmul, A @ B -> C

- A: row-major A tensor, shape (m, k), dtype float16, contiguous
- B: EXL3-quantized B tensor, shape (k//16, n//16, 16*bits), dtype uint16
- C: empty row-major C tensor, shape (m, n), dtype float16 or float23, contiguous. Does not need to be zero-initialized
- suh: optional, packed input scales/flips, shape (k//16), dtype float16
- A_had: required if suh given, may be reference to A, temporary storage for input transform, size and dtype as A
- svh: optional, packed output scales/flips, shape (n//16), dtype float16

limitations:
- k % 16 == 0
- n % 128 == 0
*/

int exl3_mgemm
(
    const at::Tensor& A,
    const at::Tensor& B,
    at::Tensor& C,
    const at::Tensor& suh,
    const at::Tensor& A_had,
    const at::Tensor& svh,
    const c10::optional<at::Tensor>& indices,
    const c10::optional<at::Tensor>& weights,
    int K,
    int force_shape_idx,
    uint32_t mcg_mult,
    uint32_t mul1_mult
)
{
    const at::cuda::OptionalCUDAGuard device_guard(A.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    TORCH_CHECK_DTYPE(A, kHalf);
    TORCH_CHECK_DTYPE(B, kLong);
    TORCH_CHECK_DTYPE(suh, kLong);
    TORCH_CHECK_DTYPE(svh, kLong);
    bool c_fp32 = C.dtype() == at::kFloat;
    if (!c_fp32) TORCH_CHECK_DTYPE(C, kHalf);
    TORCH_CHECK_DIM(A, 3);
    TORCH_CHECK_DIM(B, 1);
    TORCH_CHECK_DIM(suh, 1);
    TORCH_CHECK_DIM(svh, 1);
    TORCH_CHECK_DIM(C, 3);

    TORCH_CHECK_SHAPES(A, 1, C, 1, 1);
    TORCH_CHECK_SHAPES(B, 0, suh, 0, 1);
    TORCH_CHECK_SHAPES(B, 0, svh, 0, 1);

    int bsz = A.size(1);
    int bszm_in = A.size(0);
    int bszm_out = C.size(0);

    const long* indices_ptr = (const long*) OPTPTR(indices);
    const half* weights_ptr = (const half*) OPTPTR(weights);

    // int num_B = 0;
    if (indices)
    {
        TORCH_CHECK_DIM(indices.value(), 2);
        TORCH_CHECK_SHAPES(indices.value(), 1, C, 0, 1);
        // num_B = indices.value().size(1);
    }
    //else TORCH_CHECK(false, "Must specify indices");

    if (weights)
    {
        TORCH_CHECK_DIM(weights.value(), 2);
    }

    int size_m = A.size(1);
    int size_k = A.size(2);
    int size_n = C.size(2);

    // Device properties
    int device;
    cudaGetDevice(&device);
    int num_sms = DevCtx::instance().get_num_sms(device);
    int total_sms = num_sms;
    int cc = DevCtx::instance().get_cc(device);
    int* locks = DevCtx::instance().get_locks(device);

    // Dispatch
    const half* A_ptr = (const half*) A.data_ptr();
    const uintptr_t* B_ptr_ptr = (const uintptr_t*) B.data_ptr();
    void* C_ptr = (void*) C.data_ptr();
    const half* A_had_ptr = (const half*) A_had.data_ptr();
    const uintptr_t* suh_ptr_ptr = (const uintptr_t*) suh.data_ptr();
    const uintptr_t* svh_ptr_ptr = (const uintptr_t*) svh.data_ptr();

    // Select kernel
    TORCH_CHECK(!(mcg_mult && mul1_mult), "Specified both mcg_mult and mul1_mult")
    int cb = 0;
    uint32_t mult = 0;
    if (mcg_mult) { cb = 1; mult = mcg_mult; }
    if (mul1_mult) { cb = 2; mult = mul1_mult; }

    int selected_shape;
    int block_dim;
    fp_exl3_mgemm_kernel kernel = select_exl3_mgemm_kernel
    (
        cc, size_m, size_k, size_n, K, c_fp32,
        force_shape_idx, &block_dim, &selected_shape,
        &num_sms, cb
    );
    if (!kernel) return 0;

    // Launch bigger grid if possible
    int concurrency = MIN(total_sms / num_sms, bszm_out);
    dim3 block_grid(num_sms, 1, concurrency);

    // Launch
    if (kernel_attr_set[device].find((void*)kernel) == kernel_attr_set[device].end())
    {
        cudaFuncSetAttribute(kernel, cudaFuncAttributeMaxDynamicSharedMemorySize, SMEM_MAX);
        kernel_attr_set[device].insert((void*)kernel);
    }
    void* kernelArgs[] =
    {
        (void*)& A_ptr,
        (void*)& B_ptr_ptr,
        (void*)& C_ptr,
        (void*)& size_m,
        (void*)& size_k,
        (void*)& size_n,
        (void*)& locks,
        (void*)& suh_ptr_ptr,
        (void*)& A_had_ptr,
        (void*)& svh_ptr_ptr,
        (void*)& indices_ptr,
        (void*)& weights_ptr,
        (void*)& bszm_in,
        (void*)& bszm_out,
        (void*)& mult
    };

    cudaLaunchCooperativeKernel
    (
        (void*)kernel,
        block_grid,
        block_dim,
        kernelArgs,
        SMEM_MAX,
        stream
    );
    cuda_check(cudaPeekAtLastError());
    return selected_shape;
}
</content>

<content full_path="exllamav3/exllamav3_ext/quant/exl3_gemm_kernel.cuh">
#pragma once

#include "exl3_kernel_map.cuh"
#include "hadamard_inner.cuh"
#include "exl3_gemm_inner.cuh"

template<EXL3_GEMM_T_ARGS>
__global__ __launch_bounds__(EXL3_GEMM_BASE_THREADS * TILESIZE_K / 16)
void exl3_gemm_kernel(EXL3_GEMM_ARGS)
{
    if (suh)
    {
        int total_warps = size_m * size_k / 128;
        int warps_grid = gridDim.x * blockDim.x / 32;
        int this_warp = threadIdx.x / 32 + blockDim.x / 32 * blockIdx.x;

        for(; this_warp < total_warps; this_warp += warps_grid)
            had_hf_r_128_inner
            (
                A + this_warp * 128,
                A_had + this_warp * 128,
                suh + (this_warp * 128) % size_k,
                nullptr,
                0.088388347648f  // 1/sqrt(128)
            );

        cg::this_grid().sync();
        A = A_had;
    }

    int size_m_ = size_m;
    const half* A_ = A;
    void* C_ = C;

    while (size_m_ > 0)
    {
        exl3_gemm_kernel_inner
        <bits, c_fp32, cb, TILESIZE_M, TILESIZE_K, TILESIZE_N, SH_STAGES, FRAG_STAGES>
        (A_, B, C_, size_m_, size_k, size_n, locks, mult);

        A_ += 16 * size_k;
        if constexpr (c_fp32) C_ = (void*) (((float*) C_) + 16 * size_n);
        else                  C_ = (void*) (((half*) C_) + 16 * size_n);
        size_m_ -= 16;
        cg::this_grid().sync();
    }

    if (svh)
    {
        int total_warps = size_m * size_n / 128;
        int warps_grid = gridDim.x * blockDim.x / 32;
        int this_warp = threadIdx.x / 32 + blockDim.x / 32 * blockIdx.x;

        for(; this_warp < total_warps; this_warp += warps_grid)
        {
            if constexpr (c_fp32)
                had_ff_r_128_inner
                (
                    ((const float*) C) + this_warp * 128,
                    ((float*) C) + this_warp * 128,
                    nullptr,
                    svh + (this_warp * 128) % size_n,
                    0.088388347648f  // 1/sqrt(128)
                );
            else
                had_hf_r_128_inner
                (
                    ((const half*) C) + this_warp * 128,
                    ((half*) C) + this_warp * 128,
                    nullptr,
                    svh + (this_warp * 128) % size_n,
                    0.088388347648f  // 1/sqrt(128)
                );
        }
    }
}


template<EXL3_GEMM_T_ARGS>
__global__ __launch_bounds__(EXL3_GEMM_BASE_THREADS * TILESIZE_K / 16)
void exl3_mgemm_kernel(EXL3_MGEMM_ARGS)
{
    int bszm = MAX(bszm_in, bszm_out);
    for (int i = 0; i < bszm; i += gridDim.z)
    {
        int j = i + blockIdx.z;
        int mat_index = -1;
        const uint16_t* B = nullptr;
        if (j >= bszm) j = -1;
        else
        {
            mat_index = B_indices ? (int) B_indices[j] : j;
            B = B_list[mat_index];
        }

        // Had and input scales

        if (B)
        {
            int total_warps = size_m * size_k / 128;
            int warps_grid = gridDim.x * blockDim.x / 32;
            int this_warp = threadIdx.x / 32 + blockDim.x / 32 * blockIdx.x;

            const half* suh = suh_list[mat_index];
            const half* A_ = bszm_in == 1 ? A : A + j * size_m * size_k;
            half* A_had_ = A_had + j * size_m * size_k;

            for(; this_warp < total_warps; this_warp += warps_grid)
                had_hf_r_128_inner
                (
                    A_ + this_warp * 128,
                    A_had_ + this_warp * 128,
                    suh + (this_warp * 128) % size_k,
                    nullptr,
                    0.088388347648f  // 1/sqrt(128)
                );
        }
        cg::this_grid().sync();

        // Matmul

        int size_m_ = size_m;
        half* A_ = A_had + j * size_m * size_k;
        void* C_;
        if constexpr (c_fp32) C_ = (void*) (((float*) C) + j * size_m * size_n);
        else                  C_ = (void*) (((half*) C) + j * size_m * size_n);

        while (size_m_ > 0)
        {
            if (B)
            {
                int lock_offs = blockIdx.z * size_n / 128;

                exl3_gemm_kernel_inner
                <bits, c_fp32, cb, TILESIZE_M, TILESIZE_K, TILESIZE_N, SH_STAGES, FRAG_STAGES>
                (A_, B, C_, size_m_, size_k, size_n, locks + lock_offs, mult);
             }

            A_ += 16 * size_k;
            if constexpr (c_fp32) C_ = (void*) (((float*) C_) + 16 * size_n);
            else                  C_ = (void*) (((half*) C_) + 16 * size_n);
            size_m_ -= 16;
            cg::this_grid().sync();
        }

        // Had and output scales

        if (B)
        {
            int total_warps = size_m * size_n / 128;
            int warps_grid = gridDim.x * blockDim.x / 32;
            int this_warp = threadIdx.x / 32 + blockDim.x / 32 * blockIdx.x;

            const half* svh = svh_list[mat_index];
            float scale = 0.088388347648f;  // 1/sqrt(128)
            if (B_weights) scale *= __half2float(B_weights[j]);

            if constexpr (c_fp32) C_ = (void*) (((float*) C) + j * size_m * size_n);
            else                  C_ = (void*) (((half*) C) + j * size_m * size_n);

            for(; this_warp < total_warps; this_warp += warps_grid)
            {
                if constexpr (c_fp32)
                    had_ff_r_128_inner
                    (
                        ((const float*) C_) + this_warp * 128,
                        ((float*) C_) + this_warp * 128,
                        nullptr,
                        svh + (this_warp * 128) % size_n,
                        scale
                    );
                else
                    had_hf_r_128_inner
                    (
                        ((const half*) C_) + this_warp * 128,
                        ((half*) C_) + this_warp * 128,
                        nullptr,
                        svh + (this_warp * 128) % size_n,
                        scale
                    );
            }
        }

        cg::this_grid().sync();
    }

    // Final reduction
    if (B_weights && blockIdx.z == 0)
    {
        int total_warps = size_m * size_n / 32;
        int warps_grid = gridDim.x * blockDim.x / 32;
        int this_warp = threadIdx.x / 32 + blockDim.x / 32 * blockIdx.x;
        int this_lane = threadIdx.x % 32;

        for(; this_warp < total_warps; this_warp += warps_grid)
        {
            if constexpr (c_fp32)
            {
                float* C__ = ((float*) C) + this_warp * 32 + this_lane;
                float* C___ = C__;
                float sum = *C___;
                for (int j = 1; j < bszm; ++j)
                {
                    C___ += size_m * size_n;
                    sum += *C___;
                }
                *C__ = sum;
            }
            else
            {
                half* C__ = ((half*) C) + this_warp * 32 + this_lane;
                half* C___ = C__;
                half sum = *C___;
                for (int j = 1; j < bszm; ++j)
                {
                    C___ += size_m * size_n;
                    sum = __hadd(sum, *C___);
                }
                *C__ = sum;
            }
        }
    }
}
</content>

<content full_path="exllamav3/exllamav3_ext/quant/reconstruct.cuh">
#pragma once

#include <ATen/Tensor.h>

void reconstruct
(
    at::Tensor unpacked,
    at::Tensor packed,
    int K,
    uint32_t mcg_mult,
    uint32_t mul1_mult
);

</content>

<content full_path="exllamav3/exllamav3_ext/quant/exl3_devctx.cu">
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include <cooperative_groups.h>
namespace cg = cooperative_groups;
#include "exl3_devctx.cuh"
#include "../util.h"
#include "../util.cuh"

//DevCtx::DevCtc()
//{
//    int num_sms[MAX_DEVICES] = {};
//    int cc[MAX_DEVICES] = {};
//    void* locks[MAX_DEVICES] = {};
//    std::mutex mtx;
//}

DevCtx& DevCtx::instance()
{
    static DevCtx ctx;
    return ctx;
}

int DevCtx::get_num_sms(int device)
{
    std::lock_guard<std::mutex> lock(mtx);
    if (!num_sms[device])
        cuda_check(cudaDeviceGetAttribute(&num_sms[device], cudaDevAttrMultiProcessorCount, device));
    return num_sms[device];
}

int DevCtx::get_cc(int device)
{
    std::lock_guard<std::mutex> lock(mtx);
    if (!cc[device])
    {
        cudaDeviceProp prop;
        cuda_check(cudaGetDeviceProperties(&prop, device));
        if (prop.major >= 10) cc[device] = CC_BLACKWELL;
        else if (prop.major >= 9) cc[device] = CC_HOPPER;
        else if (prop.major >= 8 && prop.minor >= 9) cc[device] = CC_ADA;
        else if (prop.major >= 8 && prop.minor >= 6) cc[device] = CC_AMPERE;
        else cc[device] = CC_OLD;
    }
    return cc[device];
}

int* DevCtx::get_locks(int device)
{
    std::lock_guard<std::mutex> lock(mtx);
    if (!locks[device])
    {
        cudaSetDevice(device);
        cudaMalloc(&locks[device], MAX_TILES_C * sizeof(int));
        cudaMemset(locks[device], 0, MAX_TILES_C * sizeof(int));
    }
    return (int*) locks[device];
}
</content>

<content full_path="exllamav3/exllamav3_ext/quant/exl3_kernel_map.cuh">
#pragma once

int select_gemm_shape(int cc, int size_m, int size_k, int size_n, int bits);
int exl3_gemm_num_kernel_shapes();
bool exl3_gemm_shape_compat(int shape_idx, int size_m, int size_k, int size_n, int bits);

#define EXL3_GEMM_T_ARGS \
    int bits, \
    bool c_fp32, \
    int cb, \
    int TILESIZE_M, \
    int TILESIZE_K, \
    int TILESIZE_N, \
    int SH_STAGES, \
    int FRAG_STAGES

#define EXL3_GEMM_ARGS \
    const half* __restrict__  A, \
    const uint16_t* __restrict__ B, \
    void* __restrict__ C, \
    int size_m, \
    int size_k, \
    int size_n, \
    int* __restrict__ locks, \
    const half* __restrict__ suh, \
    half* __restrict__ A_had, \
    const half* __restrict__ svh, \
    uint32_t mult

#define EXL3_MGEMM_ARGS \
    const half* __restrict__  A, \
    const uint16_t** __restrict__ B_list, \
    void* __restrict__ C, \
    int size_m, \
    int size_k, \
    int size_n, \
    int* __restrict__ locks, \
    const half** __restrict__ suh_list, \
    half* __restrict__ A_had, \
    const half** __restrict__ svh_list, \
    const uint64_t* B_indices, \
    const half* B_weights, \
    int bszm_in, \
    int bszm_out, \
    uint32_t mult

typedef void (*fp_exl3_gemm_kernel) (EXL3_GEMM_ARGS);
typedef void (*fp_exl3_mgemm_kernel) (EXL3_MGEMM_ARGS);

#define EXL3_GEMM_SHAPE_1     16,     16,    128,     6,     5
#define EXL3_GEMM_SHAPE_2     16,     32,    128,     4,     3
#define EXL3_GEMM_SHAPE_3     16,     32,    256,     4,     3
#define EXL3_GEMM_SHAPE_4     16,     16,    512,     4,     3

#define EXL3_GEMM_TILESIZE_K  0, 16, 32, 32, 16
#define EXL3_GEMM_TILESIZE_N  0, 128, 128, 256, 512
#define EXL3_GEMM_BLOCKDIM  0, 256, 512, 512, 256

#define EXL3_GEMM_NUM_SHAPES 4

// Shape 1 not currently used anywhere
#define EXL3_GEMM_KERNEL_INSTANCES(_bits, _c_fp32, cb) \
    nullptr, \
    exl3_gemm_kernel<_bits, _c_fp32, cb, EXL3_GEMM_SHAPE_1>, \
    exl3_gemm_kernel<_bits, _c_fp32, cb, EXL3_GEMM_SHAPE_2>, \
    exl3_gemm_kernel<_bits, _c_fp32, cb, EXL3_GEMM_SHAPE_3>, \
    exl3_gemm_kernel<_bits, _c_fp32, cb, EXL3_GEMM_SHAPE_4>

#define EXL3_MGEMM_KERNEL_INSTANCES(_bits, _c_fp32, cb) \
    nullptr, \
    exl3_mgemm_kernel<_bits, _c_fp32, cb, EXL3_GEMM_SHAPE_1>, \
    exl3_mgemm_kernel<_bits, _c_fp32, cb, EXL3_GEMM_SHAPE_2>, \
    exl3_mgemm_kernel<_bits, _c_fp32, cb, EXL3_GEMM_SHAPE_3>, \
    exl3_mgemm_kernel<_bits, _c_fp32, cb, EXL3_GEMM_SHAPE_4>

#define EXL3_GEMM_BASE_THREADS 256

#define ALL_EXL3_KERNEL_EXTERNS(K) \
    extern fp_exl3_gemm_kernel tfp_exl3_gemm_kernel_fp32_b##K[]; \
    extern fp_exl3_gemm_kernel tfp_exl3_gemm_kernel_fp16_b##K[]; \
    extern fp_exl3_mgemm_kernel tfp_exl3_mgemm_kernel_fp32_b##K[]; \
    extern fp_exl3_mgemm_kernel tfp_exl3_mgemm_kernel_fp16_b##K[]; \

#define ALL_EXL3_KERNEL_INSTANCES(K) \
    fp_exl3_gemm_kernel tfp_exl3_gemm_kernel_fp32_b##K[] = { \
        EXL3_GEMM_KERNEL_INSTANCES(K, true, 0), \
        EXL3_GEMM_KERNEL_INSTANCES(K, true, 1), \
        EXL3_GEMM_KERNEL_INSTANCES(K, true, 2) \
    }; \
    \
    fp_exl3_gemm_kernel tfp_exl3_gemm_kernel_fp16_b##K[] = { \
        EXL3_GEMM_KERNEL_INSTANCES(K, false, 0), \
        EXL3_GEMM_KERNEL_INSTANCES(K, false, 1), \
        EXL3_GEMM_KERNEL_INSTANCES(K, false, 2) \
    }; \
    \
    fp_exl3_mgemm_kernel tfp_exl3_mgemm_kernel_fp32_b##K[] = { \
        EXL3_MGEMM_KERNEL_INSTANCES(K, true, 0), \
        EXL3_MGEMM_KERNEL_INSTANCES(K, true, 1), \
        EXL3_MGEMM_KERNEL_INSTANCES(K, true, 2) \
    }; \
    \
    fp_exl3_mgemm_kernel tfp_exl3_mgemm_kernel_fp16_b##K[] = { \
        EXL3_MGEMM_KERNEL_INSTANCES(K, false, 0), \
        EXL3_MGEMM_KERNEL_INSTANCES(K, false, 1), \
        EXL3_MGEMM_KERNEL_INSTANCES(K, false, 2) \
    };

fp_exl3_gemm_kernel select_exl3_gemm_kernel
(
    int cc,
    int size_m,
    int size_k,
    int size_n,
    int bits,
    bool c_fp32,
    int force_shape_idx,
    int* out_block_dim,
    int* out_shape_idx,
    int* out_num_sms,
    int cb
);

fp_exl3_mgemm_kernel select_exl3_mgemm_kernel
(
    int cc,
    int size_m,
    int size_k,
    int size_n,
    int bits,
    bool c_fp32,
    int force_shape_idx,
    int* out_block_dim,
    int* out_shape_idx,
    int* out_num_sms,
    int cb
);
</content>

<content full_path="exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_6.cuh">
#pragma once

ALL_EXL3_KERNEL_EXTERNS(6)
</content>

<content full_path="exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_7.cuh">
#pragma once

ALL_EXL3_KERNEL_EXTERNS(7)
</content>

<content full_path="exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_5.cuh">
#pragma once

ALL_EXL3_KERNEL_EXTERNS(5)
</content>

<content full_path="exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_4.cuh">
#pragma once

ALL_EXL3_KERNEL_EXTERNS(4)
</content>

<content full_path="exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_8.cu">
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include <cooperative_groups.h>
namespace cg = cooperative_groups;
#include "../../util.h"
#include "../../util.cuh"
#include "../../ptx.cuh"
#include "../exl3_gemm_kernel.cuh"
#include "exl3_comp_unit_8.cuh"

ALL_EXL3_KERNEL_INSTANCES(8)

</content>

<content full_path="exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_1.cuh">
#pragma once

ALL_EXL3_KERNEL_EXTERNS(1)
</content>

<content full_path="exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_3.cuh">
#pragma once

ALL_EXL3_KERNEL_EXTERNS(3)
</content>

<content full_path="exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_2.cuh">
#pragma once

ALL_EXL3_KERNEL_EXTERNS(2)
</content>

<content full_path="exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_6.cu">
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include <cooperative_groups.h>
namespace cg = cooperative_groups;
#include "../../util.h"
#include "../../util.cuh"
#include "../../ptx.cuh"
#include "../exl3_gemm_kernel.cuh"
#include "exl3_comp_unit_6.cuh"

ALL_EXL3_KERNEL_INSTANCES(6)

</content>

<content full_path="exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_2.cu">
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include <cooperative_groups.h>
namespace cg = cooperative_groups;
#include "../../util.h"
#include "../../util.cuh"
#include "../../ptx.cuh"
#include "../exl3_gemm_kernel.cuh"
#include "exl3_comp_unit_2.cuh"

ALL_EXL3_KERNEL_INSTANCES(2)

</content>

<content full_path="exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_3.cu">
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include <cooperative_groups.h>
namespace cg = cooperative_groups;
#include "../../util.h"
#include "../../util.cuh"
#include "../../ptx.cuh"
#include "../exl3_gemm_kernel.cuh"
#include "exl3_comp_unit_3.cuh"

ALL_EXL3_KERNEL_INSTANCES(3)

</content>

<content full_path="exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_7.cu">
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include <cooperative_groups.h>
namespace cg = cooperative_groups;
#include "../../util.h"
#include "../../util.cuh"
#include "../../ptx.cuh"
#include "../exl3_gemm_kernel.cuh"
#include "exl3_comp_unit_7.cuh"

ALL_EXL3_KERNEL_INSTANCES(7)

</content>

<content full_path="exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_4.cu">
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include <cooperative_groups.h>
namespace cg = cooperative_groups;
#include "../../util.h"
#include "../../util.cuh"
#include "../../ptx.cuh"
#include "../exl3_gemm_kernel.cuh"
#include "exl3_comp_unit_4.cuh"

ALL_EXL3_KERNEL_INSTANCES(4)

</content>

<content full_path="exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_5.cu">
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include <cooperative_groups.h>
namespace cg = cooperative_groups;
#include "../../util.h"
#include "../../util.cuh"
#include "../../ptx.cuh"
#include "../exl3_gemm_kernel.cuh"
#include "exl3_comp_unit_5.cuh"

ALL_EXL3_KERNEL_INSTANCES(5)

</content>

<content full_path="exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_1.cu">
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include <cooperative_groups.h>
namespace cg = cooperative_groups;
#include "../../util.h"
#include "../../util.cuh"
#include "../../ptx.cuh"
#include "../exl3_gemm_kernel.cuh"
#include "exl3_comp_unit_1.cuh"

ALL_EXL3_KERNEL_INSTANCES(1)
</content>

<content full_path="exllamav3/exllamav3_ext/quant/comp_units/exl3_comp_unit_8.cuh">
#pragma once

ALL_EXL3_KERNEL_EXTERNS(8)
</content>

<content full_path="exllamav3/exllamav3_ext/cache/q_cache.cu">
#include "q_cache.cuh"
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include "../util.h"
#include "../util.cuh"
#include <limits>
#include "../quant/codebook.cuh"
#include "q_cache_kernels.cuh"

/*
Quantize contiguous tensor

in: float16, shape (..., dim)
out: int32, shape (..., dim / 32 * bitrate)
out_scales: float16, shape (..., dim / 32)
*/

void quant_cache_cont
(
    const at::Tensor& in,
    const at::Tensor& out,
    const at::Tensor& out_scales
)
{
    const at::cuda::OptionalCUDAGuard device_guard(in.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();
    TORCH_CHECK_DTYPE(in, kHalf);
    TORCH_CHECK_DTYPE(out, kInt);
    TORCH_CHECK_DTYPE(out_scales, kHalf);

    int bsz = in.numel() / 32;
    int head_dim = in.size(-1);
    int head_blocks = head_dim / 32;
    TORCH_CHECK(head_dim == 32 * head_blocks, "head_dim must be a multiple of 32");
    int bits = out.size(-1) / head_blocks;
    TORCH_CHECK(out.numel() == bsz * bits, "out is wrong size");
    TORCH_CHECK(out_scales.numel() == bsz, "out_scales is wrong size");

    TORCH_CHECK(2 <= bits && bits <= 8, "no kernel for K/V bitrate");

    quant_cache_cont_kernel_instances[bits - 2]<<<bsz, 32, 0, stream>>>
    (
        (const half*) in.data_ptr(),
        (uint32_t*) out.data_ptr(),
        (half*) out_scales.data_ptr()
    );
    cuda_check(cudaPeekAtLastError());
}

/*
Dequantize contiguous tensor

in: int32, shape (..., dim / 32 * bitrate)
in_scales: float16, shape (..., dim / 32)
out: float16, shape (..., dim)
*/

void dequant_cache_cont
(
    const at::Tensor& in,
    const at::Tensor& in_scales,
    const at::Tensor& out
)
{
    const at::cuda::OptionalCUDAGuard device_guard(in.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();
    TORCH_CHECK_DTYPE(in, kInt);
    TORCH_CHECK_DTYPE(in_scales, kHalf);
    TORCH_CHECK_DTYPE(out, kHalf);

    int bsz = out.numel() / 32;
    int head_dim = out.size(-1);
    int head_blocks = head_dim / 32;
    TORCH_CHECK(head_dim == 32 * head_blocks, "head_dim must be a multiple of 32");
    int bits = in.size(-1) / head_blocks;
    TORCH_CHECK(in.numel() == bsz * bits, "in is wrong size");
    TORCH_CHECK(in_scales.numel() == bsz, "in_scales is wrong size");

    TORCH_CHECK(2 <= bits && bits <= 8, "no kernel for K/V bitrate");

    dequant_cache_cont_kernel_instances[bits - 2]<<<bsz, 32, 0, stream>>>
    (
        (const uint32_t*) in.data_ptr(),
        (const half*) in_scales.data_ptr(),
        (half*) out.data_ptr()
    );
    cuda_check(cudaPeekAtLastError());
}

/*
Quantize paged tensor

k_in, v_in: float16, shape (1, cache_size, dim)
k_out, v_out: int32, shape (1, cache_size, dim / 32 * bitrate)
k_out_scales, v_out_scales: float16, shape (1, cache_size, dim / 32)
cache_seqlens: int32, length of each sequence in batch, k_out and v_out are updated _from_ this point
block_table: int32, shape (bsz, blocks_per_seq)
page_size: 256
seq_len: number of positions (size: dim) to update from end of each sequence
*/

void quant_cache_paged
(
    const at::Tensor& k_in,
    const at::Tensor& k_out,
    const at::Tensor& k_out_scales,
    const at::Tensor& v_in,
    const at::Tensor& v_out,
    const at::Tensor& v_out_scales,
    const at::Tensor& cache_seqlens,
    const at::Tensor& block_table,
    int page_size,
    int seq_len
)
{
    const at::cuda::OptionalCUDAGuard device_guard(k_in.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    TORCH_CHECK_DTYPE(k_in, kHalf);
    TORCH_CHECK_DTYPE(k_out, kInt);
    TORCH_CHECK_DTYPE(k_out_scales, kHalf);
    TORCH_CHECK_DTYPE(v_in, kHalf);
    TORCH_CHECK_DTYPE(v_out, kInt);
    TORCH_CHECK_DTYPE(v_out_scales, kHalf);
    TORCH_CHECK_SHAPES_FULL(k_in, v_in);
    TORCH_CHECK_SHAPES_FULL(k_out_scales, v_out_scales);

    int dim;
    if (k_in.dim() == 4)
        dim = k_in.size(2) * k_in.size(3);
    else if (k_in.dim() == 3)
        dim = k_in.size(2);
    else
        TORCH_CHECK(false, "paged cache must be 3D or 4D")

    int warps_per_token = dim / 32;
    TORCH_CHECK(dim == 32 * warps_per_token, "dim must be a multiple of 32");
    int tb_per_token = CEIL_DIVIDE(warps_per_token, MAX_WARPS);  // Threadblocks per token position
    int tb_usage = CEIL_DIVIDE(warps_per_token, tb_per_token);   // Number of warps to use per threadblock

    TORCH_CHECK(k_out.dim() == 3 && v_out.dim() == 3, "paged q.cache must have shape (num_pages, page_size, dim // 32 * bitrate)")
    int k_bits = k_out.size(2) / warps_per_token;
    int v_bits = v_out.size(2) / warps_per_token;

    int bsz = block_table.size(0);
    int blocks_per_seq = block_table.size(1);

    dim3 blocks(tb_per_token, seq_len, bsz);
    dim3 threads(32 * tb_usage);

    TORCH_CHECK(2 <= k_bits && k_bits <= 8 && 2 <= v_bits && v_bits <= 8, "no kernel for K/V bitrate");

    quant_cache_paged_kernel_instances[k_bits - 2][v_bits - 2]<<<blocks, threads, 0, stream>>>
    (
        (const half*) k_in.data_ptr(),
        (uint32_t*) k_out.data_ptr(),
        (half*) k_out_scales.data_ptr(),
        (const half*) v_in.data_ptr(),
        (uint32_t*) v_out.data_ptr(),
        (half*) v_out_scales.data_ptr(),
        (const uint32_t*) cache_seqlens.data_ptr(),
        (const uint32_t*) block_table.data_ptr(),
        page_size,
        blocks_per_seq,
        dim
    );
    cuda_check(cudaPeekAtLastError());
}

/*
Dequantize paged tensor

k_in, v_in: int32, shape (1, cache_size, dim / 32 * bitrate)
k_in_scales, v_in_scales: float16, shape (1, cache_size, dim / 32)
k_out, v_out: float16, shape (1, cache_size, dim)
cache_seqlens: int32, length of each sequence in batch, k_out and v_out are updated _up_to_ this point
block_table: int32, shape (bsz, blocks_per_seq)
page_size: 256
*/

void dequant_cache_paged
(
    const at::Tensor& k_in,
    const at::Tensor& k_in_scales,
    const at::Tensor& k_out,
    const at::Tensor& v_in,
    const at::Tensor& v_in_scales,
    const at::Tensor& v_out,
    const at::Tensor& cache_seqlens,
    const at::Tensor& block_table,
    int page_size
)
{
    const at::cuda::OptionalCUDAGuard device_guard(k_in.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    TORCH_CHECK_DTYPE(k_in, kInt);
    TORCH_CHECK_DTYPE(k_in_scales, kHalf);
    TORCH_CHECK_DTYPE(k_out, kHalf);
    TORCH_CHECK_DTYPE(v_in, kInt);
    TORCH_CHECK_DTYPE(v_in_scales, kHalf);
    TORCH_CHECK_DTYPE(v_out, kHalf);
    TORCH_CHECK_SHAPES_FULL(k_in_scales, v_in_scales);
    TORCH_CHECK_SHAPES_FULL(k_out, v_out);

    int dim;
    if (k_out.dim() == 4)
        dim = k_out.size(2) * k_out.size(3);
    else if (k_out.dim() == 3)
        dim = k_out.size(2);
    else
        TORCH_CHECK(false, "paged cache must be 3D or 4D")

    int warps_per_token = dim / 32;
    TORCH_CHECK(dim == 32 * warps_per_token, "dim must be a multiple of 32");

    int bsz = block_table.size(0);
    int pages_per_seq = block_table.size(1);
    int warps_per_seq = pages_per_seq * page_size * warps_per_token;

    int num_tb = CEIL_DIVIDE(32 * warps_per_seq, 1024);
    int num_threads = MIN(32 * warps_per_seq, 1024);
    dim3 blocks(num_tb, bsz);
    dim3 threads(num_threads);

    TORCH_CHECK(k_in.dim() == 3 && v_in.dim() == 3, "paged q.cache must have shape (num_pages, page_size, dim // 32 * bitrate)")
    int k_bits = k_in.size(2) / warps_per_token;
    int v_bits = v_in.size(2) / warps_per_token;

    TORCH_CHECK(2 <= k_bits && k_bits <= 8 && 2 <= v_bits && v_bits <= 8, "no kernel for K/V bitrate");

    dequant_cache_paged_kernel_instances[k_bits - 2][v_bits - 2]<<<blocks, threads, 0, stream>>>
    (
        (const uint32_t*) k_in.data_ptr(),
        (const half*) k_in_scales.data_ptr(),
        (half*) k_out.data_ptr(),
        (const uint32_t*) v_in.data_ptr(),
        (const half*) v_in_scales.data_ptr(),
        (half*) v_out.data_ptr(),
        (const uint32_t*) cache_seqlens.data_ptr(),
        (const uint32_t*) block_table.data_ptr(),
        page_size,
        pages_per_seq,
        warps_per_token
    );
    cuda_check(cudaPeekAtLastError());
}
</content>

<content full_path="exllamav3/exllamav3_ext/cache/q_cache_kernels.cuh">

__device__ inline float shuffle_had_fx32(float v, int lane_id)
{
    for (int i = 1; i < 32; i <<= 1)
    {
        float pv = __shfl_xor_sync(0xffffffff, v, i);
        uint32_t* vi = reinterpret_cast<uint32_t*>(&v);
        int32_t sfm = -static_cast<int16_t>(lane_id & i) >> 31;
        *vi ^= (sfm & 0x80000000);
        v = v + pv;
    }
    return v;
}

__device__ inline float shuffle_sum_fx32(float s)
{
    for (int i = 1; i < 32; i <<= 1)
        s += __shfl_xor_sync(0xffffffff, s, i);
    return s;
}

__device__ inline float shuffle_max_fx32(float s)
{
    for (int i = 1; i < 32; i <<= 1)
        s = fmaxf(s, __shfl_xor_sync(0xffffffff, s, i));
    return s;
}

template <int bits>
__device__ inline void quant_block
(
    const half* in,
    uint32_t* out,
    half* out_scales
)
{
    int t = threadIdx.x % 32;

    // Load, rotate and scale 32 values
    float v = __half2float(in[t]);
    v = shuffle_had_fx32(v, t);
    v *= 0.17678f;  // 0.17678 = 1 / sqrt(32)
    float s = shuffle_max_fx32(fabsf(v) + 1e-10);
    half sh = __float2half_rn(s);
    s = __half2float(sh);
    v /= s;

    // Quantize and clamp
    int m = (1 << (bits - 1));
    v *= __int2float_rn(m);
    int q = lrintf(v) + m;
    q = max(min((1 << bits) - 1, q), 0);

    // Pack bits
    register uint32_t bitplanes[bits];
    for (int i = 0, mask = 1; mask <= m; ++i, mask <<= 1)
        bitplanes[i] = __ballot_sync(0xffffffff, q & mask);

    // Write output
    if (t < bits)
        out[t] = bitplanes[t];
    if (t == bits)
        *out_scales = sh;
}

#define MAX_WARPS 32

template <int bits>
__device__ inline void dequant_block
(
    const uint32_t* in,
    const half* in_scales,
    half* out
)
{
    int t = threadIdx.x % 32;
    int warp_id = threadIdx.x / 32;

    // Load scale and bitplanes
    float s = __half2float(*in_scales);
    __shared__ uint32_t bitplanes[MAX_WARPS][bits];
    if (t < bits)
        bitplanes[warp_id][t] = in[t];
    __syncthreads();

    // Unpack bits
    int m = (1 << (bits - 1));
    uint32_t mask = 1 << t;
    int q = 0;
    for (int i = 0; i < bits; ++i)
        q |= ((bitplanes[warp_id][i] & mask) >> t) << i;

    // Dequantize
    float v = __int2float_rn(q - m);
    v /= __int2float_rn(m);

    // Scale and rotate
    v *= s;
    v = shuffle_had_fx32(v, t);
    v *= 0.17678f;  // 0.17678 = 1 / sqrt(32)

    // Store
    out[t] = __float2half(v);
}

template <int bits>
__global__ __launch_bounds__(1024)
void quant_cache_cont_kernel
(
    const half* __restrict__ in,
    uint32_t* __restrict__ out,
    half* __restrict__ out_scales
)
{
    in += 32 * blockIdx.x;
    out += bits * blockIdx.x;
    out_scales += blockIdx.x;
    quant_block<bits>(in, out, out_scales);
}

#define __(i) quant_cache_cont_kernel<i>
constexpr auto quant_cache_cont_kernel_instances = std::array
{
    __(2), __(3), __(4), __(5), __(6), __(7), __(8)
};
#undef __


template <int bits>
__global__ __launch_bounds__(32)
void dequant_cache_cont_kernel
(
    const uint32_t* __restrict__ in,
    const half* __restrict__ in_scales,
    half* __restrict__ out
)
{
    in += bits * blockIdx.x;
    in_scales += blockIdx.x;
    out += 32 * blockIdx.x;
    dequant_block<bits>(in, in_scales, out);
}

#define __(i) dequant_cache_cont_kernel<i>
constexpr auto dequant_cache_cont_kernel_instances = std::array
{
    __(2), __(3), __(4), __(5), __(6), __(7), __(8)
};
#undef __


template <int k_bits, int v_bits>
__global__ __launch_bounds__(1024)
void quant_cache_paged_kernel
(
    const half* __restrict__ k_in,
    uint32_t* __restrict__ k_out,
    half* __restrict__ k_out_scales,
    const half* __restrict__ v_in,
    uint32_t* __restrict__ v_out,
    half* __restrict__ v_out_scales,
    const uint32_t* __restrict__ cache_seqlens,
    const uint32_t* __restrict__ block_table,
    int page_size,
    int blocks_per_seq,
    int token_dim
)
{
    int batch_idx = blockIdx.z;
    int token_idx = blockIdx.y + cache_seqlens[batch_idx];
    int page_idx = token_idx / page_size;
    int token_pos = block_table[blocks_per_seq * batch_idx + page_idx] * page_size + (token_idx % page_size);
    int sub_pos = (token_pos * token_dim + blockDim.x * blockIdx.x + threadIdx.x) / 32;

    quant_block<k_bits>(k_in + sub_pos * 32, k_out + sub_pos * k_bits, k_out_scales + sub_pos);
    quant_block<v_bits>(v_in + sub_pos * 32, v_out + sub_pos * v_bits, v_out_scales + sub_pos);
}

#define __(i, j) quant_cache_paged_kernel<i, j>
constexpr auto quant_cache_paged_kernel_instances = std::array
{
    std::array{ __(2, 2), __(2, 3), __(2, 4), __(2, 5), __(2, 6), __(2, 7), __(2, 8) },
    std::array{ __(3, 2), __(3, 3), __(3, 4), __(3, 5), __(3, 6), __(3, 7), __(3, 8) },
    std::array{ __(4, 2), __(4, 3), __(4, 4), __(4, 5), __(4, 6), __(4, 7), __(4, 8) },
    std::array{ __(5, 2), __(5, 3), __(5, 4), __(5, 5), __(5, 6), __(5, 7), __(5, 8) },
    std::array{ __(6, 2), __(6, 3), __(6, 4), __(6, 5), __(6, 6), __(6, 7), __(6, 8) },
    std::array{ __(7, 2), __(7, 3), __(7, 4), __(7, 5), __(7, 6), __(7, 7), __(7, 8) },
    std::array{ __(8, 2), __(8, 3), __(8, 4), __(8, 5), __(8, 6), __(8, 7), __(8, 8) }
};
#undef __


template <int k_bits, int v_bits>
__global__ __launch_bounds__(1024)
void dequant_cache_paged_kernel
(
    const uint32_t* __restrict__ k_in,
    const half* __restrict__ k_in_scales,
    half* __restrict__ k_out,
    const uint32_t* __restrict__ v_in,
    const half* __restrict__ v_in_scales,
    half* __restrict__ v_out,
    const uint32_t* __restrict__ cache_seqlens,
    const uint32_t* __restrict__ block_table,
    int page_size,
    int pages_per_seq,
    int warps_per_token
)
{
    int batch_idx = blockIdx.y;
    int t_warp_id = (blockDim.x * blockIdx.x + threadIdx.x) / 32;
    int token_idx = t_warp_id / warps_per_token;
    int max_token_idx = cache_seqlens[batch_idx];
    if (token_idx >= max_token_idx) return;
    int page_idx = token_idx / page_size;
    int page_sub = t_warp_id % (warps_per_token * page_size);
    int mapped_page = block_table[batch_idx * pages_per_seq + page_idx];
    int addr = mapped_page * page_size * warps_per_token + page_sub;

    dequant_block<k_bits>(k_in + addr * k_bits, k_in_scales + addr, k_out + addr * 32);
    dequant_block<v_bits>(v_in + addr * v_bits, v_in_scales + addr, v_out + addr * 32);
}

#define __(i, j) dequant_cache_paged_kernel<i, j>
constexpr auto dequant_cache_paged_kernel_instances = std::array
{
    std::array{ __(2, 2), __(2, 3), __(2, 4), __(2, 5), __(2, 6), __(2, 7), __(2, 8) },
    std::array{ __(3, 2), __(3, 3), __(3, 4), __(3, 5), __(3, 6), __(3, 7), __(3, 8) },
    std::array{ __(4, 2), __(4, 3), __(4, 4), __(4, 5), __(4, 6), __(4, 7), __(4, 8) },
    std::array{ __(5, 2), __(5, 3), __(5, 4), __(5, 5), __(5, 6), __(5, 7), __(5, 8) },
    std::array{ __(6, 2), __(6, 3), __(6, 4), __(6, 5), __(6, 6), __(6, 7), __(6, 8) },
    std::array{ __(7, 2), __(7, 3), __(7, 4), __(7, 5), __(7, 6), __(7, 7), __(7, 8) },
    std::array{ __(8, 2), __(8, 3), __(8, 4), __(8, 5), __(8, 6), __(8, 7), __(8, 8) }
};
#undef __

</content>

<content full_path="exllamav3/exllamav3_ext/cache/q_cache.cuh">
#pragma once

#include <ATen/Tensor.h>

void quant_cache_cont
(
    const at::Tensor& in,
    const at::Tensor& out,
    const at::Tensor& out_scales
);

void dequant_cache_cont
(
    const at::Tensor& in,
    const at::Tensor& in_scales,
    const at::Tensor& out
);

void quant_cache_paged
(
    const at::Tensor& k_in,
    const at::Tensor& k_out,
    const at::Tensor& k_out_scales,
    const at::Tensor& v_in,
    const at::Tensor& v_out,
    const at::Tensor& v_out_scales,
    const at::Tensor& cache_seqlens,
    const at::Tensor& block_table,
    int page_size,
    int seq_len
);

void dequant_cache_paged
(
    const at::Tensor& k_in,
    const at::Tensor& k_in_scales,
    const at::Tensor& k_out,
    const at::Tensor& v_in,
    const at::Tensor& v_in_scales,
    const at::Tensor& v_out,
    const at::Tensor& cache_seqlens,
    const at::Tensor& block_table,
    int page_size
);
</content>

<content full_path="exllamav3/exllamav3_ext/libtorch/blocksparse_mlp.h">
#pragma once

#include <ATen/Tensor.h>
#include <vector>
#include <pybind11/pybind11.h>

namespace py = pybind11;

std::tuple<at::Tensor, at::Tensor> blocksparse_mlp_routing(
    int bsz,
    const py::object& cfg,
    const at::Tensor& y,
    const py::dict& params
);
</content>

<content full_path="exllamav3/exllamav3_ext/libtorch/blocksparse_mlp.cpp">
#include <Python.h>
#include "blocksparse_mlp.h"
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <torch/extension.h>
#include "../util.h"
#include "../hgemm.cuh"

std::tuple<at::Tensor, at::Tensor> blocksparse_mlp_routing(
    int bsz,
    const py::object& cfg,
    const at::Tensor& y,
    const py::dict& params
)
{
    bool activate_all = false;
    if (params.contains("activate_all_experts"))
        activate_all = params["activate_all_experts"].cast<bool>();

    at::Tensor gate_tensor = cfg.attr("gate_tensor").cast<at::Tensor>();
    int64_t num_experts = cfg.attr("num_experts").cast<int64_t>();
    int64_t num_exp_per_tok = cfg.attr("num_experts_per_tok").cast<int64_t>();

    if (!activate_all && bsz == 1)
    {
        at::Tensor router_logits_bsz1 = cfg.attr("router_logits_bsz1").cast<at::Tensor>();
        at::Tensor routing_weights_bsz1 = cfg.attr("routing_weights_bsz1").cast<at::Tensor>();
        at::Tensor selected_experts_bsz1 = cfg.attr("selected_experts_bsz1").cast<at::Tensor>();

        at::matmul_out(router_logits_bsz1, y, gate_tensor);
        at::topk_out
        (
            routing_weights_bsz1,
            selected_experts_bsz1,
            router_logits_bsz1,
            num_exp_per_tok,
            -1,
            true,
            false
        );

        at::softmax_out(routing_weights_bsz1, routing_weights_bsz1, -1);
        return {selected_experts_bsz1, routing_weights_bsz1};
    }
    else
    {
        int64_t k = activate_all ? num_experts : num_exp_per_tok;

        at::Tensor router_logits = at::matmul(y, gate_tensor);

        auto topk_result = at::topk(router_logits, k, -1);
        at::Tensor routing_weights = std::get<0>(topk_result);
        at::Tensor selected_experts = std::get<1>(topk_result);

        routing_weights = at::softmax(routing_weights, -1);

        return {selected_experts, routing_weights};
    }
}
</content>

<content full_path="exllamav3/exllamav3_ext/generator/sampling_basic.cuh">
#pragma once

#include <ATen/Tensor.h>

void argmax_sample
(
    const at::Tensor& logits,
    at::Tensor& ids,
    int max_logit
);

void gumbel_sample
(
    const at::Tensor& logits,
    at::Tensor& ids,
    int max_logit,
    uint32_t random
);
</content>

<content full_path="exllamav3/exllamav3_ext/generator/cache.cu">
#include "cache.cuh"
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include "../util.h"
#include "../util.cuh"

#define NUM_THREADS 512
#define NUM_BLOCKS 128

__global__ __launch_bounds__(NUM_THREADS)
void cache_rotate_kernel
(
    uint8_t* __restrict__ cache,
    const int32_t* __restrict__ order,
    uint8_t* __restrict__ temp,
    size_t page_size,
    size_t rotate_len
)
{
    // Chunk for current CTA
    size_t block_size = CEIL_DIVIDE(page_size, gridDim.x);
    size_t block_beg = blockIdx.x * block_size;
    size_t block_end = MIN(block_beg + block_size, page_size);
    block_size = block_end - block_beg;
    if (block_size <= 0) return;

    // Rotate pages
    for (int i = 0; i < rotate_len; ++i)
    {
        int64_t a = (int64_t) order[2 * i];
        int64_t b = (int64_t) order[2 * i + 1];
        uint8_t* dst = (a >= 0 ? cache + page_size * a : temp) + block_beg;
        uint8_t* src = (b >= 0 ? cache + page_size * b : temp) + block_beg;
        for (int offset = threadIdx.x * 16; offset < block_size; offset += NUM_THREADS * 16)
            *((uint4*) (dst + offset)) = *((uint4*) (src + offset));
        __syncthreads();
    }
}

/*
Reorder cache pages
- cache, paged cache, shape (num_pages, ...), any dtype, contiguous
- order, sequence to rotate, shape (2*n,), dtype int
- temp, temp storage, sized as one cache page

Performs:

for i in range(n):
    a = order[2*i]
    b = order[2*i+1]
    copy: (page[a] if a >= 0 else temp) <- (page[b] if b >= 0 else temp)
*/

void cache_rotate
(
    const at::Tensor& cache,
    const at::Tensor& order,
    const at::Tensor& temp
)
{
    const at::cuda::OptionalCUDAGuard device_guard(cache.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    TORCH_CHECK(cache.dim() >= 2, "cache argument must have dim >= 2")
    TORCH_CHECK(order.dim() == 1, "order argument must have dim == 1")
    TORCH_CHECK_DTYPE(order, kInt);

    size_t num_pages = cache.size(0);
    size_t page_size = cache.nbytes() / num_pages;
    size_t rotate_len = order.size(0) / 2;

    TORCH_CHECK(temp.nbytes() == page_size, "temp tensor incorrect size");

    cache_rotate_kernel<<<NUM_BLOCKS, NUM_THREADS, 0, stream>>>
    (
        (uint8_t*) cache.data_ptr(),
        (const int32_t*) order.data_ptr(),
        (uint8_t*) temp.data_ptr(),
        page_size,
        rotate_len
    );
}
</content>

<content full_path="exllamav3/exllamav3_ext/generator/strings.cpp">
#include "strings.h"
#include "../util.h"

// Compare string Q against list of strings S, utf-32 encoded and packed in byte array.
//
// Returns:
// -1: No matches
// -2: Partial match; at least one string in S partially overlaps Q on the right-hand side
// >= 0: Index into Q of full match with at least one string in S

int partial_strings_match
(
    py::buffer match,
    py::buffer offsets,
    py::buffer strings
)
{
    py::buffer_info info;

    info = match.request();
    uint32_t* q = static_cast<uint32_t*>(info.ptr);
    int q_len = info.size / 4;

    info = offsets.request();
    uint32_t* offsets_int = static_cast<uint32_t*>(info.ptr);
    int num_strings = info.size / 4 - 1;

    info = strings.request();
    uint32_t* strings_utf32 = static_cast<uint32_t*>(info.ptr);

    for (int i = 0; i < num_strings; ++i)
    {
        int beg = offsets_int[i] / 4;
        int s_len = offsets_int[i + 1] / 4 - beg;
        uint32_t* s = strings_utf32 + beg;

        int a = 0;
        int b = 0;
        while (a < q_len)
        {
            int a0 = a;
            while (q[a++] == s[b++])
            {
                if (b == s_len) return a0;
                if (a == q_len) return -2;
            }
            a = a0 + 1;
            b = 0;
       }
    }

    return -1;
}

int count_match_tensor
(
    at::Tensor a,
    at::Tensor b,
    int max_a
)
{
    uint64_t* pa = (uint64_t*) a.data_ptr();
    uint64_t* pb = (uint64_t*) b.data_ptr();
    int max_b = b.size(1);
    if (max_b < max_a) max_a = max_b;

    int match = 0;
    while (match < max_a && *pa++ == *pb++)
        match++;

    return match;
}
</content>

<content full_path="exllamav3/exllamav3_ext/generator/sampling_basic.cu">
#include "sampling_basic.cuh"
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include "../util.h"
#include "../util.cuh"
#include <limits>
#include <curand_kernel.h>
#include "../reduction.cuh"

constexpr float NEG_INF_F32 = -std::numeric_limits<float>::infinity();
// constexpr float POS_INF_F32 = std::numeric_limits<float>::infinity();

#define NUM_THREADS 1024

inline __device__ float gumbel(float x)
{
    return -__logf(fmaxf(-__logf(fmaxf(x, 1e-20)), 1e-20));
}

inline __device__ ValIdx argmax2f(int idx, float& x0, float& x1)
{
    ValIdx vi;
    if (x0 >= x1)
    {
        vi.val = x0;
        vi.idx = idx;
    }
    else
    {
        vi.val = x1;
        vi.idx = idx + 1;
    }
    vi = block_reduce_argmax(vi);
    return vi;
}

inline __device__ bool read2f
(
    const half* logits_ptr,
    int idx,
    float& x0,
    float& x1,
    int num_logits,
    int max_logit
)
{
    if (idx >= num_logits)
    {
        x0 = NEG_INF_F32;
        x1 = NEG_INF_F32;
        return false;
    }
    else
    {
        half2 x0x1 = *((half2*) (logits_ptr + idx));
        if (idx < max_logit - 1) x0 = __half2float(__low2half(x0x1));
        else x0 = NEG_INF_F32;
        if (idx < max_logit) x1 = __half2float(__high2half(x0x1));
        else x1 = NEG_INF_F32;
        return true;
    }
}

__global__ __launch_bounds__(NUM_THREADS)
void argmax_sample_kernel
(
    const half* __restrict__ logits,
    uint64_t* __restrict__ ids,
    int num_logits,
    int max_logit
)
{
    const half* logits_ptr = logits + num_logits * blockIdx.x;
    uint64_t* ids_ptr = ids + blockIdx.x;

    ValIdx maxvi = { NEG_INF_F32, 0 };
    int idx = threadIdx.x * 2;
    int blocks = CEIL_DIVIDE(max_logit, NUM_THREADS * 2);
    for (int block = 0; block < blocks; ++block, idx += NUM_THREADS * 2)
    {
        float x0, x1;
        read2f(logits_ptr, idx, x0, x1, num_logits, max_logit);
        ValIdx vi = argmax2f(idx, x0, x1);
        if (threadIdx.x == 0 && vi.val > maxvi.val)
            maxvi = vi;
    }

    if (threadIdx.x == 0)
        *ids_ptr = (uint64_t) maxvi.idx;
}

__global__ __launch_bounds__(NUM_THREADS)
void gumbel_sample_kernel
(
    const half* __restrict__ logits,
    uint64_t* __restrict__ ids,
    int num_logits,
    int max_logit,
    uint32_t random
)
{
    const half* logits_ptr = logits + num_logits * blockIdx.x;
    uint64_t* ids_ptr = ids + blockIdx.x;

    curandStatePhilox4_32_10_t state;
    curand_init(random, threadIdx.x, 0, &state);

    ValIdx maxvi = { NEG_INF_F32, 0 };
    int idx = threadIdx.x * 2;
    int blocks = CEIL_DIVIDE(max_logit, NUM_THREADS * 2);
    for (int block = 0; block < blocks; ++block, idx += NUM_THREADS * 2)
    {
        float x0, x1;
        if (read2f(logits_ptr, idx, x0, x1, num_logits, max_logit))
        {
            float rf0 = curand_uniform(&state);
            float rf1 = curand_uniform(&state);
            x0 += gumbel(rf0);
            x1 += gumbel(rf1);
        }
        ValIdx vi = argmax2f(idx, x0, x1);
        if (threadIdx.x == 0 && vi.val > maxvi.val)
            maxvi = vi;
    }

    if (threadIdx.x == 0)
        *ids_ptr = (uint64_t) maxvi.idx;
}

void common
(
    const at::Tensor& logits,
    at::Tensor& ids,
    int& bsz,
    int& num_logits,
    int& max_logit
)
{
    TORCH_CHECK_DIM(logits, 2);
    TORCH_CHECK_DIM(ids, 2);
    TORCH_CHECK_DTYPE(logits, kHalf);
    TORCH_CHECK_DTYPE(ids, kLong);
    TORCH_CHECK_SHAPES(logits, 0, ids, 0, 1);

    bsz = logits.size(0);
    num_logits = logits.size(1);
    if (max_logit > num_logits) max_logit = num_logits;
}

void argmax_sample
(
    const at::Tensor& logits,
    at::Tensor& ids,
    int max_logit
)
{
    const at::cuda::OptionalCUDAGuard device_guard(logits.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    if (!max_logit) max_logit = logits.size(-1);

    int bsz, num_logits;
    common(logits, ids, bsz, num_logits, max_logit);
    argmax_sample_kernel<<<bsz, NUM_THREADS, 0, stream>>>
    (
        (const half*) logits.data_ptr(),
        (uint64_t*) ids.data_ptr(),
        num_logits,
        max_logit
    );
}

void gumbel_sample
(
    const at::Tensor& logits,
    at::Tensor& ids,
    int max_logit,
    uint32_t random
)
{
    const at::cuda::OptionalCUDAGuard device_guard(logits.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    if (!max_logit) max_logit = logits.size(-1);

    int bsz, num_logits;
    common(logits, ids, bsz, num_logits, max_logit);
    gumbel_sample_kernel<<<bsz, NUM_THREADS, 0, stream>>>
    (
        (const half*) logits.data_ptr(),
        (uint64_t*) ids.data_ptr(),
        num_logits,
        max_logit,
        random
    );
}

</content>

<content full_path="exllamav3/exllamav3_ext/generator/strings.h">
#pragma once

#include <vector>
#include <string>
#include <pybind11/pybind11.h>
#include <pybind11/pytypes.h>

#include <ATen/Tensor.h>

namespace py = pybind11;

int partial_strings_match
(
    py::buffer match,
    py::buffer offsets,
    py::buffer strings
);

int count_match_tensor
(
    at::Tensor a,
    at::Tensor b,
    int max_a
);

</content>

<content full_path="exllamav3/exllamav3_ext/generator/gumbel.cuh">
#pragma once

#include <ATen/Tensor.h>

void gumbel_noise_f16
(
    const at::Tensor& logits_in,
    at::Tensor& logits,
    uint32_t random
);

void gumbel_noise_f32
(
    const at::Tensor& logits_in,
    at::Tensor& logits,
    uint32_t random
);

void gumbel_noise_log
(
    const at::Tensor& probs,
    at::Tensor& logits,
    uint32_t random
);
</content>

<content full_path="exllamav3/exllamav3_ext/generator/cache.cuh">
#pragma once

#include <ATen/Tensor.h>

void cache_rotate
(
    const at::Tensor& cache,
    const at::Tensor& order,
    const at::Tensor& temp
);
</content>

<content full_path="exllamav3/exllamav3_ext/generator/rep_pen.cuh">
#pragma once

#include <ATen/Tensor.h>

void apply_rep_pens
(
    const at::Tensor& in_logits,
    const at::Tensor& out_logits,
    const at::Tensor& past_ids,
    float rep_p,
    int sustain_range,
    int decay_range
);

void apply_pres_freq_pens
(
    const at::Tensor& in_logits,
    const at::Tensor& out_logits,
    const at::Tensor& past_ids,
    float pres_p,
    float freq_p,
    int sustain_range,
    int decay_range
);

</content>

<content full_path="exllamav3/exllamav3_ext/generator/rep_pen.cu">
#include "rep_pen.cuh"
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include "../util.h"
#include "../util.cuh"
#include <limits>

#define BLOCK_VOCAB_SPAN 4096
#define NUM_THREADS 1024

__device__ __forceinline__
float shmemAtomicMaxF(float* addr, float val)  // val > 0
{
    auto uaddr = reinterpret_cast<unsigned int*>(addr);
    unsigned int old = atomicMax(uaddr, __float_as_uint(val));
    return __uint_as_float(old);
}

template <bool input_fp16>
__global__ __launch_bounds__(NUM_THREADS)
void apply_rep_pens_kernel
(
    const void* __restrict__ in_logits,
    float* __restrict__ out_logits,
    const uint64_t* __restrict__ past_ids,
    int past_len,
    int vocab_size,
    float rep_p,
    int sustain_range,
    int decay_range
)
{
    // Each block processes a range of the logits
    int range_min = blockIdx.x * BLOCK_VOCAB_SPAN;
    int range_max = MIN(range_min + BLOCK_VOCAB_SPAN, vocab_size);

    __shared__ float factors[BLOCK_VOCAB_SPAN];
    for (int i = threadIdx.x; i < BLOCK_VOCAB_SPAN; i += NUM_THREADS)
        factors[i] = 0.0f;
    __syncthreads();

    // Record which tokens from the range appear in past_ids and
    for (int i = threadIdx.x; i < past_len; i += NUM_THREADS)
    {
        if (i < past_len - sustain_range - decay_range)
            continue;

        int tid = (int) past_ids[i];
        if (tid < range_min || tid >= range_max)
            continue;

        int dist = past_len - i;
        if (dist <= sustain_range)
            factors[tid - range_min] = 1.0f;
        else
        {
            float f = MAX(0.0f, 1.0f - ((float)dist - (float)sustain_range) / (float)decay_range);
            shmemAtomicMaxF(factors + tid - range_min, f);
        }
    }
    __syncthreads();

    // Apply penalties to range
    for (int i = threadIdx.x; i < BLOCK_VOCAB_SPAN; i += NUM_THREADS)
    {
        if (i + range_min >= vocab_size)
            break;

        float v;
        if constexpr (input_fp16)
            v = __half2float(((half*) in_logits)[i + range_min]);
        else
            v = ((float*) in_logits)[i + range_min];

        float w = v > 0.0f ? v / rep_p : v * rep_p;
        float f = factors[i] + 1e-30;
        float f1 = (1.0f - f) + 1e-30;
        float o = v * f1 + w * f;
        out_logits[i + range_min] = o;
    }
}

void apply_rep_pens
(
    const at::Tensor& in_logits,
    const at::Tensor& out_logits,
    const at::Tensor& past_ids,
    float rep_p,
    int sustain_range,
    int decay_range
)
{
    const at::cuda::OptionalCUDAGuard device_guard(in_logits.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    TORCH_CHECK_DTYPE(out_logits, kFloat);
    TORCH_CHECK_DTYPE(past_ids, kLong);
    TORCH_CHECK(in_logits.size(0) == 1, "rep. penalties only implemented for bsz 1");  // TODO
    TORCH_CHECK_SHAPES(past_ids, 0, in_logits, 0, 1);
    TORCH_CHECK_SHAPES_FULL(in_logits, out_logits);

    int past_len = past_ids.size(1);
    int vocab_size = in_logits.size(1);
    int num_blocks = CEIL_DIVIDE(vocab_size, BLOCK_VOCAB_SPAN);

    #define kernel_args \
        (const void*) in_logits.data_ptr(), \
        (float*) out_logits.data_ptr(), \
        (const uint64_t*) past_ids.data_ptr(), \
        past_len, \
        vocab_size, \
        rep_p, \
        sustain_range, \
        decay_range

    if (in_logits.dtype() == at::kHalf)
        apply_rep_pens_kernel<true><<<num_blocks, NUM_THREADS, 0, stream>>>(kernel_args);
    else
        apply_rep_pens_kernel<false><<<num_blocks, NUM_THREADS, 0, stream>>>(kernel_args);

    #undef kernel_args

    cuda_check(cudaPeekAtLastError());
}


template <bool input_fp16>
__global__ __launch_bounds__(NUM_THREADS)
void apply_pres_freq_pens_kernel
(
    const void* __restrict__ in_logits,
    float* __restrict__ out_logits,
    const uint64_t* __restrict__ past_ids,
    int past_len,
    int vocab_size,
    float pres_p,
    float freq_p,
    int sustain_range,
    int decay_range
)
{
    // Each block processes a range of the logits
    int range_min = blockIdx.x * BLOCK_VOCAB_SPAN;
    int range_max = MIN(range_min + BLOCK_VOCAB_SPAN, vocab_size);

    __shared__ float frequency[BLOCK_VOCAB_SPAN];
    __shared__ float presence[BLOCK_VOCAB_SPAN];
    for (int i = threadIdx.x; i < BLOCK_VOCAB_SPAN; i += NUM_THREADS)
    {
        frequency[i] = 0.0f;
        presence[i] = 0.0f;
    }
    __syncthreads();

    // Record which tokens from the range appear in past_ids and
    for (int i = threadIdx.x; i < past_len; i += NUM_THREADS)
    {
        if (i < past_len - sustain_range - decay_range)
            continue;

        int tid = (int) past_ids[i];
        if (tid < range_min || tid >= range_max)
            continue;

        int dist = past_len - i;
        float pen = MIN(1.0f, MAX(0.0f, 1.0f - ((float)dist - (float)sustain_range) / (float)decay_range));
        atomicAdd(frequency + tid - range_min, pen * freq_p);
        shmemAtomicMaxF(presence + tid - range_min, pen * pres_p);
    }
    __syncthreads();

    // Apply frequency to range
    for (int i = threadIdx.x; i < BLOCK_VOCAB_SPAN; i += NUM_THREADS)
    {
        if (i + range_min >= vocab_size)
            break;

        float v;
        if constexpr (input_fp16)
            v = __half2float(((half*) in_logits)[i + range_min]);
        else
            v = ((float*) in_logits)[i + range_min];

        v -= frequency[i];
        v -= presence[i];
        out_logits[i + range_min] = v;
    }
}

void apply_pres_freq_pens
(
    const at::Tensor& in_logits,
    const at::Tensor& out_logits,
    const at::Tensor& past_ids,
    float pres_p,
    float freq_p,
    int sustain_range,
    int decay_range
)
{
    const at::cuda::OptionalCUDAGuard device_guard(in_logits.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    TORCH_CHECK_DTYPE(out_logits, kFloat);
    TORCH_CHECK_DTYPE(past_ids, kLong);
    TORCH_CHECK(in_logits.size(0) == 1, "rep. penalties only implemented for bsz 1");  // TODO
    TORCH_CHECK_SHAPES(past_ids, 0, in_logits, 0, 1);
    TORCH_CHECK_SHAPES_FULL(in_logits, out_logits);

    int past_len = past_ids.size(1);
    int vocab_size = in_logits.size(1);
    int num_blocks = CEIL_DIVIDE(vocab_size, BLOCK_VOCAB_SPAN);

    #define kernel_args \
        (const void*) in_logits.data_ptr(), \
        (float*) out_logits.data_ptr(), \
        (const uint64_t*) past_ids.data_ptr(), \
        past_len, \
        vocab_size, \
        pres_p, \
        freq_p, \
        sustain_range, \
        decay_range

    if (in_logits.dtype() == at::kHalf)
        apply_pres_freq_pens_kernel<true><<<num_blocks, NUM_THREADS, 0, stream>>>(kernel_args);
    else
        apply_pres_freq_pens_kernel<false><<<num_blocks, NUM_THREADS, 0, stream>>>(kernel_args);

    #undef kernel_args

    cuda_check(cudaPeekAtLastError());
}
</content>

<content full_path="exllamav3/exllamav3_ext/generator/gumbel.cu">
#include "sampling_basic.cuh"
#include <c10/cuda/CUDAGuard.h>
#include <ATen/cuda/CUDAContext.h>
#include <cuda_fp16.h>
#include "../util.h"
#include "../util.cuh"
#include <limits>
#include <curand_kernel.h>

#define NUM_THREADS 1024

inline __device__ float gumbel(float x)
{
    return -__logf(fmaxf(-__logf(fmaxf(x, 1e-20)), 1e-20));
}

__global__ __launch_bounds__(NUM_THREADS)
void gumbel_noise_kernel_f16
(
    const half* __restrict__ in_logits,
    half* __restrict__ logits,
    int size,
    uint32_t random
)
{
    int idx = (threadIdx.x + NUM_THREADS * blockIdx.x) * 2;
    if (idx >= size) return;

    curandStatePhilox4_32_10_t state;
    curand_init(random, idx, 0, &state);

    half2 x01 = *((half2*) (in_logits + idx));
    float x0 = __half2float(__low2half(x01));
    float x1 = __half2float(__high2half(x01));
    float rf0 = curand_uniform(&state);
    curand_init(random, idx + 1, 0, &state);
    float rf1 = curand_uniform(&state);
    x0 += gumbel(rf0);
    x1 += gumbel(rf1);
    x01 = __floats2half2_rn(x0, x1);
    *((half2*) (logits + idx)) = x01;
}

__global__ __launch_bounds__(NUM_THREADS)
void gumbel_noise_kernel_f32
(
    const float* __restrict__ in_logits,
    float* __restrict__ logits,
    int size,
    uint32_t random
)
{
    int idx = threadIdx.x + NUM_THREADS * blockIdx.x;
    if (idx >= size) return;

    curandStatePhilox4_32_10_t state;
    curand_init(random, idx, 0, &state);

    float x = in_logits[idx];
    float rf = curand_uniform(&state);
    x += gumbel(rf);
    logits[idx] = x;
}


__global__ __launch_bounds__(NUM_THREADS)
void gumbel_noise_kernel_log
(
    const float* __restrict__ probs,
    float* __restrict__ logits,
    int size,
    uint32_t random
)
{
    int idx = threadIdx.x + NUM_THREADS * blockIdx.x;
    if (idx >= size) return;

    curandStatePhilox4_32_10_t state;
    curand_init(random, idx, 0, &state);

    float x = probs[idx];
    x = __logf(x);
    float rf = curand_uniform(&state);
    x += gumbel(rf);
    logits[idx] = x;
}

void gumbel_noise_f16
(
    const at::Tensor& logits_in,
    at::Tensor& logits,
    uint32_t random
)
{
    const at::cuda::OptionalCUDAGuard device_guard(logits.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    TORCH_CHECK_DTYPE(logits_in, kHalf);
    TORCH_CHECK_DTYPE(logits, kHalf);

    int size = logits.numel();
    int blocks = CEIL_DIVIDE(size / 2, NUM_THREADS);

    gumbel_noise_kernel_f16<<<blocks, NUM_THREADS, 0, stream>>>
    (
        (const half*) logits_in.data_ptr(),
        (half*) logits.data_ptr(),
        size,
        random
    );
}

void gumbel_noise_f32
(
    const at::Tensor& logits_in,
    at::Tensor& logits,
    uint32_t random
)
{
    const at::cuda::OptionalCUDAGuard device_guard(logits.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    TORCH_CHECK_DTYPE(logits_in, kFloat);
    TORCH_CHECK_DTYPE(logits, kFloat);

    int size = logits.numel();
    int blocks = CEIL_DIVIDE(size, NUM_THREADS);

    gumbel_noise_kernel_f32<<<blocks, NUM_THREADS, 0, stream>>>
    (
        (const float*) logits_in.data_ptr(),
        (float*) logits.data_ptr(),
        size,
        random
    );
}

void gumbel_noise_log
(
    const at::Tensor& probs,
    at::Tensor& logits,
    uint32_t random
)
{
    const at::cuda::OptionalCUDAGuard device_guard(logits.device());
    cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();

    TORCH_CHECK_DTYPE(probs, kFloat);
    TORCH_CHECK_DTYPE(logits, kFloat);
    TORCH_CHECK_SHAPES_FULL(probs, logits);

    int size = probs.numel();
    int blocks = CEIL_DIVIDE(size, NUM_THREADS);

    gumbel_noise_kernel_log<<<blocks, NUM_THREADS, 0, stream>>>
    (
        (const float*) probs.data_ptr(),
        (float*) logits.data_ptr(),
        size,
        random
    );
}
</content>

<content full_path="exllamav3/models/phi3.py">
from __future__ import annotations
from typing_extensions import override
import torch
from .config import Config, no_default
from .model import Model
from ..util.rope import RopeSettings, RopeStyle
from ..modules import RMSNorm, Embedding, TransformerBlock, Attention, GatedMLP, Linear
from ..modules.attn import prepare_for_attn

class Phi3Config(Config):
    arch_string = "Phi3ForCausalLM"

    def __init__(
        self,
        directory: str,
        **kwargs,
    ):
        super().__init__(
            directory,
            {"text": Phi3Model},
            **kwargs
        )

        # Attention params
        self.head_dim = self.read_cfg(int, "head_dim", None)
        self.hidden_size = self.read_cfg(int, "hidden_size", no_default)
        self.num_q_heads = self.read_cfg(int, "num_attention_heads", no_default)
        self.num_kv_heads = self.read_cfg(int, "num_key_value_heads", self.num_q_heads)

        if not self.head_dim:
            self.head_dim = self.hidden_size // self.num_q_heads

        # MLP
        self.assert_cfg(str, "hidden_act", "silu", True)
        self.intermediate_size = self.read_cfg(int, "intermediate_size", no_default)

        # Norms
        self.rms_norm_eps = self.read_cfg(float, "rms_norm_eps", no_default)

        # Layers
        self.num_hidden_layers = self.read_cfg(int, "num_hidden_layers", no_default)
        self.tie_word_embeddings = self.read_cfg(bool, "tie_word_embeddings", False)

        # RoPE
        self.rope_settings = self.read_rope_settings_default(RopeStyle.NEOX)


    @override
    def override_dynamic_seq_len(self, new_max_position_embeddings: int):
        self.rope_settings.override_max_position_embeddings = new_max_position_embeddings


class Phi3Model(Model):
    config_class = Phi3Config

    def __init__(
        self,
        config: Phi3Config,
        **kwargs
    ):
        super().__init__(config, **kwargs)

        self.modules += [
            Embedding(
                config = config,
                key = "model.embed_tokens",
                vocab_size = config.vocab_size,
                hidden_size = config.hidden_size,
            )
        ]

        self.first_block_idx = len(self.modules)

        self.modules += [
            TransformerBlock(
                config = config,
                key = f"model.layers.{idx}",
                attn_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.input_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                ),
                attn = Attention(
                    config = config,
                    key = f"model.layers.{idx}.self_attn",
                    layer_idx = idx,
                    hidden_size = config.hidden_size,
                    head_dim = config.head_dim,
                    num_q_heads = config.num_q_heads,
                    num_kv_heads = config.num_kv_heads,
                    rope_settings = config.rope_settings,
                    sm_scale = None,
                    key_q = "q_proj",
                    key_k = "k_proj",
                    key_v = "v_proj",
                    key_o = "o_proj",
                    key_fused_qkv = "qkv_proj",
                    qmap = "block.attn",
                ),
                mlp_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.post_attention_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                ),
                mlp = GatedMLP(
                    config = config,
                    key = f"model.layers.{idx}.mlp",
                    hidden_size = config.hidden_size,
                    intermediate_size = config.intermediate_size,
                    key_up = "up_proj",
                    key_gate = "gate_proj",
                    key_down = "down_proj",
                    key_fused_gate_up = "gate_up_proj",
                    qmap = "block.mlp",
                ),
            )
            for idx in range(config.num_hidden_layers)
        ]

        self.last_kv_module_idx = len(self.modules) - 1

        head_alt_key = None
        if config.tie_word_embeddings and not self.config.stc.has_tensor("lm_head"):
            head_alt_key = "model.embed_tokens"

        self.modules += [
            RMSNorm(
                config = config,
                key = "model.norm",
                rms_norm_eps = config.rms_norm_eps,
                out_dtype = torch.half,
            ),
            Linear(
                config = config,
                key = "lm_head",
                qbits_key = "head_bits",
                alt_key = head_alt_key,
                in_features = config.hidden_size,
                out_features = config.vocab_size,
                qmap = "block",
                caps = {"logits_output": True}
            )
        ]

        self.logit_layer_idx = len(self.modules) - 1


    @override
    def prepare_inputs(self, input_ids: torch.Tensor, params: dict) -> torch.Tensor:
        params["input_ids"] = input_ids
        input_ids = prepare_for_attn(input_ids, params)
        return input_ids
</content>

<content full_path="exllamav3/models/config.py">
from __future__ import annotations
from abc import ABC, abstractproperty, abstractmethod
import torch
import torch.nn.functional as F
from torch import nn
import os, json
from ..util.rope import RopeSettings, RopeStyle
from ..loader import SafetensorsCollection, VariantSafetensorsCollection
from ..util.file import read_dict, no_value, no_default
import uuid

class Config(ABC):
    arch_string = None
    load_isq: bool

    def __init__(
        self,
        directory: str,
        model_classes: dict,
        **kwargs,
    ):
        """
        Read a HF model config and prepare it for instantiation and loading

        :param directory:
            Directory containg the model config.json, weights, etc.

        :param expect_arch:
            Expected achitecture string
        """

        self.directory = directory
        self.model_classes = model_classes
        self.uuid = uuid.uuid4()

        # Verify architecture
        self.config_filename = os.path.join(directory, "config.json")
        with open(self.config_filename, encoding = "utf8") as f:
            self.config_dict = json.load(f)

        assert len(self.config_dict["architectures"]) == 1, \
            f"Multiple architectures defined in {self.config_filename}"

        arch = self.config_dict["architectures"][0]
        assert arch == self.arch_string, \
            f"Unexpected architecture {arch} in {self.config_filename}, should be {self.arch_string}."
        self.architecture = arch

        # Special mode to load tensors from across multiple variants of the same model
        if kwargs.get("st_variants"):
            self.stc = VariantSafetensorsCollection(kwargs.get("st_variants"))

        # Collect all .safetensors files in directory
        else:
            self.stc = SafetensorsCollection(directory, load_method = kwargs.get("load_method"))

        # Standard params, vocab
        self.bos_token_id = self.read_cfg(int, "bos_token_id", None)
        self.eos_token_id = self.read_cfg([int, list], "eos_token_id", None)
        self.pad_token_id = self.read_cfg(int, "pad_token_id", None)
        self.vocab_size = self.read_cfg(int, ["vocab_size", "text_config->vocab_size"], None)
        if isinstance(self.eos_token_id, list):
            self.eos_token_id_list = self.eos_token_id
            self.eos_token_id = self.eos_token_id[0]
        else:
            self.eos_token_id_list = [self.eos_token_id]

        # Standard params, unused
        self.initializer_range = self.read_cfg(float, "initializer_range", 0.02)

        # Universal params
        self.num_hidden_layers = -1
        self.head_dim = -1
        self.num_q_heads = -1
        self.num_kv_heads = -1
        self.pos_encoding_mode = "NONE"

        # Load parameters
        self.load_isq = False


    def read_cfg(self, *args):
        """
        Read from config.json, see read()
        """
        return read_dict(self.config_dict, *args)


    def assert_cfg(
        self,
        expected_type: type | list[type],
        keys: str | list[str],
        expected_value = no_value,
        optional = False
    ):
        """
        Read from config.json, see read(). Assert that config item either:
            - has expected value, or
            - has one of the expected values (if expected_value is list), or
            - is not present (if expected_value == no_value), or
        """

        value = self.read_cfg(expected_type, keys, no_value)
        if isinstance(expected_value, list):
            if value not in expected_value:
                raise ValueError(f"Key {keys} expected to be one of {expected_value} but was {value}")
        else:
            if value == no_value and not optional:
                raise ValueError(f"Key {keys} expected but not present.")
            if value != no_value and value != expected_value:
                raise ValueError(f"Key {keys} expected to have value {expected_value} but was {value}")


    @staticmethod
    def from_directory(directory: str, **kwargs) -> Config:
        """
        Create config from the specified directory if it contains a HF model of a supported architecture

        :param directory:
            Directory containing model files

        :param kwargs:
            load_method:
                See exllamav3.loader.safetensors.SafetensorsCollection

        :return:
            Architecture-specific config deriving from Exl2Config
        """

        from exllamav3.models.architectures import get_architectures
        architectures = get_architectures()

        config_filename = os.path.join(directory, "config.json")
        with open(config_filename, encoding = "utf8") as f:
            config_dict = json.load(f)

        assert "architectures" in config_dict, f"No architecture defined in {config_filename}"
        archs = config_dict["architectures"]
        assert len(archs) == 1, f"Multiple architectures defined in {config_filename}"
        arch = archs[0]
        assert arch in architectures, f"Unknown architecture {arch} in {config_filename}"

        arch_def = architectures[arch]
        config_class = arch_def["config_class"]
        config = config_class(directory, **kwargs)
        return config


    def read_rope_settings_default(
        self,
        rope_style: RopeStyle,
        default_rope_theta: float = 10000.0,
        default_partial_rotary_factor: float = 1.0,
        config_dict: dict | None = None,
    ):
        if config_dict is None:
            config_dict = self.config_dict

        return RopeSettings(
            head_dim = self.head_dim,
            rope_theta = read_dict(config_dict, float, "rope_theta", default_rope_theta),
            rope_scaling = read_dict(config_dict, dict, "rope_scaling", None),
            partial_rotary_factor = read_dict(config_dict, float, "partial_rotary_factor", default_partial_rotary_factor),
            max_position_embeddings = read_dict(config_dict, int, "max_position_embeddings", None),
            original_max_position_embeddings = read_dict(config_dict, int, "original_max_position_embeddings", None),
            rope_style = rope_style,
        )


    def override_dynamic_seq_len(self, new_max_position_embeddings: int):
        """
        Override max_position_embeddings from the config. Necessary for some models (like Phi) that have two
        sets of RoPE factors, so the correct set can be loaded as the model is initialized. Changing this after
        the model is created has no effect.
        """
        pass
</content>

<content full_path="exllamav3/models/mistral.py">

from .llama import LlamaConfig, LlamaModel

# Mistral is identical to Llama

class MistralConfig(LlamaConfig):
    arch_string = "MistralForCausalLM"

    def __init__(
        self,
        directory: str,
        **kwargs,
    ):
        super().__init__(
            directory,
            derived_model = {"text": MistralModel},
            **kwargs
        )


class MistralModel(LlamaModel):
    config_class = MistralConfig

    def __init__(
        self,
        config: MistralConfig,
        **kwargs
    ):
        super().__init__(config, **kwargs)

</content>

<content full_path="exllamav3/models/__init__.py">
from __future__ import annotations
from .model import Model
from .config import Config

</content>

<content full_path="exllamav3/models/qwen3_moe.py">
from __future__ import annotations
from typing_extensions import override
import torch
from .config import Config, no_default
from .model import Model
from ..util.rope import RopeSettings, RopeStyle
from ..modules import RMSNorm, Embedding, TransformerBlock, Attention, BlockSparseMLP, Linear
from ..modules.attn import prepare_for_attn

class Qwen3MoeConfig(Config):
    arch_string = "Qwen3MoeForCausalLM"

    def __init__(
        self,
        directory: str,
        **kwargs,
    ):
        super().__init__(
            directory,
            {"text": Qwen3MoeModel},
            **kwargs
        )

        # Attention params
        self.head_dim = self.read_cfg(int, "head_dim", None)
        self.hidden_size = self.read_cfg(int, "hidden_size", no_default)
        self.num_q_heads = self.read_cfg(int, "num_attention_heads", no_default)
        self.num_kv_heads = self.read_cfg(int, "num_key_value_heads", self.num_q_heads)

        if not self.head_dim:
            self.head_dim = self.hidden_size // self.num_q_heads

        # MLP params
        self.assert_cfg(str, "hidden_act", "silu", True)
        self.assert_cfg(bool, "norm_topk_prob", True, True)
        self.moe_intermediate_size = self.read_cfg(int, "moe_intermediate_size", no_default)
        self.num_experts = self.read_cfg(int, "num_experts", no_default)
        self.num_experts_per_tok = self.read_cfg(int, "num_experts_per_tok", no_default)

        # Norms
        self.rms_norm_eps = self.read_cfg(float, "rms_norm_eps", no_default)

        # Layers
        self.num_hidden_layers = self.read_cfg(int, "num_hidden_layers", no_default)
        self.tie_word_embeddings = self.read_cfg(bool, "tie_word_embeddings", False)

        # RoPE
        self.rope_settings = self.read_rope_settings_default(RopeStyle.NEOX)


class Qwen3MoeModel(Model):
    config_class = Qwen3MoeConfig

    def __init__(
        self,
        config: Qwen3MoeConfig,
        **kwargs
    ):
        super().__init__(config, **kwargs)

        self.modules += [
            Embedding(
                config = config,
                key = "model.embed_tokens",
                vocab_size = config.vocab_size,
                hidden_size = config.hidden_size,
            )
        ]

        self.first_block_idx = len(self.modules)

        self.modules += [
            TransformerBlock(
                config = config,
                key = f"model.layers.{idx}",
                attn_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.input_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                ),
                attn = Attention(
                    config = config,
                    key = f"model.layers.{idx}.self_attn",
                    layer_idx = idx,
                    hidden_size = config.hidden_size,
                    head_dim = config.head_dim,
                    num_q_heads = config.num_q_heads,
                    num_kv_heads = config.num_kv_heads,
                    rope_settings = config.rope_settings,
                    sm_scale = None,
                    key_q = "q_proj",
                    key_k = "k_proj",
                    key_v = "v_proj",
                    key_o = "o_proj",
                    qmap = "block.attn",
                    q_norm = RMSNorm(
                        config = config,
                        key = f"model.layers.{idx}.self_attn.q_norm",
                        rms_norm_eps = config.rms_norm_eps,
                    ),
                    k_norm = RMSNorm(
                        config = config,
                        key = f"model.layers.{idx}.self_attn.k_norm",
                        rms_norm_eps = config.rms_norm_eps,
                    ),
                    out_dtype = torch.float
                ),
                mlp_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.post_attention_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                ),
                mlp = BlockSparseMLP(
                    config = config,
                    key = f"model.layers.{idx}.mlp",
                    hidden_size = config.hidden_size,
                    intermediate_size = config.moe_intermediate_size,
                    num_experts = self.config.num_experts,
                    num_experts_per_tok = self.config.num_experts_per_tok,
                    key_up = "experts.{expert_idx}.up_proj",
                    key_gate = "experts.{expert_idx}.gate_proj",
                    key_down = "experts.{expert_idx}.down_proj",
                    key_routing_gate = "gate",
                    qmap = "block.mlp",
                    interm_dtype = torch.half,
                    out_dtype = torch.float,
                ),
            )
            for idx in range(config.num_hidden_layers)
        ]

        self.last_kv_module_idx = len(self.modules) - 1

        head_alt_key = None
        if config.tie_word_embeddings and not self.config.stc.has_tensor("lm_head"):
            head_alt_key = "model.embed_tokens"

        self.modules += [
            RMSNorm(
                config = config,
                key = "model.norm",
                rms_norm_eps = config.rms_norm_eps,
                out_dtype = torch.half,
            ),
            Linear(
                config = config,
                key = "lm_head",
                qbits_key = "head_bits",
                alt_key = head_alt_key,
                in_features = config.hidden_size,
                out_features = config.vocab_size,
                qmap = "block",
                caps = {"logits_output": True}
            )
        ]

        self.logit_layer_idx = len(self.modules) - 1

        # Activate all experts during H capture pass in quantization
        self.calibration_all_experts = True


    @override
    def prepare_inputs(self, input_ids: torch.Tensor, params: dict) -> torch.Tensor:
        params["input_ids"] = input_ids
        input_ids = prepare_for_attn(input_ids, params)
        return input_ids
</content>

<content full_path="exllamav3/models/model.py">
from __future__ import annotations

from functools import lru_cache
from typing import Callable

import torch
import torch.nn.functional as F
from torch import nn
import os, json
from .config import Config
from ..util.progress import ProgressBar
from ..util.memory import set_memory_fraction_reserve, set_memory_fraction_use, unset_memory_fraction, free_mem

class Model:

    def __init__(
        self,
        config: Config,
        **kwargs,
    ):
        self.config = config

        self.modules = []

        # Index of last layer that affects KV cache, used during prefill
        self.last_kv_module_idx = None
        self.logit_layer_idx = None
        self.first_block_idx = None

        # Calibration options
        self.calibration_all_experts = False


    def __iter__(self):
        for module in self.modules:
            yield from module


    def find_module(self, key: str):
        for module in self:
            if module.key == key:
                return module


    @lru_cache
    def get_cache_layers(self):
        return [m for m in self if m.caps.get("kv_cache")]


    @staticmethod
    def from_config(
        config: Config,
        component: str = "text",
        **kwargs
    ):
        """
        Create model instance from config

        :param config:
            Config created with Config.from_directory()

        :param component:
            Which component model to load, for models with multiple component.
            # TODO: Implement multimodal components
        """

        assert component in config.model_classes, \
            f"{config.architecture} does not define a '{component}` component model"

        model = config.model_classes[component](config, **kwargs)
        return model


    def prepare_inputs(self, input_ids: torch.Tensor, params: dict) -> torch.Tensor:
        params["input_ids"] = input_ids
        return input_ids


    @torch.inference_mode
    def prefill(self, input_ids: torch.Tensor, params: dict):
        x = self.prepare_inputs(input_ids, params)
        for idx, module in enumerate(self.modules):
            params["prefill"] = (idx == self.last_kv_module_idx)
            x = module.prepare_for_device(x, params)
            x = module.forward(x, params)
            if idx == self.last_kv_module_idx:
                break


    @torch.inference_mode
    def forward(self, input_ids: torch.Tensor, params: dict | None = None):
        if params is None:
            params = {}
        x = self.prepare_inputs(input_ids, params)
        for idx, module in enumerate(self.modules):
            x = module.prepare_for_device(x, params)
            x = module.forward(x, params)
        return x


    def unload(self):
        for module in self.modules:
            module.unload()


    # Load to single device
    def _load_single(self, progressbar: bool, device: torch.device):
        with ProgressBar(f"Loading" if progressbar else None, len(self.modules)) as progress:
            for idx, module in enumerate(self.modules):
                defer = module.can_defer_load()
                if defer:
                    self.config.stc.begin_deferred_load()
                module.load(torch.device("cpu") if module.caps.get("prefer_cpu") else device)
                if defer:
                    self.config.stc.end_deferred_load()
                progress.update(idx + 1)


    # Load with split
    def _load_autosplit(
        self,
        progressbar: bool,
        reserve_per_device: list[int] | None,
        use_per_device: list[int] | None,
        active_devices: list[int],
        max_chunk_size: int,
        max_output_size: int,
        max_output_factor: int,
        callback_sync: Callable[[int, int], None],
        generator: bool
    ):
        current_device_i = 0
        backup_shape = (1, max_chunk_size)
        backup_dtype = torch.long
        dummy_state = None
        prev_load_device = None
        touched_devices = []

        with ProgressBar(f"Loading" if progressbar else None, len(self.modules)) as progress:

            for idx, module in enumerate(self.modules):

                if callback_sync: callback_sync(idx, len(self.modules))
                if generator: yield idx, len(self.modules)

                # Narrow state to max_output_size for logit output layer
                is_logits_layer = module.caps.get("logits_output")
                if is_logits_layer:
                    b, c, d = backup_shape
                    backup_shape = (b, min(max_output_size, c), d)
                    if dummy_state is not None:
                        dummy_state = dummy_state[:, :max_output_size, :]

                while True:
                    try:
                        # Select device
                        load_device = torch.device("cpu") if module.caps.get("prefer_cpu") else \
                            torch.device(active_devices[current_device_i])

                        # Set VRAM limit if new device
                        if load_device != torch.device("cpu") and load_device != prev_load_device:
                            prev_load_device = load_device
                            i = active_devices[current_device_i]
                            if reserve_per_device is not None:
                                set_memory_fraction_reserve(reserve_per_device[i], i)
                            elif use_per_device is not None:
                                 set_memory_fraction_use(use_per_device[i], i)
                            else:
                                raise RuntimeError("Logic error")
                            touched_devices.append(i)

                        # (Re)create or backup hidden state (metadata)
                        if dummy_state is None:
                            dummy_state = torch.zeros(backup_shape, dtype = backup_dtype, device = load_device)
                        else:
                            backup_shape = dummy_state.shape
                            backup_dtype = dummy_state.dtype

                        # Load module
                        defer = module.can_defer_load()
                        if defer:
                            self.config.stc.begin_deferred_load()
                        module.load(load_device)
                        if defer:
                            self.config.stc.end_deferred_load()

                        # Forward dummy state through module
                        dummy_state = module.prepare_for_device(dummy_state, {})
                        dummy_state = module.forward(dummy_state, {})

                        # Account for max_output_factor after last layer,
                        if is_logits_layer:
                            extra_dummy_states = [
                                torch.empty_like(dummy_state)
                                for _ in range(max_output_factor - 1)
                            ]

                        # We're good
                        fail = False
                        progress.update(idx + 1)

                    # We're not good
                    except Exception as e:
                        self.config.stc.abort_deferred_load()
                        if e.__class__.__name__ == "OutOfMemoryError" or \
                            "CUDA out of memory" in str(e) or \
                            "HIP out of memory" in str(e):
                            # Exception object will hold references to tensors so we can't free them here
                            fail = True
                        else:
                            raise

                    # Module failed to load with an OoM error, so advance to the next device if possible
                    if fail:
                        module.unload()
                        dummy_state = None
                        free_mem()
                        current_device_i += 1
                        if current_device_i >= len(active_devices):
                            raise RuntimeError("Insufficient VRAM in split for model and cache")
                        continue

                    # On to next module
                    break

            if callback_sync: callback_sync(len(self.modules), len(self.modules))
            if generator: yield len(self.modules), len(self.modules)

            dummy_state = None
            unset_memory_fraction(touched_devices)

        # Python will not run anything in an async function without at least one yield statement
        if 'yield' in locals():
            yield


    def load_gen(
        self,
        device: torch.device | str | int | None = None,
        reserve_per_device: list[float] | float | None = None,
        use_per_device: list[float] | float | None = None,
        tensor_p: bool = False,
        progressbar: bool = False,
        max_chunk_size: int = 2048,
        max_output_size: int = 32,
        max_output_factor: int = 1,
        callback: Callable[[int, int], None] | None = None,
        generator: bool = True
    ):
        """
        Load model, generator function. For regular function, call load() with the same arguments

        :param device:
            (optional) If specified, load to single device, e.g. "cuda:0"

        :param reserve_per_device:
            (optional) Amount of memory to reserve for any device. Either a value in GB to apply on all devices
            or a list of floats giving an individual reserve per device. Negative reserve excludes device from
            split. E.g.:

            # reserve 4.5 GB on cuda:0, 1 GB on each cuda:1 and on cuda:2
            model.load(reserve_per_device = [4.5, 1, 1])

            # reserve 1 GB on cuda:0 and cuda:2, exclude cuda:1
            model.load(reserve_per_device = [1, -1, 1])

            The default reserve per device is 0.25 GB. This applies to devices not included in reserve_per_device
            as well.

        :param use_per_device:
            (optional) Amount of memory to use per device.

            Does not account for memory allocated by other processes or by the calling process up to the call
            to model.load(), i.e. if cuda:0 currently has 3 GB in use and user_per_device = [12, ...], at the
            end of loading cuda:0 will have up to 15 GB of VRAM allocated, using up to 15 GB during a forward
            pass.

            Devices not included in use_per_device, or included with a value of 0, will not be used, e.g.:

            # use up to 23 GB on cuda:0 and cuda:2, do not load on cuda:1 and cuda:3 (if present)
            model.load(use_per_device = [23, 0, 23])

        :param tensor_p:
            Load in tensor-parallel mode (not implemented yet)  TODO

        :param max_chunk_size:
            The maximum number of tokens to expect in a single forward pass. Informs the layer split only, and
            makes no difference when loading on a single device.

        :param max_output_size:
            The maximum number of output tokens to expect in a single forward pass. Informs the estimate of the
            size of the output logits. Values larger than max_chunk_size have no effect.

        :param max_output_factor:
            When estimating the memory footprint of the output layer, scale the size of the output tensor by
            this factor. For instance, if the first thing you wish to do with a float16 output tensor is upcast
            to float32, a value of 3 here would (attempt to) make sure the output layer always ends up on a
            device where there is enough space for that.

        :param progressbar:
            Show rich progressbar while loading

        :param callback:
            If provided, called with (current_module, num_modules) for every module loaded. Don't specify a
            callback function when using the

        :param generator:
            Always true when using the _gen function directly
        """

        free_mem()

        assert not (bool(reserve_per_device) and bool(use_per_device)), \
            "Cannot specify both memory usage and memory reserve."

        assert max_chunk_size >= 1, "max_chunk_size must be positive"
        assert max_output_size >= 1, "max_output_size must be positive"
        assert max_output_factor >= 1, "max_output_factor must be positive"

        # Load to single device
        if device is not None:
            assert not bool(reserve_per_device) and not bool(use_per_device), \
                "Cannot specify reserve_per_device or use_per_device when loading to single device."
            assert not tensor_p, \
                "Cannot use tensor_p when loading to single device."
            self._load_single(progressbar, device)

        # Split load
        elif not tensor_p:
            rpd = reserve_per_device is not None
            upd = use_per_device is not None
            assert not (rpd and upd), \
                "Cannot specify both reserve_per_device or use_per_device."
            num_devices = torch.cuda.device_count()

            if not upd:
                if reserve_per_device is None:
                    reserve_per_device = [0.25] * num_devices
                elif any(isinstance(reserve_per_device, t) for t in [float, int]):
                    reserve_per_device = [reserve_per_device] * num_devices
                elif not isinstance(reserve_per_device, list):
                    raise ValueError("reserve_per_device must be float or list[float]")
                while len(reserve_per_device) < num_devices:
                    reserve_per_device.append(0.25)
                reserve_per_device = [int(x * 1024**3) for x in reserve_per_device]
                active_devices = [
                    i for i in range(num_devices)
                    if i >= len(reserve_per_device) or reserve_per_device[i] >= 0
                ]

            if upd:
                if any(isinstance(use_per_device, t) for t in [float, int]):
                    use_per_device = [use_per_device] * num_devices
                elif not isinstance(use_per_device, list):
                    raise ValueError("use_per_device must be float or list[float]")
                use_per_device = [int(x * 1024**3) for x in use_per_device]
                active_devices = [
                    i for i, x in enumerate(use_per_device)
                    if x > 0
                ]

            yield from self._load_autosplit(
                progressbar,
                reserve_per_device,
                use_per_device,
                active_devices,
                max_chunk_size,
                max_output_size,
                max_output_factor,
                callback,
                generator
            )

        # Tensor-p load
        else:
            raise NotImplementedError()

        self.config.stc.close()
        free_mem()


    @torch.inference_mode
    def load(self, *args, **kwargs):
        """
        Load as a regular function, see arguments for load_gen().
        """

        kwargs["generator"] = False
        f = self.load_gen(*args, **kwargs)
        for _ in f: pass


    def get_load_metrics(self):
        return self.config.stc.get_metrics()


    def get_layout_tree(self, pre_indent: int) -> str:
        def get_branch(module, b_indent) -> str:
            lines = [get_branch(m, b_indent + 4) for m in module.modules]
            dedup_lines = []
            count = 1
            for i in range(len(lines)):
                if i < len(lines) - 1 and lines[i] == lines[i + 1]:
                    count += 1
                else:
                    pref = ""
                    if count > 1:
                        pref = f"[{count}x] "
                        count = 1
                    dedup_lines.append(lines[i].replace("[]", pref))
            r = " " * (pre_indent + b_indent) + " - []" + module.get_name() + "\n"
            r += "".join(dedup_lines)
            return r
        return get_branch(self, 0).replace("[]", "").rstrip()


    def get_storage_info(self):
        from ..modules import Linear
        def get_tensor_size(tensors):
            return 8 * sum(t.element_size() * t.numel() for t in tensors.values())
        sum_bits = 0
        sum_numel = 0
        head_bpw = 0
        head_numel = 0
        for module in self:
            if module.key.endswith("lm_head"):
                head_bpw = get_tensor_size(module.get_tensors()) / module.weights_numel()
                head_numel = module.weights_numel()
            elif isinstance(module, Linear):
                sum_bits += get_tensor_size(module.get_tensors())
                sum_numel += module.weights_numel()
        vram_bits = head_numel * head_bpw + sum_bits
        return sum_bits / sum_numel, head_bpw, vram_bits


    def get_name(self):
        return self.__class__.__name__


    @staticmethod
    def get_additional_compiled_tensors(config: Config):
        return {}
</content>

<content full_path="exllamav3/models/llama.py">
from __future__ import annotations
from typing_extensions import override
import torch
from .config import Config, no_default
from .model import Model
from ..util.rope import RopeSettings, RopeStyle
from ..modules import RMSNorm, Embedding, TransformerBlock, Attention, GatedMLP, Linear
from ..modules.attn import prepare_for_attn

class LlamaConfig(Config):
    arch_string = "LlamaForCausalLM"

    def __init__(
        self,
        directory: str,
        derived_model: dict | None = None,
        **kwargs,
    ):
        super().__init__(
            directory,
            derived_model if derived_model else {"text": LlamaModel},
            **kwargs
        )

        # Attention params
        self.head_dim = self.read_cfg(int, "head_dim", None)
        self.hidden_size = self.read_cfg(int, "hidden_size", no_default)
        self.num_q_heads = self.read_cfg(int, "num_attention_heads", no_default)
        self.num_kv_heads = self.read_cfg(int, "num_key_value_heads", self.num_q_heads)

        if not self.head_dim:
            self.head_dim = self.hidden_size // self.num_q_heads

        # MLP params
        self.assert_cfg(str, "hidden_act", "silu", True)
        self.intermediate_size = self.read_cfg(int, "intermediate_size", no_default)

        # Norms
        self.rms_norm_eps = self.read_cfg(float, "rms_norm_eps", no_default)

        # Layers
        self.num_hidden_layers = self.read_cfg(int, "num_hidden_layers", no_default)
        self.tie_word_embeddings = self.read_cfg(bool, "tie_word_embeddings", False)

        # RoPE
        self.rope_settings = self.read_rope_settings_default(RopeStyle.NEOX)


class LlamaModel(Model):
    config_class = LlamaConfig

    def __init__(
        self,
        config: LlamaConfig,
        **kwargs
    ):
        super().__init__(config, **kwargs)

        self.modules += [
            Embedding(
                config = config,
                key = "model.embed_tokens",
                vocab_size = config.vocab_size,
                hidden_size = config.hidden_size,
            )
        ]

        self.first_block_idx = len(self.modules)

        self.modules += [
            TransformerBlock(
                config = config,
                key = f"model.layers.{idx}",
                attn_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.input_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                ),
                attn = Attention(
                    config = config,
                    key = f"model.layers.{idx}.self_attn",
                    layer_idx = idx,
                    hidden_size = config.hidden_size,
                    head_dim = config.head_dim,
                    num_q_heads = config.num_q_heads,
                    num_kv_heads = config.num_kv_heads,
                    rope_settings = config.rope_settings,
                    sm_scale = None,
                    key_q = "q_proj",
                    key_k = "k_proj",
                    key_v = "v_proj",
                    key_o = "o_proj",
                    qmap = "block.attn",
                ),
                mlp_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.post_attention_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                ),
                mlp = GatedMLP(
                    config = config,
                    key = f"model.layers.{idx}.mlp",
                    hidden_size = config.hidden_size,
                    intermediate_size = config.intermediate_size,
                    key_up = "up_proj",
                    key_gate = "gate_proj",
                    key_down = "down_proj",
                    qmap = "block.mlp",
                ),
            )
            for idx in range(config.num_hidden_layers)
        ]

        self.last_kv_module_idx = len(self.modules) - 1

        head_alt_key = None
        if config.tie_word_embeddings and not self.config.stc.has_tensor("lm_head"):
            head_alt_key = "model.embed_tokens"

        self.modules += [
            RMSNorm(
                config = config,
                key = "model.norm",
                rms_norm_eps = config.rms_norm_eps,
                out_dtype = torch.half,
            ),
            Linear(
                config = config,
                key = "lm_head",
                qbits_key = "head_bits",
                alt_key = head_alt_key,
                in_features = config.hidden_size,
                out_features = config.vocab_size,
                qmap = "block",
                caps = {"logits_output": True}
            )
        ]

        self.logit_layer_idx = len(self.modules) - 1


    @override
    def prepare_inputs(self, input_ids: torch.Tensor, params: dict) -> torch.Tensor:
        params["input_ids"] = input_ids
        input_ids = prepare_for_attn(input_ids, params)
        return input_ids
</content>

<content full_path="exllamav3/models/gemma2.py">
from __future__ import annotations
from typing_extensions import override
import torch
from .config import Config, no_default
from .model import Model
from ..util.rope import RopeSettings, RopeStyle
from ..modules import RMSNorm, Embedding, TransformerBlock, Attention, GatedMLP, Linear
from ..modules.attn import prepare_for_attn

class Gemma2Config(Config):
    arch_string = "Gemma2ForCausalLM"

    def __init__(
        self,
        directory: str,
        **kwargs,
    ):
        super().__init__(
            directory,
            {"text": Gemma2Model},
            **kwargs
        )

        # Attention params
        self.head_dim = self.read_cfg(int, "head_dim", None)
        self.hidden_size = self.read_cfg(int, "hidden_size", no_default)
        self.num_q_heads = self.read_cfg(int, "num_attention_heads", no_default)
        self.num_kv_heads = self.read_cfg(int, "num_key_value_heads", self.num_q_heads)

        self.query_pre_attn_scalar = self.read_cfg(float, "query_pre_attn_scalar", 224)
        self.attn_logit_softcapping = self.read_cfg(float, "attn_logit_softcapping", 50.0)
        self.sliding_window = self.read_cfg(int, "sliding_window_size", -1)

        if not self.head_dim:
            self.head_dim = self.hidden_size // self.num_q_heads

        # MLP params
        self.assert_cfg(str, "hidden_act", "gelu_pytorch_tanh", True)
        self.intermediate_size = self.read_cfg(int, "intermediate_size", no_default)

        # Norms
        self.rms_norm_eps = self.read_cfg(float, "rms_norm_eps", no_default)

        # Layers
        self.num_hidden_layers = self.read_cfg(int, "num_hidden_layers", no_default)
        self.tie_word_embeddings = self.read_cfg(bool, "tie_word_embeddings", False)

        # RoPE
        self.rope_settings = self.read_rope_settings_default(RopeStyle.NEOX)

        # Output softcap
        self.final_logit_softcapping = self.read_cfg(float, "final_logit_softcapping", 30.0)


class Gemma2Model(Model):
    config_class = Gemma2Config

    def __init__(
        self,
        config: Gemma2Config,
        **kwargs
    ):
        super().__init__(config, **kwargs)

        self.modules += [
            Embedding(
                config = config,
                key = "model.embed_tokens",
                vocab_size = config.vocab_size,
                hidden_size = config.hidden_size,
                normalize = True,
            )
        ]

        self.first_block_idx = len(self.modules)

        self.modules += [
            TransformerBlock(
                config = config,
                key = f"model.layers.{idx}",
                attn_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.input_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                    constant_bias = 1.0,
                ),
                attn = Attention(
                    config = config,
                    key = f"model.layers.{idx}.self_attn",
                    layer_idx = idx,
                    hidden_size = config.hidden_size,
                    head_dim = config.head_dim,
                    num_q_heads = config.num_q_heads,
                    num_kv_heads = config.num_kv_heads,
                    rope_settings = config.rope_settings,
                    sm_scale = config.query_pre_attn_scalar ** (-0.5),
                    logit_softcapping = config.attn_logit_softcapping,
                    sliding_window = config.sliding_window if not bool(idx % 2) else -1,
                    key_q = "q_proj",
                    key_k = "k_proj",
                    key_v = "v_proj",
                    key_o = "o_proj",
                    qmap = "block.attn",
                ),
                attn_post_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.post_attention_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                    constant_bias = 1.0,
                    out_dtype = torch.float,
                ),
                mlp_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.pre_feedforward_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                    constant_bias = 1.0,
                ),
                mlp = GatedMLP(
                    config = config,
                    key = f"model.layers.{idx}.mlp",
                    hidden_size = config.hidden_size,
                    intermediate_size = config.intermediate_size,
                    key_up = "up_proj",
                    key_gate = "gate_proj",
                    key_down = "down_proj",
                    qmap = "block.mlp",
                    activation_fn = "gelu"
                ),
                mlp_post_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.post_feedforward_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                    constant_bias = 1.0,
                    out_dtype = torch.float,
                ),
            )
            for idx in range(config.num_hidden_layers)
        ]

        self.last_kv_module_idx = len(self.modules) - 1

        self.modules += [
            RMSNorm(
                config = config,
                key = "model.norm",
                rms_norm_eps = config.rms_norm_eps,
                out_dtype = torch.half,
                constant_bias = 1.0,
            ),
            Linear(
                config = config,
                key = "lm_head",
                qbits_key = "head_bits",
                alt_key = "model.embed_tokens",
                in_features = config.hidden_size,
                out_features = config.vocab_size,
                qmap = "block",
                softcap = config.final_logit_softcapping,
                caps = {"logits_output": True}
            )
        ]

        self.logit_layer_idx = len(self.modules) - 1


    @override
    def prepare_inputs(self, input_ids: torch.Tensor, params: dict) -> torch.Tensor:
        params["input_ids"] = input_ids
        input_ids = prepare_for_attn(input_ids, params)
        return input_ids
</content>

<content full_path="exllamav3/models/glm4.py">
from __future__ import annotations
from typing_extensions import override
import torch
from .config import Config, no_default
from .model import Model
from ..util.rope import RopeSettings, RopeStyle
from ..modules import RMSNorm, Embedding, TransformerBlock, Attention, GatedMLP, Linear
from ..modules.attn import prepare_for_attn

class Glm4Config(Config):
    arch_string = "Glm4ForCausalLM"

    def __init__(
        self,
        directory: str,
        **kwargs,
    ):
        super().__init__(
            directory,
            {"text": Glm4Model},
            **kwargs
        )

        # Attention params
        self.head_dim = self.read_cfg(int, "head_dim", None)
        self.hidden_size = self.read_cfg(int, "hidden_size", no_default)
        self.num_q_heads = self.read_cfg(int, "num_attention_heads", no_default)
        self.num_kv_heads = self.read_cfg(int, "num_key_value_heads", self.num_q_heads)

        if not self.head_dim:
            self.head_dim = self.hidden_size // self.num_q_heads

        # MLP params
        self.assert_cfg(str, "hidden_act", "silu", True)
        self.intermediate_size = self.read_cfg(int, "intermediate_size", no_default)

        # Norms
        self.rms_norm_eps = self.read_cfg(float, "rms_norm_eps", no_default)

        # Layers
        self.num_hidden_layers = self.read_cfg(int, "num_hidden_layers", no_default)
        self.tie_word_embeddings = self.read_cfg(bool, "tie_word_embeddings", False)

        # RoPE
        self.rope_settings = self.read_rope_settings_default(RopeStyle.GPTJ)


class Glm4Model(Model):
    config_class = Glm4Config

    def __init__(
        self,
        config: Glm4Config,
        **kwargs
    ):
        super().__init__(config, **kwargs)

        self.modules += [
            Embedding(
                config = config,
                key = "model.embed_tokens",
                vocab_size = config.vocab_size,
                hidden_size = config.hidden_size,
            )
        ]

        self.first_block_idx = len(self.modules)

        self.modules += [
            TransformerBlock(
                config = config,
                key = f"model.layers.{idx}",
                attn_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.input_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                ),
                attn = Attention(
                    config = config,
                    key = f"model.layers.{idx}.self_attn",
                    layer_idx = idx,
                    hidden_size = config.hidden_size,
                    head_dim = config.head_dim,
                    num_q_heads = config.num_q_heads,
                    num_kv_heads = config.num_kv_heads,
                    rope_settings = config.rope_settings,
                    sm_scale = None,
                    key_q = "q_proj",
                    key_k = "k_proj",
                    key_v = "v_proj",
                    key_o = "o_proj",
                    qmap = "block.attn",
                ),
                attn_post_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.post_self_attn_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                    out_dtype = torch.float
                ),
                mlp_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.post_attention_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                ),
                mlp = GatedMLP(
                    config = config,
                    key = f"model.layers.{idx}.mlp",
                    hidden_size = config.hidden_size,
                    intermediate_size = config.intermediate_size,
                    key_up = "up_proj",
                    key_gate = "gate_proj",
                    key_down = "down_proj",
                    key_fused_gate_up = "gate_up_proj",
                    qmap = "block.mlp",
                    interm_dtype = torch.half,
                    out_dtype = torch.float,
                ),
                mlp_post_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.post_mlp_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                    out_dtype = torch.float
                ),
            )
            for idx in range(config.num_hidden_layers)
        ]

        self.last_kv_module_idx = len(self.modules) - 1

        head_alt_key = None
        if config.tie_word_embeddings and not self.config.stc.has_tensor("lm_head"):
            head_alt_key = "model.embed_tokens"

        self.modules += [
            RMSNorm(
                config = config,
                key = "model.norm",
                rms_norm_eps = config.rms_norm_eps,
                out_dtype = torch.half,
            ),
            Linear(
                config = config,
                key = "lm_head",
                qbits_key = "head_bits",
                alt_key = head_alt_key,
                in_features = config.hidden_size,
                out_features = config.vocab_size,
                qmap = "block",
                caps = {"logits_output": True}
            )
        ]

        self.logit_layer_idx = len(self.modules) - 1


    @override
    def prepare_inputs(self, input_ids: torch.Tensor, params: dict) -> torch.Tensor:
        params["input_ids"] = input_ids
        input_ids = prepare_for_attn(input_ids, params)
        return input_ids
</content>

<content full_path="exllamav3/models/architectures.py">
from .llama import LlamaModel
from .mistral import MistralModel
from .mixtral import MixtralModel
from .qwen2 import Qwen2Model
from .qwen3 import Qwen3Model
from .qwen3_moe import Qwen3MoeModel
from .phi3 import Phi3Model
from .gemma2 import Gemma2Model
from .gemma3 import Gemma3Model
from .decilm import DeciLMModel
from .glm4 import Glm4Model
from .cohere import CohereModel
from .cohere2 import Cohere2Model

ARCHITECTURES = {
    m.config_class.arch_string: {
        "architecture": m.config_class.arch_string,
        "config_class": m.config_class,
        "model_class": m,
    } for m in [
        LlamaModel,
        MistralModel,
        MixtralModel,
        Qwen2Model,
        Qwen3Model,
        Qwen3MoeModel,
        Phi3Model,
        Gemma2Model,
        Gemma3Model,
        DeciLMModel,
        Glm4Model,
        CohereModel,
        Cohere2Model,
    ]
}

def get_architectures():
    return ARCHITECTURES
</content>

<content full_path="exllamav3/models/cohere2.py">
from __future__ import annotations
from typing_extensions import override
import torch
from .config import Config, no_default
from .model import Model
from ..util.rope import RopeSettings, RopeStyle
from ..modules import LayerNorm, Embedding, ParallelDecoderBlock, Attention, GatedMLP, Linear
from ..modules.attn import prepare_for_attn

class Cohere2Config(Config):
    arch_string = "Cohere2ForCausalLM"

    def __init__(
        self,
        directory: str,
        **kwargs,
    ):
        super().__init__(
            directory,
            {"text": Cohere2Model},
            **kwargs
        )

        # Attention params
        self.head_dim = self.read_cfg(int, "head_dim", None)
        self.hidden_size = self.read_cfg(int, "hidden_size", no_default)
        self.num_q_heads = self.read_cfg(int, "num_attention_heads", no_default)
        self.num_kv_heads = self.read_cfg(int, "num_key_value_heads", self.num_q_heads)

        self.sliding_window = self.read_cfg(int, "sliding_window", -1)
        self.sliding_window_pattern = self.read_cfg(int, "sliding_window_pattern", 1)
        self.assert_cfg(str, "order_of_interleaved_layers", "local_attn_first")

        if not self.head_dim:
            self.head_dim = self.hidden_size // self.num_q_heads

        # MLP params
        self.assert_cfg(str, "hidden_act", "silu", True)
        self.intermediate_size = self.read_cfg(int, "intermediate_size", no_default)

        # Norms
        self.layernorm_eps = self.read_cfg(float, "layer_norm_eps", 1e-05)

        # Layers
        self.num_hidden_layers = self.read_cfg(int, "num_hidden_layers", no_default)
        self.tie_word_embeddings = self.read_cfg(bool, "tie_word_embeddings", True)

        # RoPE
        self.rope_settings = self.read_rope_settings_default(RopeStyle.GPTJ)

        # Logit scale
        self.logit_scale = self.read_cfg(float, "logit_scale", 0.0625)


class Cohere2Model(Model):
    config_class = Cohere2Config

    def __init__(
        self,
        config: Cohere2Config,
        **kwargs
    ):
        super().__init__(config, **kwargs)

        self.modules += [
            Embedding(
                config = config,
                key = "model.embed_tokens",
                vocab_size = config.vocab_size,
                hidden_size = config.hidden_size,
            )
        ]

        self.first_block_idx = len(self.modules)

        swa = [
            config.sliding_window if (idx + 1) % config.sliding_window_pattern != 0 else -1
            for idx in range(config.num_hidden_layers)
        ]

        self.modules += [
            ParallelDecoderBlock(
                config = config,
                key = f"model.layers.{idx}",
                input_norm = LayerNorm(
                    config = config,
                    key = f"model.layers.{idx}.input_layernorm",
                    layernorm_eps = config.layernorm_eps,
                ),
                attn = Attention(
                    config = config,
                    key = f"model.layers.{idx}.self_attn",
                    layer_idx = idx,
                    hidden_size = config.hidden_size,
                    head_dim = config.head_dim,
                    num_q_heads = config.num_q_heads,
                    num_kv_heads = config.num_kv_heads,
                    rope_settings = config.rope_settings if swa[idx] >= 0 else None,
                    sm_scale = None,
                    sliding_window = swa[idx],
                    key_q = "q_proj",
                    key_k = "k_proj",
                    key_v = "v_proj",
                    key_o = "o_proj",
                    qmap = "block.parallel",
                ),
                mlp = GatedMLP(
                    config = config,
                    key = f"model.layers.{idx}.mlp",
                    hidden_size = config.hidden_size,
                    intermediate_size = config.intermediate_size,
                    key_up = "up_proj",
                    key_gate = "gate_proj",
                    key_down = "down_proj",
                    qmap = "block.parallel",
                ),
            )
            for idx in range(config.num_hidden_layers)
        ]

        self.last_kv_module_idx = len(self.modules) - 1

        head_alt_key = None
        if config.tie_word_embeddings and not self.config.stc.has_tensor("lm_head"):
            head_alt_key = "model.embed_tokens"

        self.modules += [
            LayerNorm(
                config = config,
                key = "model.norm",
                layernorm_eps = config.layernorm_eps,
                out_dtype = torch.half,
            ),
            Linear(
                config = config,
                key = "lm_head",
                qbits_key = "head_bits",
                alt_key = head_alt_key,
                in_features = config.hidden_size,
                out_features = config.vocab_size,
                qmap = "block",
                caps = {"logits_output": True},
                post_scale = config.logit_scale
            )
        ]

        self.logit_layer_idx = len(self.modules) - 1


    @override
    def prepare_inputs(self, input_ids: torch.Tensor, params: dict) -> torch.Tensor:
        params["input_ids"] = input_ids
        input_ids = prepare_for_attn(input_ids, params)
        return input_ids
</content>

<content full_path="exllamav3/models/gemma3.py">
from __future__ import annotations
from typing_extensions import override
import torch
from .config import Config, no_default
from .model import Model
from ..util.rope import RopeSettings, RopeStyle
from ..modules import RMSNorm, Embedding, TransformerBlock, Attention, GatedMLP, Linear
from ..modules.attn import prepare_for_attn

class Gemma3Config(Config):
    arch_string = "Gemma3ForConditionalGeneration"

    def __init__(
        self,
        directory: str,
        **kwargs,
    ):
        super().__init__(
            directory,
            {"text": Gemma3Model, "vision": Gemma3VisionModel},
            **kwargs
        )

        # Gemma3 quirk, vocab size is implicit on HF versions
        if self.vocab_size is None:
            self.vocab_size = 262208

        # Attention params
        self.head_dim = self.read_cfg(int, "text_config->head_dim", 256)
        self.hidden_size = self.read_cfg(int, "text_config->hidden_size", 2304)
        self.num_q_heads = self.read_cfg(int, "text_config->num_attention_heads", 8)
        self.num_kv_heads = self.read_cfg(int, "text_config->num_key_value_heads", 4)

        self.query_pre_attn_scalar = self.read_cfg(float, "text_config->query_pre_attn_scalar", 256)
        self.attn_logit_softcapping = self.read_cfg(float, "text_config->attn_logit_softcapping", 0.0)
        self.sliding_window = self.read_cfg(int, "text_config->sliding_window", 4096)
        self.sliding_window_pattern = self.read_cfg(int, "text_config->sliding_window_pattern", 6)

        if not self.head_dim:
            self.head_dim = self.hidden_size // self.num_q_heads

        # MLP params
        self.intermediate_size = self.read_cfg(int, "text_config->intermediate_size", 9216)

        # Norms
        self.rms_norm_eps = self.read_cfg(float, "text_config->rms_norm_eps", 1e-6)

        # Layers
        self.num_hidden_layers = self.read_cfg(int, "text_config->num_hidden_layers", no_default)
        self.tie_word_embeddings = self.read_cfg(bool, "text_config->tie_word_embeddings", True)

        # RoPE
        self.rope_settings_global = self.read_rope_settings_default(
            RopeStyle.NEOX,
            default_rope_theta = 1e6,
            config_dict = self.read_cfg(dict, "text_config", no_default)
        )
        self.rope_settings_local = self.read_rope_settings_default(
            RopeStyle.NEOX,
            default_rope_theta = 1e4,
            config_dict = {}
        )

        # Output softcap
        self.final_logit_softcapping = self.read_cfg(float, "text_config->final_logit_softcapping", 0.0)


class Gemma3Model(Model):
    config_class = Gemma3Config

    def __init__(
        self,
        config: Gemma3Config,
        **kwargs
    ):
        super().__init__(config, **kwargs)

        self.modules += [
            Embedding(
                config = config,
                key = "language_model.model.embed_tokens",
                vocab_size = config.vocab_size,
                hidden_size = config.hidden_size,
                normalize = True,
            )
        ]

        self.first_block_idx = len(self.modules)

        is_local = [
            bool((idx + 1) % config.sliding_window_pattern)
            for idx in range(config.num_hidden_layers)
        ]

        self.modules += [
            TransformerBlock(
                config = config,
                key = f"language_model.model.layers.{idx}",
                attn_norm = RMSNorm(
                    config = config,
                    key = f"language_model.model.layers.{idx}.input_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                    constant_bias = 1.0,
                ),
                attn = Attention(
                    config = config,
                    key = f"language_model.model.layers.{idx}.self_attn",
                    layer_idx = idx,
                    hidden_size = config.hidden_size,
                    head_dim = config.head_dim,
                    num_q_heads = config.num_q_heads,
                    num_kv_heads = config.num_kv_heads,
                    rope_settings = config.rope_settings_local if is_local[idx] else config.rope_settings_global,
                    sm_scale = config.query_pre_attn_scalar ** (-0.5),
                    logit_softcapping = config.attn_logit_softcapping,
                    sliding_window = config.sliding_window if is_local[idx] else -1,
                    key_q = "q_proj",
                    key_k = "k_proj",
                    key_v = "v_proj",
                    key_o = "o_proj",
                    qmap = "block.attn",
                    q_norm = RMSNorm(
                        config = config,
                        key = f"language_model.model.layers.{idx}.self_attn.q_norm",
                        rms_norm_eps = config.rms_norm_eps,
                        constant_bias = 1.0,
                    ),
                    k_norm = RMSNorm(
                        config = config,
                        key = f"language_model.model.layers.{idx}.self_attn.k_norm",
                        rms_norm_eps = config.rms_norm_eps,
                        constant_bias = 1.0,
                    ),
                ),
                attn_post_norm = RMSNorm(
                    config = config,
                    key = f"language_model.model.layers.{idx}.post_attention_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                    constant_bias = 1.0,
                    out_dtype = torch.float,
                ),
                mlp_norm = RMSNorm(
                    config = config,
                    key = f"language_model.model.layers.{idx}.pre_feedforward_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                    constant_bias = 1.0,
                ),
                mlp = GatedMLP(
                    config = config,
                    key = f"language_model.model.layers.{idx}.mlp",
                    hidden_size = config.hidden_size,
                    intermediate_size = config.intermediate_size,
                    key_up = "up_proj",
                    key_gate = "gate_proj",
                    key_down = "down_proj",
                    qmap = "block.mlp",
                    activation_fn = "gelu"
                ),
                mlp_post_norm = RMSNorm(
                    config = config,
                    key = f"language_model.model.layers.{idx}.post_feedforward_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                    constant_bias = 1.0,
                    out_dtype = torch.float,
                ),
            )
            for idx in range(config.num_hidden_layers)
        ]

        self.last_kv_module_idx = len(self.modules) - 1

        self.modules += [
            RMSNorm(
                config = config,
                key = "language_model.model.norm",
                rms_norm_eps = config.rms_norm_eps,
                out_dtype = torch.half,
                constant_bias = 1.0,
            ),
            Linear(
                config = config,
                key = "language_model.lm_head",
                qbits_key = "head_bits",
                alt_key = "language_model.model.embed_tokens",
                in_features = config.hidden_size,
                out_features = config.vocab_size,
                qmap = "block",
                softcap = config.final_logit_softcapping,
                caps = {"logits_output": True}
            )
        ]

        self.logit_layer_idx = len(self.modules) - 1


    @override
    def prepare_inputs(self, input_ids: torch.Tensor, params: dict) -> torch.Tensor:
        params["input_ids"] = input_ids
        input_ids = prepare_for_attn(input_ids, params)
        return input_ids


class Gemma3VisionModel(Model):
    def __init__(
        self,
        config: Gemma3Config,
        **kwargs
    ):
        super().__init__(config, **kwargs)
        # TODO

    def load(self):
        raise NotImplementedError()  # TODO

    def unload(self):
        raise NotImplementedError()  # TODO

    @staticmethod
    @override
    def get_additional_compiled_tensors(config: Gemma3Config) -> dict:
        vlm_tensors = config.stc.list_tensors(prefix = "vision_tower")
        mmp_tensors = config.stc.list_tensors(prefix = "multi_modal_projector")
        return vlm_tensors | mmp_tensors
</content>

<content full_path="exllamav3/models/decilm.py">
from __future__ import annotations
from typing_extensions import override
import torch
from .config import Config, no_default
from .model import Model
from ..util.rope import RopeSettings, RopeStyle
from ..modules import RMSNorm, Embedding, TransformerBlock, Attention, GatedMLP, Linear
from ..modules.attn import prepare_for_attn

class DeciLMConfig(Config):
    arch_string = "DeciLMForCausalLM"

    def __init__(
        self,
        directory: str,
        **kwargs,
    ):
        super().__init__(
            directory,
            {"text": DeciLMModel},
            **kwargs
        )

        # Global attention params
        self.head_dim = self.read_cfg(int, "head_dim", None)
        self.hidden_size = self.read_cfg(int, "hidden_size", no_default)
        self.num_q_heads = self.read_cfg(int, "num_attention_heads", no_default)

        if not self.head_dim:
            self.head_dim = self.hidden_size // self.num_q_heads

        # MLP params
        self.assert_cfg(str, "hidden_act", "silu", True)

        # Norms
        self.rms_norm_eps = self.read_cfg(float, "rms_norm_eps", no_default)

        # Layers
        self.num_hidden_layers = self.read_cfg(int, "num_hidden_layers", no_default)
        self.tie_word_embeddings = self.read_cfg(bool, "tie_word_embeddings", False)

        # RoPE
        self.rope_settings = self.read_rope_settings_default(RopeStyle.NEOX)

        # Block configs
        self.block_configs = self.read_cfg(list, "block_configs", no_default)
        assert len(self.block_configs) == self.num_hidden_layers, \
            "Number of hidden layers does not match length of block_configs list"


class DeciLMModel(Model):
    config_class = DeciLMConfig

    def __init__(
        self,
        config: DeciLMConfig,
        **kwargs
    ):
        super().__init__(config, **kwargs)

        self.modules += [
            Embedding(
                config = config,
                key = "model.embed_tokens",
                vocab_size = config.vocab_size,
                hidden_size = config.hidden_size,
            )
        ]

        self.first_block_idx = len(self.modules)
        self.last_kv_module_idx = 0
        cache_layer_idx = 0

        for idx, cfg in enumerate(config.block_configs):
            cfg_attn = cfg["attention"]
            cfg_ffn = cfg["ffn"]

            if cfg_attn.get("no_op"):
                attn_norm = None
                attn = None
            else:
                assert not cfg_attn.get("num_sink_tokens"), "DeciLM: num_sink_tokens not supported"
                assert not cfg_attn.get("replace_with_linear"), "DeciLM: replace_with_linear not supported"
                assert not cfg_attn.get("sparsify"), "DeciLM: sparsify not supported"
                assert not cfg_attn.get("unshifted_sink"), "DeciLM: unshifted_sink not supported"
                assert not cfg_attn.get("use_prefill_window_in_sink_attention"), \
                    "DeciLM: use_prefill_window_in_sink_attention not supported"
                attn_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.input_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                )
                attn = Attention(
                    config = config,
                    key = f"model.layers.{idx}.self_attn",
                    layer_idx = cache_layer_idx,
                    hidden_size = config.hidden_size,
                    head_dim = config.head_dim,
                    num_q_heads = config.num_q_heads,
                    num_kv_heads = config.num_q_heads // cfg_attn["n_heads_in_group"],
                    rope_settings = config.rope_settings,
                    sm_scale = None,
                    key_q = "q_proj",
                    key_k = "k_proj",
                    key_v = "v_proj",
                    key_o = "o_proj",
                    qmap = "block.attn",
                )
                cache_layer_idx += 1
                self.last_kv_module_idx = len(self.modules)

            if cfg_ffn.get("no_op"):
                mlp_norm = None
                mlp = None
            else:
                assert not cfg_ffn.get("replace_with_linear"), "DeciLM: replace_with_linear not supported"
                assert not cfg_ffn.get("sparsify"), "DeciLM: sparsify not supported"
                mlp_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.post_attention_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                )
                interm_size = int(2 * cfg_ffn["ffn_mult"] * config.hidden_size / 3)
                interm_size = ((interm_size + 255) // 256) * 256
                mlp = GatedMLP(
                    config = config,
                    key = f"model.layers.{idx}.mlp",
                    hidden_size = config.hidden_size,
                    intermediate_size = interm_size,
                    key_up = "up_proj",
                    key_gate = "gate_proj",
                    key_down = "down_proj",
                    qmap = "block.mlp",
                )

            self.modules += [
                TransformerBlock(
                    config = config,
                    key = f"model.layers.{idx}",
                    attn_norm = attn_norm,
                    attn = attn,
                    mlp_norm = mlp_norm,
                    mlp = mlp,
                )
            ]

        head_alt_key = None
        if config.tie_word_embeddings and not self.config.stc.has_tensor("lm_head"):
            head_alt_key = "model.embed_tokens"

        self.modules += [
            RMSNorm(
                config = config,
                key = "model.norm",
                rms_norm_eps = config.rms_norm_eps,
                out_dtype = torch.half,
            ),
            Linear(
                config = config,
                key = "lm_head",
                qbits_key = "head_bits",
                alt_key = head_alt_key,
                in_features = config.hidden_size,
                out_features = config.vocab_size,
                qmap = "block",
                caps = {"logits_output": True}
            )
        ]

        self.logit_layer_idx = len(self.modules) - 1


    @override
    def prepare_inputs(self, input_ids: torch.Tensor, params: dict) -> torch.Tensor:
        params["input_ids"] = input_ids
        input_ids = prepare_for_attn(input_ids, params)
        return input_ids
</content>

<content full_path="exllamav3/models/qwen3.py">
from __future__ import annotations
from typing_extensions import override
import torch
from .config import Config, no_default
from .model import Model
from ..util.rope import RopeSettings, RopeStyle
from ..modules import RMSNorm, Embedding, TransformerBlock, Attention, GatedMLP, Linear
from ..modules.attn import prepare_for_attn

class Qwen3Config(Config):
    arch_string = "Qwen3ForCausalLM"

    def __init__(
        self,
        directory: str,
        **kwargs,
    ):
        super().__init__(
            directory,
            {"text": Qwen3Model},
            **kwargs
        )

        # Attention params
        self.head_dim = self.read_cfg(int, "head_dim", None)
        self.hidden_size = self.read_cfg(int, "hidden_size", no_default)
        self.num_q_heads = self.read_cfg(int, "num_attention_heads", no_default)
        self.num_kv_heads = self.read_cfg(int, "num_key_value_heads", self.num_q_heads)

        if not self.head_dim:
            self.head_dim = self.hidden_size // self.num_q_heads

        # MLP params
        self.assert_cfg(str, "hidden_act", "silu", True)
        self.intermediate_size = self.read_cfg(int, "intermediate_size", no_default)

        # Norms
        self.rms_norm_eps = self.read_cfg(float, "rms_norm_eps", no_default)

        # Layers
        self.num_hidden_layers = self.read_cfg(int, "num_hidden_layers", no_default)
        self.tie_word_embeddings = self.read_cfg(bool, "tie_word_embeddings", False)

        # RoPE
        self.rope_settings = self.read_rope_settings_default(RopeStyle.NEOX)


class Qwen3Model(Model):
    config_class = Qwen3Config

    def __init__(
        self,
        config: Qwen3Config,
        **kwargs
    ):
        super().__init__(config, **kwargs)

        self.modules += [
            Embedding(
                config = config,
                key = "model.embed_tokens",
                vocab_size = config.vocab_size,
                hidden_size = config.hidden_size,
            )
        ]

        self.first_block_idx = len(self.modules)

        self.modules += [
            TransformerBlock(
                config = config,
                key = f"model.layers.{idx}",
                attn_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.input_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                ),
                attn = Attention(
                    config = config,
                    key = f"model.layers.{idx}.self_attn",
                    layer_idx = idx,
                    hidden_size = config.hidden_size,
                    head_dim = config.head_dim,
                    num_q_heads = config.num_q_heads,
                    num_kv_heads = config.num_kv_heads,
                    rope_settings = config.rope_settings,
                    sm_scale = None,
                    key_q = "q_proj",
                    key_k = "k_proj",
                    key_v = "v_proj",
                    key_o = "o_proj",
                    qmap = "block.attn",
                    q_norm = RMSNorm(
                        config = config,
                        key = f"model.layers.{idx}.self_attn.q_norm",
                        rms_norm_eps = config.rms_norm_eps,
                    ),
                    k_norm = RMSNorm(
                        config = config,
                        key = f"model.layers.{idx}.self_attn.k_norm",
                        rms_norm_eps = config.rms_norm_eps,
                    ),
                ),
                mlp_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.post_attention_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                ),
                mlp = GatedMLP(
                    config = config,
                    key = f"model.layers.{idx}.mlp",
                    hidden_size = config.hidden_size,
                    intermediate_size = config.intermediate_size,
                    key_up = "up_proj",
                    key_gate = "gate_proj",
                    key_down = "down_proj",
                    qmap = "block.mlp",
                    interm_dtype = torch.half,
                    out_dtype = torch.float,
                ),
            )
            for idx in range(config.num_hidden_layers)
        ]

        self.last_kv_module_idx = len(self.modules) - 1

        head_alt_key = None
        if config.tie_word_embeddings and not self.config.stc.has_tensor("lm_head"):
            head_alt_key = "model.embed_tokens"

        self.modules += [
            RMSNorm(
                config = config,
                key = "model.norm",
                rms_norm_eps = config.rms_norm_eps,
                out_dtype = torch.half,
            ),
            Linear(
                config = config,
                key = "lm_head",
                qbits_key = "head_bits",
                alt_key = head_alt_key,
                in_features = config.hidden_size,
                out_features = config.vocab_size,
                qmap = "block",
                caps = {"logits_output": True}
            )
        ]

        self.logit_layer_idx = len(self.modules) - 1


    @override
    def prepare_inputs(self, input_ids: torch.Tensor, params: dict) -> torch.Tensor:
        params["input_ids"] = input_ids
        input_ids = prepare_for_attn(input_ids, params)
        return input_ids
</content>

<content full_path="exllamav3/models/cohere.py">
from __future__ import annotations
from typing_extensions import override
import torch
from .config import Config, no_default
from .model import Model
from ..util.rope import RopeSettings, RopeStyle
from ..modules import LayerNorm, Embedding, ParallelDecoderBlock, Attention, GatedMLP, Linear
from ..modules.attn import prepare_for_attn

class CohereConfig(Config):
    arch_string = "CohereForCausalLM"

    def __init__(
        self,
        directory: str,
        **kwargs,
    ):
        super().__init__(
            directory,
            {"text": CohereModel},
            **kwargs
        )

        # Attention params
        self.head_dim = self.read_cfg(int, "head_dim", None)
        self.hidden_size = self.read_cfg(int, "hidden_size", no_default)
        self.num_q_heads = self.read_cfg(int, "num_attention_heads", no_default)
        self.num_kv_heads = self.read_cfg(int, "num_key_value_heads", self.num_q_heads)

        if not self.head_dim:
            self.head_dim = self.hidden_size // self.num_q_heads

        self.use_qk_norm = self.read_cfg(int, "use_qk_norm", False)

        # MLP params
        self.assert_cfg(str, "hidden_act", "silu", True)
        self.intermediate_size = self.read_cfg(int, "intermediate_size", no_default)

        # Norms
        self.layernorm_eps = self.read_cfg(float, "layer_norm_eps", 1e-05)

        # Layers
        self.num_hidden_layers = self.read_cfg(int, "num_hidden_layers", no_default)
        self.tie_word_embeddings = self.read_cfg(bool, "tie_word_embeddings", True)

        # RoPE
        self.rope_settings = self.read_rope_settings_default(RopeStyle.GPTJ)

        # Logit scale
        self.logit_scale = self.read_cfg(float, "logit_scale", 0.0625)


class CohereModel(Model):
    config_class = CohereConfig

    def __init__(
        self,
        config: CohereConfig,
        **kwargs
    ):
        super().__init__(config, **kwargs)

        self.modules += [
            Embedding(
                config = config,
                key = "model.embed_tokens",
                vocab_size = config.vocab_size,
                hidden_size = config.hidden_size,
            )
        ]

        self.first_block_idx = len(self.modules)

        self.modules += [
            ParallelDecoderBlock(
                config = config,
                key = f"model.layers.{idx}",
                input_norm = LayerNorm(
                    config = config,
                    key = f"model.layers.{idx}.input_layernorm",
                    layernorm_eps = config.layernorm_eps,
                ),
                attn = Attention(
                    config = config,
                    key = f"model.layers.{idx}.self_attn",
                    layer_idx = idx,
                    hidden_size = config.hidden_size,
                    head_dim = config.head_dim,
                    num_q_heads = config.num_q_heads,
                    num_kv_heads = config.num_kv_heads,
                    rope_settings = config.rope_settings,
                    sm_scale = None,
                    key_q = "q_proj",
                    key_k = "k_proj",
                    key_v = "v_proj",
                    key_o = "o_proj",
                    qmap = "block.parallel",
                    q_norm = LayerNorm(
                        config = config,
                        key = f"model.layers.{idx}.self_attn.q_norm",
                        layernorm_eps = config.layernorm_eps,
                    ) if config.use_qk_norm else None,
                    k_norm = LayerNorm(
                        config = config,
                        key = f"model.layers.{idx}.self_attn.k_norm",
                        layernorm_eps = config.layernorm_eps,
                    ) if config.use_qk_norm else None,
                ),
                mlp = GatedMLP(
                    config = config,
                    key = f"model.layers.{idx}.mlp",
                    hidden_size = config.hidden_size,
                    intermediate_size = config.intermediate_size,
                    key_up = "up_proj",
                    key_gate = "gate_proj",
                    key_down = "down_proj",
                    qmap = "block.parallel",
                    out_dtype = torch.float,
                ),
            )
            for idx in range(config.num_hidden_layers)
        ]

        self.last_kv_module_idx = len(self.modules) - 1

        head_alt_key = None
        if config.tie_word_embeddings and not self.config.stc.has_tensor("lm_head"):
            head_alt_key = "model.embed_tokens"

        self.modules += [
            LayerNorm(
                config = config,
                key = "model.norm",
                layernorm_eps = config.layernorm_eps,
                out_dtype = torch.half,
            ),
            Linear(
                config = config,
                key = "lm_head",
                qbits_key = "head_bits",
                alt_key = head_alt_key,
                in_features = config.hidden_size,
                out_features = config.vocab_size,
                qmap = "block",
                caps = {"logits_output": True},
                post_scale = config.logit_scale
            )
        ]

        self.logit_layer_idx = len(self.modules) - 1


    @override
    def prepare_inputs(self, input_ids: torch.Tensor, params: dict) -> torch.Tensor:
        params["input_ids"] = input_ids
        input_ids = prepare_for_attn(input_ids, params)
        return input_ids
</content>

<content full_path="exllamav3/models/qwen2.py">

from .llama import LlamaConfig, LlamaModel

# Qwen2 is identical to Llama except for bias on Q, K and V projections, but Linear module automatically
# detects *.bias tensor

class Qwen2Config(LlamaConfig):
    arch_string = "Qwen2ForCausalLM"

    def __init__(
        self,
        directory: str,
        **kwargs,
    ):
        super().__init__(
            directory,
            derived_model = {"text": Qwen2Model},
            **kwargs
        )


class Qwen2Model(LlamaModel):
    config_class = Qwen2Config

    def __init__(
        self,
        config: Qwen2Config,
        **kwargs
    ):
        super().__init__(config, **kwargs)

</content>

<content full_path="exllamav3/models/mixtral.py">
from __future__ import annotations
from typing_extensions import override
import torch
from .config import Config, no_default
from .model import Model
from ..util.rope import RopeSettings, RopeStyle
from ..modules import RMSNorm, Embedding, TransformerBlock, Attention, BlockSparseMLP, Linear
from ..modules.attn import prepare_for_attn

class MixtralConfig(Config):
    arch_string = "MixtralForCausalLM"

    def __init__(
        self,
        directory: str,
        **kwargs,
    ):
        super().__init__(
            directory,
            {"text": MixtralModel},
            **kwargs
        )

        # Attention params
        self.head_dim = self.read_cfg(int, "head_dim", None)
        self.hidden_size = self.read_cfg(int, "hidden_size", no_default)
        self.num_q_heads = self.read_cfg(int, "num_attention_heads", no_default)
        self.num_kv_heads = self.read_cfg(int, "num_key_value_heads", self.num_q_heads)

        if not self.head_dim:
            self.head_dim = self.hidden_size // self.num_q_heads

        # MLP params
        self.assert_cfg(str, "hidden_act", "silu", True)
        self.moe_intermediate_size = self.read_cfg(int, "intermediate_size", no_default)
        self.num_experts = self.read_cfg(int, "num_local_experts", no_default)
        self.num_experts_per_tok = self.read_cfg(int, "num_experts_per_tok", no_default)

        # Norms
        self.rms_norm_eps = self.read_cfg(float, "rms_norm_eps", no_default)

        # Layers
        self.num_hidden_layers = self.read_cfg(int, "num_hidden_layers", no_default)
        self.tie_word_embeddings = self.read_cfg(bool, "tie_word_embeddings", False)

        # RoPE
        self.rope_settings = self.read_rope_settings_default(RopeStyle.NEOX)


class MixtralModel(Model):
    config_class = MixtralConfig

    def __init__(
        self,
        config: MixtralConfig,
        **kwargs
    ):
        super().__init__(config, **kwargs)

        self.modules += [
            Embedding(
                config = config,
                key = "model.embed_tokens",
                vocab_size = config.vocab_size,
                hidden_size = config.hidden_size,
            )
        ]

        self.first_block_idx = len(self.modules)

        self.modules += [
            TransformerBlock(
                config = config,
                key = f"model.layers.{idx}",
                attn_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.input_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                ),
                attn = Attention(
                    config = config,
                    key = f"model.layers.{idx}.self_attn",
                    layer_idx = idx,
                    hidden_size = config.hidden_size,
                    head_dim = config.head_dim,
                    num_q_heads = config.num_q_heads,
                    num_kv_heads = config.num_kv_heads,
                    rope_settings = config.rope_settings,
                    sm_scale = None,
                    key_q = "q_proj",
                    key_k = "k_proj",
                    key_v = "v_proj",
                    key_o = "o_proj",
                    qmap = "block.attn",
                    out_dtype = torch.float
                ),
                mlp_norm = RMSNorm(
                    config = config,
                    key = f"model.layers.{idx}.post_attention_layernorm",
                    rms_norm_eps = config.rms_norm_eps,
                ),
                mlp = BlockSparseMLP(
                    config = config,
                    key = f"model.layers.{idx}.block_sparse_moe",
                    hidden_size = config.hidden_size,
                    intermediate_size = config.moe_intermediate_size,
                    num_experts = self.config.num_experts,
                    num_experts_per_tok = self.config.num_experts_per_tok,
                    key_up = "experts.{expert_idx}.w3",
                    key_gate = "experts.{expert_idx}.w1",
                    key_down = "experts.{expert_idx}.w2",
                    key_routing_gate = "gate",
                    qmap = "block.mlp",
                    interm_dtype = torch.half,
                    out_dtype = torch.half,
                ),
            )
            for idx in range(config.num_hidden_layers)
        ]

        self.last_kv_module_idx = len(self.modules) - 1

        head_alt_key = None
        if config.tie_word_embeddings and not self.config.stc.has_tensor("lm_head"):
            head_alt_key = "model.embed_tokens"

        self.modules += [
            RMSNorm(
                config = config,
                key = "model.norm",
                rms_norm_eps = config.rms_norm_eps,
                out_dtype = torch.half,
            ),
            Linear(
                config = config,
                key = "lm_head",
                qbits_key = "head_bits",
                alt_key = head_alt_key,
                in_features = config.hidden_size,
                out_features = config.vocab_size,
                qmap = "block",
                caps = {"logits_output": True}
            )
        ]

        self.logit_layer_idx = len(self.modules) - 1


    @override
    def prepare_inputs(self, input_ids: torch.Tensor, params: dict) -> torch.Tensor:
        params["input_ids"] = input_ids
        input_ids = prepare_for_attn(input_ids, params)
        return input_ids
</content>

<content full_path="exllamav3/generator/job.py">
from __future__ import annotations
import torch
from dataclasses import dataclass
from typing import TYPE_CHECKING
from ..cache.cache import Cache
if TYPE_CHECKING:
    from .generator import Generator
from ..constants import PAGE_SIZE
import numpy as np
from .pagetable import CachePage, Sequence, tensor_hash_checksum, random_hash
import random
from collections import deque
import time
from ..ext import exllamav3_ext as ext
from .sampler import Sampler, DefaultSampler
from ..util.tensor import SeqTensor
from ..util import profile_opt

# Convert list of strings to UTF32 format to pass by reference to partial matching function
def _strings_to_utf32(strings: list[str]) -> tuple[np.ndarray, np.ndarray] | None:

    if not strings: return bytearray(), None

    encoded_strings = [s.encode("utf-32-le") for s in strings]
    encoded_lengths = [len(s) for s in encoded_strings]
    offsets = [0] + encoded_lengths
    for i in range(1, len(offsets)):
        offsets[i] += offsets[i - 1]
    total_length = offsets[-1]
    concat_strings = bytearray(total_length)
    for s, offset in zip(encoded_strings, offsets[:-1]):
        concat_strings[offset:offset + len(s)] = s

    concat_strings = np.frombuffer(concat_strings, dtype = np.uint8)
    offsets = np.frombuffer(np.array(offsets, dtype = np.int32), dtype = np.uint8)
    return concat_strings, offsets


class Job:

    def __init__(
        self,
        input_ids: torch.Tensor | list[torch.Tensor],
        max_new_tokens: int | None = None,
        min_new_tokens: int = 0,
        max_skips: int | None = 4,
        sampler: Sampler | None = None,
        seed: int = None,
        stop_conditions: list | tuple | set | None = None,
        decode_special_tokens: bool = False,
        return_top_tokens: int = 0,
        return_logits: bool = False,
        return_probs: bool = False,
#        filters: list[Filter] | None = None,
#        filter_prefer_eos: bool = False,
        token_healing: bool = False,
        identifier: object | None = None,
        banned_strings: list[str] | None = None,
#        embeddings: list[MMEmbedding] | None = None,
        **kwargs
    ):
        """
        Create new job.

        :param input_ids:
            Tokenized IDs of the input prompt, shape (1, n). Alternatively, list of tokenized IDs to inference on
            seperately but sample collectively (e.g. CFG prompt pair)

        :param max_new_tokens:
            Max no. output tokens to allow

        :param min_new_tokens:
            Minimum number of tokens to generate before stop tokens become active. Until this number have been
            sampled, stop tokens are suppressed but stop strings will still end response. May produce garbage output.

        :param max_skips:
            In the event that the job is too large to fit in the cache at any given moment but there are
            smaller jobs pending that would fit, those smaller jobs are started instead. This number
            specifies the maximum number of times a job can be skipped over in favor of a smaller job before it
            stalls the queue. After this, the job is guaranteed to be the next job started.

        :param sampler:
            Sampler

        :param seed:
            RNG seed (determinism is not guaranteed)

        :param stop_conditions:
            List of strings and/or token IDs that will trigger the EOS condition. If a stop condition is
            encountered it is not emitted as output. If the beginning of a stop string is sampled, stream output
            will be held until the stop condition can be resolved.

        :param decode_special_tokens:
            If True, special tokens like <|im_start|> etc. will be decoded and included in the text output.
            If False, special tokens will still be respected as stop conditions.

        :param return_top_tokens:
            Number of top tokens to return, along with their final sampling probabilities. There is some
            performance penalty for enabling this.

        :param return_logits:
            Return pre-sampling logits along with output tokens.

        :param return_probs:
            Return final sampling probability for each chosen token.

        # :param filters:
        #     List of Filters to apply during generation. TODO

        # :param filter_prefer_eos:
        #     If True, the sampler will prefer whatever token the filter presents as an EOS condition, e.g.
        #     the outer closing bracket in a JSON grammar, as soon as that (sub)token is legal under the TODO
        #     grammar.

        :param token_healing:
            Resample the last token of the input with a prefix constraint. E.g. if the last token is
            "_Hel", it is removed from the input and the first token of the output will be constrained to
            one of "_Hello", "_Help", "_Helium", etc. Only the added part of the healed token is emitted as
            text, i.e. "lo", "p", "ium" etc.

        :param identifier:
            Arbitrary object to return with every stream event relating to this job, e.g. an index to identify the
            output as belonging to a specific position in a batch

        # :param embeddings:
        #     Optional list of MMEmbeddings to use, or list of lists for batched generation TODO

        :param kwargs:
        """

        assert all(ids.device.type == "cpu" for ids in input_ids), \
            "input_ids must reside in system memory"

        self.generator = None
        self.pagetable = None
        self.serial_number = None
        self.identifier = identifier

        self.max_skips = max_skips
        self.skips = 0
        self.all_unique_hashes = None

        # Default sampler settings
        if sampler is None:
            sampler = DefaultSampler()

        # Sampling state
        self.held_text = ""
        self.held_tokens = None
        self.held_k_tokens = None
        self.held_k_probs = None
        self.held_probs = None
        self.held_logits = None
        self.full_completion = ""

        # Prepare sequences
        if not isinstance(input_ids, list):
            input_ids = [input_ids]

        if token_healing and all(ids.shape[-1] > 1 for ids in input_ids):
            input_seq_ids = [ids[:, :-1] for ids in input_ids]
            self.prefix_token = torch.cat([ids[:, -1:] for ids in input_ids], dim = 0)
        else:
            input_seq_ids = input_ids
            self.prefix_token = None

        self.sequences = []
        for ids, seq_ids in zip(input_ids, input_seq_ids):
            assert ids.shape[-1] > 0, \
                "Input IDs cannot be empty."
            assert ids.shape[0] == 1, \
                "input_ids must be [1, seq_len] tensor or list of [1, seq_len] tensors"
            seq = Sequence(ids, seq_ids)
            self.sequences.append(seq)

        # Generation parameters
        self.max_new_tokens = max_new_tokens or 100
        self.min_new_tokens = min_new_tokens
        self.new_tokens = 0 if self.prefix_token is None else -1
        self.sampler = sampler
        self.rng = random.Random() if seed is None else random.Random(seed)

        # Output options
        self.decode_special_tokens = decode_special_tokens
        self.return_top_tokens = return_top_tokens
        self.return_logits = return_logits
        self.return_probs = return_probs

        # Stop conditions
        self.stop_strings = set()
        self.stop_tokens = set()
        if stop_conditions is not None:
            for t in stop_conditions:
                if isinstance(t, int):
                    self.stop_tokens.add(t)
                elif isinstance(t, str):
                    self.stop_strings.add(t)
                else:
                    raise ValueError("Unsupported type in stop_conditions")
            self.stop_strings_utf32_buffer, self.stop_strings_utf32_offsets = \
                _strings_to_utf32(list(self.stop_strings))
        else:
            self.stop_strings_utf32_buffer, self.stop_strings_utf32_offsets = None, None

        self.stop_tokens_list = list(self.stop_tokens)
        self.stop_strings_list = list(self.stop_strings)

        # Banned strings
        if banned_strings:
            # TODO: (filters)
            # assert filters is None or len(filters) == 0, \
            #     "Cannot combine banned strings with filters"
            self.banned_strings = [s.lower() for s in banned_strings]
            self.banned_strings_utf32_buffer, self.banned_strings_utf32_offsets = \
                _strings_to_utf32(self.banned_strings)
        else:
            self.banned_strings = []
            self.banned_strings_utf32_buffer, self.banned_strings_utf32_offsets = None, None

        self.checkpoint = None

        # Metrics
        self.time_enqueue = None
        self.time_first_prefill = None
        self.time_first_token = None
        self.time_last_token = None
        self.accepted_draft_tokens = 0
        self.rejected_draft_tokens = 0
        self.cached_pages = 0
        self.cached_tokens = 0
        self.is_finished = False
        self.non_sequential_pages = 0
        self.total_pages = 0

        # Filters
        # TODO: (filters)
        # self.filters = filters if filters is not None else []
        # self.filter_prefer_eos = filter_prefer_eos

        # Embeddings
        # TODO: (embeddings)
        # self.embeddings = embeddings or []
        # self.alt_rope_embed = {}
        # self.alt_rope_offset = 0

        # Pinned buffer for IDs during sampling
        self.current_pinned_ids = None
        if self.sampler.reqs_past_ids:
            max_ids = max(len(seq.sequence_ids) for seq in self.sequences) + self.max_new_tokens + 8
            self.pinned_ids = torch.empty((1, max_ids), dtype = torch.long, pin_memory = True)


    def __repr__(self):
        if self.serial_number is None:
            return "Generator job (new)"
        else:
            return f"Generator job #{self.serial_number}"


    def is_prefill_done(self):
        return all(seq.kv_position == len(seq.sequence_ids) - 1 for seq in self.sequences)


    def get_max_seq_len(self):
        if not self.is_prefill_done():
            return 0
        max_seq_len = 0
        for seq in self.sequences:
            if seq.kv_position == len(seq.sequence_ids) - 1:
                max_seq_len = max(max_seq_len, len(seq.sequence_ids))
        return max_seq_len


    def get_input_ids_list(
        self,
        draft_tokens: torch.Tensor | None = None,
        idx: int = 0,
        add_to_cache: bool = False
    ):
        input_ids_list = []
        for seq in self.sequences:
            ids = seq.sequence_ids.torch_slice(seq.kv_position, None)
            if draft_tokens is not None:
                ids = torch.cat((ids, draft_tokens[idx:idx + 1, :]), dim = -1)
            input_ids_list.append(ids)
            if add_to_cache:
                tokens_to_add = ids.shape[-1]
                skvp = seq.kv_position
                while tokens_to_add:
                    page = seq.allocated_pages[skvp // PAGE_SIZE]
                    assert page.ref_count == 1
                    tokens_page = min(tokens_to_add, PAGE_SIZE - page.kv_position)
                    page.sequence[:, page.kv_position:page.kv_position + tokens_page] = ids[:, :tokens_page]
                    page.kv_position += tokens_page
                    skvp += tokens_page
                    ids = ids[:, tokens_page:]
                    tokens_to_add -= tokens_page
                    page.can_revert = False
        return input_ids_list


    def receive_logits(
        self,
        logits: torch.Tensor,
    ):
        # TODO: (cfg)
        # assert logits.shape[0] == len(self.sequences) == (2 if self.gen_settings.cfg_scale is not None else 1)
        # assert logits.shape[0] == len(self.sequences)
        # assert self.is_prefill_done()
        # assert all(seq.live for seq in self.sequences)

        # Start filters
        # TODO: (filters)
        # if self.new_tokens == 0:
        #     for f in self.filters:
        #         f.background_drop()
        #         f.begin("")

        # Sample

        blocked_tokens = (
            self.checkpoint["explored_tokens"] if self.checkpoint and self.checkpoint["offset"] == 0
            else None
        )

        if self.new_tokens < self.min_new_tokens:
            blocked_tokens = blocked_tokens + self.stop_tokens_list if blocked_tokens else self.stop_tokens_list

        # TODO: logit mask tensor for blocked/allowed/prefix tokens
        allowed_tokens = None
        if self.prefix_token is not None and self.new_tokens == -1:
            allowed_tokens = self.generator.tokenizer.get_tokens_with_prefix_id(self.prefix_token)

        next_token = self.sampler.forward(
            logits,
            self.current_pinned_ids,
            self.rng.randint(0, (1<<32)-1),
            self.generator.tokenizer,
            blocked_tokens = blocked_tokens,
            allowed_tokens = allowed_tokens,
        )

        next_prob, next_k_tokens, next_k_probs = None, None, None

        if self.return_probs or self.return_top_tokens > 0:
            probs = torch.softmax(logits.float(), dim = -1)

            if self.return_probs:
                next_prob = torch.gather(probs.squeeze(0), dim = 1, index = next_token)

            if self.return_top_tokens > 0:
                sorted_probs, sorted_indices = torch.sort(probs, dim = -1, descending = True)
                next_k_tokens = sorted_indices[:, :, :self.return_top_tokens]
                next_k_probs = sorted_probs[:, :, :self.return_top_tokens]

        return next_token, next_k_tokens, next_k_probs, next_prob


    def receive_sample(
        self,
        logits: torch.Tensor | None,
        next_token: torch.Tensor | None,
        next_k_tokens: torch.Tensor | None,
        next_k_probs: torch.Tensor | None,
        next_prob: torch.Tensor | None,
        # filter_eos: bool | None,
        results: list,
        first_sample_in_sd_batch: bool = True
    ):
        next_token = next_token.cpu()

        # Feed filters

        # TODO: (filters)
        # if self.new_tokens >= 0:
        #     all_mask = True
        #     for f in self.filters:
        #         f.feed(next_token)
        #         if not f.can_mask_logits() or not f.use_background_worker():
        #             all_mask = False
        #     if first_sample_in_sd_batch and self.generator.filter_queue is not None:
        #         if all_mask:
        #             # Using logit mask(s)
        #             for f in self.filters:
        #                 self.generator.filter_queue.append((f, True))
        #         else:
        #             # Using allowed token list(s)
        #             for f in self.filters:
        #                 if f.use_background_worker():
        #                     self.generator.filter_queue.append((f, False))

        # Accept token
        self.new_tokens += 1

        for seq in self.sequences:

            # Accept new token
            seq.sequence_ids.append(next_token)
            page_before = seq.kv_position // PAGE_SIZE
            seq.kv_position += 1
            pos = seq.kv_position
            if self.checkpoint:
                pos -= self.checkpoint["offset"]
            page_after = pos // PAGE_SIZE

            # Hash completed page
            if page_after > page_before:
                assert page_after == page_before + 1

                page = seq.allocated_pages[page_before]

                if page_before > 0:
                    last_page = seq.allocated_pages[page_before - 1]
                    last_hash = last_page.phash
                else:
                    last_hash = None

                page_ids = seq.sequence_ids.torch_slice(page_before * PAGE_SIZE, page_after * PAGE_SIZE)
                new_hash = tensor_hash_checksum(page_ids, last_hash)

                # If another referenced page has the same hash, switch to referencing that instead
                if new_hash in self.pagetable.referenced_pages:
                    new_serial = page.access_serial
                    page.sub_ref()
                    page = self.pagetable.referenced_pages[new_hash]
                    assert page.kv_position == PAGE_SIZE
                    seq.allocated_pages[page_before] = page
                    seq.build_block_index_tensor()
                    page.add_ref(new_serial)

                else:

                    # If an unreferenced page has the same hash, clear that page
                    if new_hash in self.pagetable.unreferenced_pages:
                        up = self.pagetable.unreferenced_pages[new_hash]
                        up.clear()

                    # Update the hash
                    page.update_hash(new_hash)

                page = seq.allocated_pages[page_after]
                page.prev_hash = new_hash
                page.can_revert = False

        # Stream output

        def emit(
            results_: list,
            emit_eos: bool = False,
            eos_reason: str = None,
            emit_held = False,
            suppressed_text = None,
            suppressed_tokens = None,
            stop_token: int = None,
            stop_string: str = None,
            rem_held_text: str = None
        ):
            r = {
                "job": self,
                "stage": "streaming",
                "eos": emit_eos,
                "serial": self.serial_number,
            }

            r = {
                "job": self,
                "stage": "streaming",
                "eos": emit_eos,
                "serial": self.serial_number,
            }

            if eos_reason is not None:
                r.update({ "eos_reason": eos_reason })
                if eos_reason == "stop_token":
                    id_to_piece = self.generator.tokenizer.get_id_to_piece_list(True)
                    r.update({
                        "eos_triggering_token_id": stop_token,
                        "eos_triggering_token_str": id_to_piece[stop_token]
                    })
                    pass
                if eos_reason == "stop_string":
                    r.update({ "eos_triggering_string": stop_string })

            if emit_held:
                if self.held_text != "":
                    self.full_completion += self.held_text
                    r.update({ "text": self.held_text })
                    self.held_text = ""
                if self.held_tokens:
                    r.update({ "token_ids": self.held_tokens.torch().clone() })
                    self.held_tokens.clear()
                if self.held_probs:
                    r.update({ "token_probs": self.held_probs.torch().clone() })
                    self.held_probs.clear()
                if self.held_k_tokens:
                    r.update({ "top_k_tokens": self.held_k_tokens.torch().clone() })
                    r.update({ "top_k_probs": self.held_k_probs.torch().clone() })
                    self.held_k_tokens.clear()
                    self.held_k_probs.clear()
                if self.held_logits:
                    r.update({ "logits": self.held_logits.torch().clone() })
                    self.held_logits.clear()

            if suppressed_text:
                r.update({ "suppressed_text": suppressed_text })
                r.update({ "suppressed_tokens": suppressed_tokens.torch() })

            if emit_eos:
                self.is_finished = True
                self.time_last_token = time.time()
                r.update({
                    "full_completion": self.full_completion,
                    "new_tokens": self.new_tokens,
                    "prompt_tokens": len(self.sequences[0].input_ids),
                    "time_enqueued": self.time_first_prefill - self.time_enqueue,
                    "time_prefill": self.time_first_token - self.time_first_prefill,
                    "time_generate": self.time_last_token - self.time_first_token,
                    "cached_pages": self.cached_pages // len(self.sequences),
                    "cached_tokens": (self.cached_pages * PAGE_SIZE + self.cached_tokens) // len(self.sequences),
                })
                if self.generator.draft_model:
                    r.update({
                        "accepted_draft_tokens": self.accepted_draft_tokens,
                        "rejected_draft_tokens": self.rejected_draft_tokens
                    })
                if eos_reason == "stop_string":
                    self.held_text = rem_held_text
                rh = {}
                if self.held_text:
                    rh.update({ "text": self.held_text })
                if self.held_tokens:
                    rh.update({ "token_ids": self.held_tokens.torch().clone() })
                if self.held_probs:
                    rh.update({ "token_probs": self.held_probs.torch().clone() })
                if self.held_k_tokens:
                    rh.update({ "top_k_tokens": self.held_k_tokens.torch().clone() })
                    rh.update({ "top_k_probs": self.held_k_probs.torch().clone() })
                if self.held_logits:
                    rh.update({ "logits": self.held_logits.torch().clone() })
                if rh:
                    r.update({ "held": rh })

            if self.identifier is not None:
                r.update({ "identifier": self.identifier })

            results_.append(r)
            return emit_eos, next_token

        # Decode and buffer output
        id_to_piece = self.generator.tokenizer.get_id_to_piece_list(self.decode_special_tokens)
        new_text = id_to_piece[next_token.item()]

        if self.new_tokens == 0:
            unhealed = id_to_piece[self.prefix_token[0].item()]
            new_text = new_text[len(unhealed):]

        self.held_text += new_text
        self.held_tokens.append(next_token)
        if self.return_probs:
            self.held_probs.append(next_prob)
        if self.return_top_tokens > 0:
            self.held_k_tokens.append(next_k_tokens)
            self.held_k_probs.append(next_k_probs)
        if self.return_logits:
            self.held_logits.append(logits[:1, :, :])

        # End on stop tokens
        if next_token.item() in self.stop_tokens:
            return emit(results, emit_eos = True, eos_reason = "stop_token", stop_token = next_token.item())

        # Stop if we reach max_new_tokens
        if self.new_tokens >= self.max_new_tokens - self.generator.num_draft_tokens:
            return emit(results, emit_eos = True, emit_held = True, eos_reason = "max_new_tokens")

        # End now if newly added token ends a filter
        # if filter_eos:  # TODO: (filters)
        #     return emit(results, emit_eos = True, emit_held = True, eos_reason = "end_filter")

        # Hold text if it contains an incomplete character
        if 1 <= self.held_text.count("�") < 5:
            test_decode = self.generator.tokenizer.decode(
                self.held_tokens.torch(),
                decode_special_tokens = self.decode_special_tokens
            )[0]
            if not "�" in test_decode:
                self.held_text = test_decode
            else:
                # Don't hold forever if a broken generation yields a replacement character but never completes
                # the Unicode symbol
                return emit(results, emit_held = (len(test_decode) > 20))

        # Hold text as long as it contains part of a banned string

        def unset_checkpoint():
            self.checkpoint = None

        def set_checkpoint():
            if self.checkpoint is None:
                self.checkpoint = {
                    "offset": 1,
                    "held_text": self.held_text[:-len(new_text)],
                    "held_tokens": self.held_tokens.clone(1),
                    "held_probs": self.held_probs.clone(1),
                    "held_k_tokens": self.held_k_tokens.clone(1),
                    "held_k_probs": self.held_k_probs.clone(1),
                    "held_logits": self.held_logits.clone(1),
                    "explored_tokens": [next_token.item()],
                }
            else:
                self.checkpoint["offset"] += 1
                if self.checkpoint["offset"] == 1:
                    self.checkpoint["explored_tokens"].append(next_token.item())

        def rewind_checkpoint():
            assert self.checkpoint is not None
            offset = self.checkpoint["offset"]
            self.new_tokens -= offset
            for seq in self.sequences:
                p_page = seq.kv_position // PAGE_SIZE
                seq.kv_position -= offset
                seq.sequence_ids.truncate(len(seq.sequence_ids) - offset)
                n_page = seq.kv_position // PAGE_SIZE
                for pi in range(n_page, p_page + 1):
                    page = seq.allocated_pages[pi]
                    page.can_revert = False
                    if page.kv_position == PAGE_SIZE:
                        page.update_hash(random_hash())
                    if pi == n_page:
                        page.kv_position = seq.kv_position - pi * PAGE_SIZE
                    else:
                        page.kv_position = 0
            off_tokens = self.held_tokens.slice(len(self.checkpoint["held_tokens"]), None)
            off_text = self.held_text[len(self.checkpoint["held_text"]):]
            self.held_text = self.checkpoint["held_text"]
            self.held_token = self.checkpoint["held_tokens"]
            self.held_probs = self.checkpoint["held_probs"]
            self.held_k_tokens = self.checkpoint["held_k_tokens"]
            self.held_k_probs = self.checkpoint["held_k_probs"]
            self.held_logits = self.checkpoint["held_logits"]
            self.checkpoint["offset"] = 0
            return off_tokens, off_text

        if self.banned_strings_utf32_offsets is not None and self.new_tokens > 0:
            match = ext.partial_strings_match(
                np.frombuffer(self.held_text.lower().encode("utf-32-le"), dtype = np.uint8),
                self.banned_strings_utf32_offsets,
                self.banned_strings_utf32_buffer
            )
            if match >= 0:
                set_checkpoint()
                offending_tokens, offending_text = rewind_checkpoint()
                return emit(
                    results,
                    emit_held = True,
                    suppressed_text = offending_text,
                    suppressed_tokens = offending_tokens
                )
            elif match == -2:
                set_checkpoint()
                return emit(results)
            else:
                unset_checkpoint()

        # End on stop strings

        if self.stop_strings_utf32_offsets is not None:
            match = ext.partial_strings_match(
                np.frombuffer(self.held_text.encode("utf-32-le"), dtype = np.uint8),
                self.stop_strings_utf32_offsets,
                self.stop_strings_utf32_buffer
            )
            if match >= 0:
                held = self.held_text[match:]
                self.held_text = self.held_text[:match]
                for s in self.stop_strings:
                    if held.startswith(s):
                        return emit(
                            results,
                            emit_eos = True,
                            emit_held = True,
                            eos_reason = "stop_string",
                            stop_string = s,
                            rem_held_text = held
                        )
                assert False, "Detected stop string but couldn't identify it (logic error)"
            if match == -2:
                return emit(results)

        # Stream output

        return emit(results, emit_held = True)


    def prepare_for_queue(self, generator, serial_number: int):

        # Attach to generator
        self.serial_number = serial_number
        self.generator = generator
        self.pagetable = generator.pagetable
        self.skips = 0

        # Hash full pages of input IDs
        all_unique_hashes = set()
        all_unique_pages = 0
        for seq in self.sequences:
            unique_hashes, unique_pages = seq.prepare(self.prefix_token is not None, self.max_new_tokens)
            all_unique_hashes |= unique_hashes
            all_unique_pages += unique_pages
        self.all_unique_hashes = list(all_unique_hashes)

        # Make sure the request can potentially fit
        total_pages = len(self.all_unique_hashes) + seq.new_unique_pages
        max_pages = self.pagetable.max_pages
        assert total_pages <= max_pages, \
            f"Job requires {total_pages} pages (only {max_pages} available) and cannot " + \
            f"be enqueued. Total cache allocated is {max_pages} * {PAGE_SIZE} = " + \
            f"{self.generator.max_total_tokens} tokens"
        assert len(self.sequences) <= self.generator.max_batch_size, \
            f"Job requires a minimum batch size of {len(self.sequences)}. Max supported batch size in" + \
            f"generator is {self.generator.max_batch_size}."

        # Initial conditions
        self.held_text = ""
        self.held_tokens = SeqTensor((1, 0), dtype = torch.long, seq_dim = -1)
        self.held_k_tokens = SeqTensor((1, 0, self.return_top_tokens), dtype = torch.long, seq_dim = 1)
        self.held_k_probs = SeqTensor((1, 0, self.return_top_tokens), dtype = torch.float, seq_dim = 1)
        self.held_probs = SeqTensor((1, 0), dtype = torch.float, seq_dim = -1)
        self.held_logits = SeqTensor((1, 0, self.generator.padded_vocab_size), dtype = torch.float, seq_dim = 1)
        self.full_completion = ""

        # Prepare MRoPE embeddings
        # TODO: (embeddings)
        # if self.embeddings and generator.model.config.arch.lm.mrope:
        #     ids = self.sequences[0].sequence_ids.torch()
        #     e, offset = mrope.gen_mrope_embed(
        #         generator.model.config,
        #         ids,
        #         self.embeddings,
        #         ids.shape[-1],  # + self.max_new_tokens
        #     )
        #     self.alt_rope_embed = {"cpu": e}
        #     self.alt_rope_offset = offset - ids.shape[-1]
        # else:
        #     self.alt_rope_embed = {}
        #     self.alt_rope_offset = 0


    def current_new_pages_required(self):
        new_pages = 0
        for h in self.all_unique_hashes:
            if h not in self.pagetable.referenced_pages:
                new_pages += 1
        for s in self.sequences:
            new_pages += s.new_unique_pages
        return new_pages


    def prefill(self, results: list):
        if self.time_first_prefill is None:
            self.time_first_prefill = time.time()

        progress = 0
        for seq in self.sequences:
            if seq.prefill_complete:
                continue

            prefill_start = seq.kv_position
            prefill_end = seq.kv_position + self.generator.max_chunk_size
            prefill_end = (prefill_end // PAGE_SIZE) * PAGE_SIZE
            prefill_end = min(prefill_end, len(seq.sequence_ids) - 1)

            p0 = prefill_start // PAGE_SIZE
            p1 = (prefill_end + PAGE_SIZE - 1) // PAGE_SIZE
            for local_idx in range(p0, p1):
                page = seq.allocated_pages[local_idx]
                if page.kv_position == PAGE_SIZE:  # TODO: is this needed since seq.kv_position is set by seq.prepare?
                    prefill_start = (local_idx + 1) * PAGE_SIZE
                    seq.kv_position = prefill_start
                    self.cached_pages += 1
                    page.can_revert = False
                else:
                    break

            p0 = prefill_start // PAGE_SIZE
            for local_idx in range(p0, p1):
                page = seq.allocated_pages[local_idx]
                if page.kv_position == PAGE_SIZE:
                    prefill_end = local_idx * PAGE_SIZE
                    break

            if prefill_end <= prefill_start:
                continue

            prefill_ids = seq.sequence_ids.torch_slice(prefill_start, prefill_end)

            # Special case for partial last page, check if there's a page anywhere in the cache that
            # partially matches, then copy keys/values from there
            p0 = prefill_start // PAGE_SIZE
            p1 = prefill_end // PAGE_SIZE
            if prefill_start == p0 * PAGE_SIZE:
                prev_hash = None if p0 == 0 else seq.allocated_pages[p0 - 1].phash
                best_match = 0
                best_match_page = None
                for page in self.pagetable.all_pages:
                    if page.prev_hash != prev_hash or page == seq.allocated_pages[p0]:
                        continue
                    match = ext.count_match_tensor(page.sequence, prefill_ids, page.kv_position)
                    if match > best_match:
                        best_match = match
                        best_match_page = page
                if best_match_page and best_match > 1:
                    page = seq.allocated_pages[p0]
                    for c in [self.generator.cache] if not self.generator.draft_model else \
                            [self.generator.cache, self.generator.draft_cache]:
                        c.copy_page(
                            c,
                            best_match_page.page_index,
                            page.page_index,
                            best_match,
                        )
                    page.prev_hash = best_match_page.prev_hash
                    page.sequence[:, :best_match].copy_(prefill_ids[:, :best_match])
                    prefill_ids = prefill_ids[:, best_match:]
                    prefill_start += best_match
                    seq.kv_position += best_match
                    page.kv_position = best_match
                    page.can_revert = False
                    self.cached_tokens += best_match
                    progress += best_match

            # Inference

            if prefill_end > prefill_start:

                if self.generator.draft_model:
                    self.generator.draft_model.prefill(
                        input_ids = prefill_ids,
                        params = {
                            "attn_mode": "flash_attn",
                            "block_table": seq.block_index_tensor,
                            "cache": self.generator.draft_cache,
                            "cache_seqlens": torch.tensor([prefill_start], dtype = torch.int32)
                        }
                    )

                self.generator.model.prefill(
                    input_ids = prefill_ids,
                    params = {
                        "attn_mode": "flash_attn",
                        "block_table": seq.block_index_tensor,
                        "cache": self.generator.cache,
                        "cache_seqlens": torch.tensor([prefill_start], dtype = torch.int32)
                    }
                )

                seq.kv_position = prefill_end

                p2 = min(p1 + 1, len(seq.allocated_pages))
                for local_idx in range(p0, p2):
                    page = seq.allocated_pages[local_idx]
                    page.kv_position = min(max(prefill_end - local_idx * PAGE_SIZE, 0), PAGE_SIZE)
                    if local_idx == 0:
                        page.prev_hash = None
                    else:
                        page.prev_hash = seq.allocated_pages[local_idx - 1].phash
                    pf_a = max(local_idx * PAGE_SIZE, prefill_start)
                    pf_b = min(local_idx * PAGE_SIZE + PAGE_SIZE, prefill_end)
                    pfp_a = pf_a - local_idx * PAGE_SIZE
                    pfp_b = pf_b - local_idx * PAGE_SIZE
                    page.sequence[:, pfp_a:pfp_b].copy_(seq.sequence_ids.torch_slice(pf_a, pf_b))
                    page.can_revert = False

                progress += prefill_end - prefill_start
                if progress >= len(seq.sequence_ids) - 1:
                    seq.prefill_complete = True

        if progress:
            r = {
                "job": self,
                "stage": "prefill",
                "eos": False,
                "curr_progress": sum(seq.kv_position for seq in self.sequences),
                "max_progress": sum(len(seq.sequence_ids) - 1 for seq in self.sequences),
                "serial": self.serial_number,
            }
            if self.identifier is not None:
                r.update({"identifier": self.identifier})
            results.append(r)


    def allocate_pages(self):
        for seq in self.sequences:
            allocated_pages, cached_pages, non_sequential_pages = \
                seq.allocate_pages(self.pagetable)

            # Metrics
            self.cached_pages += cached_pages
            self.total_pages += allocated_pages
            self.non_sequential_pages += non_sequential_pages


    def deallocate_pages(self):
        for seq in self.sequences:
            if seq.allocated_pages is not None:
                self.pagetable.deallocate_pages(seq.allocated_pages)
                seq.allocated_pages = []


    def prepare_sampling_past_ids(self):
        if not self.sampler.reqs_past_ids:
            return
        self.current_pinned_ids = self.pinned_ids[:, :len(self.sequences[0].sequence_ids)]
        self.current_pinned_ids.copy_(self.sequences[0].sequence_ids.torch())

</content>

<content full_path="exllamav3/generator/__init__.py">
from .generator import Generator
from .job import Job
from .async_generator import AsyncGenerator, AsyncJob
</content>

<content full_path="exllamav3/generator/async_generator.py">
from __future__ import annotations
from .generator import Generator
from .job import Job
import asyncio

class AsyncGenerator:
    """
    Async wrapper for dynamic generator. See definition of ExLlamaV2DynamicGenerator.
    """
    def __init__(self, *args, **kwargs):
        self.generator = Generator(*args, **kwargs)
        self.jobs = {}
        self.condition = asyncio.Condition()
        self.iteration_task = asyncio.create_task(self._run_iteration())

    async def _run_iteration(self):
        try:
            while True:
                async with self.condition:
                    # Unlock if there's no jobs or if the parent task is cancelled
                    await self.condition.wait_for(lambda: len(self.jobs) > 0 or self.iteration_task.cancelled())

                results = self.generator.iterate()
                for result in results:
                    job = result["job"]
                    async_job = self.jobs[job]
                    await async_job.put_result(result)
                    if result["eos"]:
                        del self.jobs[job]
                await asyncio.sleep(0)
        except asyncio.CancelledError:
            # Silently return on cancel
            return
        except Exception as e:
            # If the generator throws an exception it won't pertain to any one ongoing job, so push it to all of them
            for async_job in self.jobs.values():
                await async_job.put_result(e)

    def enqueue(self, job: AsyncJob):
        assert job.job not in self.jobs
        self.jobs[job.job] = job
        self.generator.enqueue(job.job)
        asyncio.create_task(self._notify_condition())

    async def _notify_condition(self):
        async with self.condition:
            self.condition.notify_all()

    async def close(self):
        self.iteration_task.cancel()

        # Force a re-check of the condition to unlock the loop
        await self._notify_condition()
        try:
            await self.iteration_task
        except asyncio.CancelledError:
            pass

    async def cancel(self, job: AsyncJob):
        assert job.job in self.jobs
        self.generator.cancel(job.job)
        del self.jobs[job.job]


class AsyncJob:
    """
    Async wrapper for dynamic generator job. See definition of ExLlamaV2DynamicJob.
    """
    def __init__(self, generator: AsyncGenerator, *args: object, **kwargs: object):
        self.generator = generator
        self.job = Job(*args, **kwargs)
        self.queue = asyncio.Queue()
        self.generator.enqueue(self)
        self.cancelled = False

    async def put_result(self, result):
        await self.queue.put(result)

    async def __aiter__(self):
        while True:
            # Get out if the job is cancelled
            if self.cancelled:
                break

            result = await self.queue.get()
            if isinstance(result, Exception):
                raise result
            yield result
            if result["eos"]:
                break

    async def cancel(self):
        await self.generator.cancel(self)
        self.cancelled = True

</content>

<content full_path="exllamav3/generator/generator.py">
from __future__ import annotations
import torch
from dataclasses import dataclass
from ..models.model import Model
from ..cache.cache import Cache
from ..tokenizer.tokenizer import Tokenizer
from ..constants import PAGE_SIZE
from ..util import cuda_sync_active
from .pagetable import PageTable
from .job import Job
from concurrent.futures import ThreadPoolExecutor
from .sampler import Sampler, GumbelSampler
from .visualizer import CacheVisualizer
import time
import threading
import numpy as np
from ..util import profile_opt

class Generator:

    def __init__(
        self,
        model: Model,
        cache: Cache,
        tokenizer: Tokenizer,
        max_batch_size: int = 256,
        max_chunk_size: int = 2048,
        max_q_size: int = 8,
        draft_model: Model | None = None,
        draft_cache: Cache | None = None,
        num_draft_tokens: int = 4,
        show_visualizer: bool = False,
        enable_defrag: bool = True,
        **kwargs
    ):
        """
        Initialize generator

        :param model:
            The model (loaded)

        :param cache:
            Paged cache

        :param tokenizer:
            Tokenizer

        :param max_batch_size:
            The maximum number of sequences to process in parallel. The generator will also limit this
            dynamically considering the available cache space.

        :param max_chunk_size:
            Maximum number of tokens to process in parallel during prefill (prompt ingestion). Should not
            exceed the model's max_input_len but can be lowered to trade off prompt speed for a shorter
            interruption to ongoing jobs when a new job is started.

        :param max_q_size:
            Maximum number of tokens to evaluate per sequence during generation. Leave this at the default
            (8) unless there's a good reason to increase it.

        :param draft_model:
            Draft model. Enables speculative decoding with draft, and must be specified along with
            draft_cache. Note that speculative decoding with many parallel jobs is likely not advantageous.

        :param draft_cache:
            Cache allocated for draft model. Must be same size as main cache.

        :param num_draft_tokens:
            Number of future tokens to draft.

        :param show_visualizer:
            Open window to render visualization of cache (for debug/demonstration purposes)

        :param enable_defrag:
            Defragment cache periodically

        :param kwargs:
        """

        self.model = model
        self.cache = cache
        self.tokenizer = tokenizer
        cfg = self.model.config
        self.padded_vocab_size = ((cfg.vocab_size + 31) // 32) * 32

        # Paging
        self.pagetable = PageTable(self, cache)
        self.max_total_tokens = PAGE_SIZE * self.pagetable.max_pages

        # Draft model
        self.draft_model = draft_model
        self.draft_cache = draft_cache
        if draft_model:
            assert num_draft_tokens <= max_q_size, \
                "num_draft_tokens cannot be larger than max_q_size."
            assert draft_cache is not None, \
                "Must supply cache for draft model"
            assert draft_cache.max_num_tokens == cache.max_num_tokens, \
                "Cache and draft cache must be same size"
            self.num_draft_tokens = num_draft_tokens
        else:
            self.num_draft_tokens = 0

        # Chunking/partitioning
        self.max_batch_size = max_batch_size
        self.max_chunk_size = max_chunk_size

        # Job queues
        self.job_serial = 0
        self.pending_jobs = []
        self.active_jobs = []

        # Filter threads
        self.filter_pool = ThreadPoolExecutor(max_workers = 16)
        self.filter_queue = []

        # Buffers
        if draft_model:
            self.draft_input_ids_pinned = torch.empty(
                (max_batch_size, 1),
                dtype = torch.long,
                pin_memory = False
            )
            self.draft_ids_pinned = torch.empty(
                (max_batch_size, num_draft_tokens),
                dtype = torch.long,
                pin_memory = False
            )

        # Visualizer
        if show_visualizer:
            self.visualizer = CacheVisualizer(self.pagetable.max_pages)
        else:
            self.visualizer = None

        # Defrag
        self.enable_defrag = enable_defrag


    def num_remaining_jobs(self):
        return len(self.pending_jobs) + len(self.active_jobs)

    def num_active_jobs(self):
        return len(self.active_jobs)

    def num_pending_jobs(self):
        return len(self.pending_jobs)


    def clear_queue(self):
        """
        Abort all active and pending jobs
        """

        num_jobs = self.num_remaining_jobs()
        for job in self.active_jobs + self.pending_jobs:
            job.deallocate_pages()
        self.active_jobs.clear()
        self.pending_jobs.clear()
        if num_jobs and not self.num_remaining_jobs():
            self.pagetable.defrag()


    def enqueue(
        self,
        job: Job | list[Job]
    ) -> int | list[int]:
        """
        Adds a job or list of jobs to the queue.

        returns:
            int: (List of) unique serial number(s) for job(s)
        """

        if isinstance(job, list):
            serials = []
            for j in job:
                serials.append(self.enqueue(j))
            return serials

        job.prepare_for_queue(self, self.job_serial)
        self.job_serial += 1
        self.pending_jobs.append(job)
        job.time_enqueue = time.time()
        return job.serial_number


    def cancel(
        self,
        job: Job
    ):
        """
        Cancel single job
        """

        num_jobs = self.num_remaining_jobs()

        if job in self.pending_jobs:
            self.pending_jobs.remove(job)
        elif job in self.active_jobs:
            job.deallocate_pages()
            self.active_jobs.remove(job)
        if num_jobs and not self.num_remaining_jobs():
            self.pagetable.defrag()


    @torch.inference_mode
    def iterate(self) -> list[dict]:
        """
        Performs inference on available jobs.

        :return:
            List of dicts:

            # Job has started
            {
                "job": Job  - reference to job
                "stage": "started"
                "identifier":  - optional identifier
                "serial": int  - job serial number
                "eos": bool  - always False at this stage
            }

            # Prefill is underway
            {
                "job": Job  - reference to job
                "stage": "prefill"
                "curr_progress": int  - prompt tokens ingested so far
                "max_progress": int  - total prompt tokens to ingest
                "identifier":  - optional identifier
                "serial": int   - job serial number
                "eos": bool  - always False at this stage
            }

            # Generation is underway
            {
                "job": Job  - reference to job
                "stage": "streaming"
                "identifier":  - optional identifier
                "serial": int   - job serial number
                "eos": bool  - True if stop condition has been met

                optional, if eos:
                    "eos_reason":  - one of:
                        "stop_token"
                        "stop_string"
                        "max_new_tokens"
                        "end_filter"
                    optional, if "eos_reason" == "stop_token":
                        "eos_triggering_token_id": int
                        "eos_triggering_token_str": str
                    optional, if "eos_reason" == "stop_string":
                        "eos_triggering_string": str
                    "full_completion": str  - full text completion
                    "new_tokens": int  - number of tokens generated
                    "time_enqueued": float  - time from job was enqueued until it started, in seconds
                    "time_prefill": float  - time to first token, in seconds
                    "time_generate": float  - time to last token, in seconds
                    optional, if SD enabled:
                        "accepted_draft_tokens": int
                        "rejected_draft_tokens": int

                "text": str  - streamed text output. Does not include prefix from healed token, or stop string
                "token_ids": torch.Tensor  - output tokens, shape (1, n)
                "token_probs": torch.Tensor  - last sampling probability of output tokens, shape (1, n)
                "top_k_tokens": torch.Tensor  - shape (1, n, k)
                "top_k_probs": torch.Tensor  - shape (1, n, k)
                "logits": torch.Tensor  - shape (1, n, vocab_size)
            }
        """

        results = []
        self.iterate_start_jobs(results)

        # Perform one round of prefill
        for job in self.active_jobs:
            job.prefill(results)

        # Generation with draft model
        if self.draft_model:
            draft_tokens = self.iterate_draftmodel_gen(results)
            self.iterate_gen(results, draft_tokens)

        # Regular generation
        else:
            self.iterate_gen(results)

        # Visualization
        if self.visualizer:
            self.update_visualizer()

        # Finished iteration
        return results


    def update_visualizer(self):
        chains = []
        for job in self.active_jobs:
            for seq in job.sequences:
                idx = job.serial_number
                chain = [page.page_index for page in seq.allocated_pages]
                chains.append((idx, chain))
        usage = [0] * self.pagetable.max_pages
        for page in self.pagetable.all_pages:
            usage[page.page_index] = page.kv_position / PAGE_SIZE
        self.visualizer.update(chains, usage)

    def iterate_draftmodel_gen(self, results: list):

        # Get shape of active batch
        batch_size = 0
        max_seq_len = 0
        for job in self.active_jobs:
            if not job.is_prefill_done(): continue
            max_seq_len = max(max_seq_len, job.get_max_seq_len() + self.num_draft_tokens + 1)
            batch_size += 1
        if batch_size == 0:
            return None

        # Create block index table for batch
        max_pages_batch = (max_seq_len + PAGE_SIZE - 1) // PAGE_SIZE
        block_index = torch.zeros((batch_size, max_pages_batch), dtype = torch.int32)
        cache_seqlens = torch.zeros((batch_size,), dtype = torch.int32)
        batch = 0
        for job in self.active_jobs:
            if not job.is_prefill_done(): continue
            for seq in job.sequences:
                seq_block_index = seq.block_index_tensor[:, :max_pages_batch]
                block_index[batch:batch+1, :seq_block_index.shape[-1]].copy_(seq_block_index)
                cache_seqlens[batch] = seq.kv_position
                batch += 1

        # Indexed embeddings not supported when drafting
        # TODO: Allow multimodal draft model, perhaps with dummy embeddings?
        # for job in self.active_jobs:  TODO: (embeddings)
        #     assert not job.embeddings, \
        #         "Embeddings not supported while using draft model."

        # Collect input IDs
        input_ids_list = []
        for job in self.active_jobs:
            if not job.is_prefill_done(): continue
            if job.time_first_token is None:
                cuda_sync_active()
                job.time_first_token = time.time()
            job_ids = job.get_input_ids_list()
            input_ids_list += job_ids
        batch_ids = self.draft_input_ids_pinned[:batch_size, :]
        batch_ids.copy_(torch.cat(input_ids_list, dim = 0))

        # Greedy sample num_draft_tokens batched tokens
        for idx in range(self.num_draft_tokens):
            batch_logits = self.draft_model.forward(
                input_ids = batch_ids,
                params = {
                    "attn_mode": "flash_attn",
                    "block_table": block_index,
                    "cache": self.draft_cache,
                    "cache_seqlens": cache_seqlens,
                }
            )
            new_ids = torch.argmax(batch_logits, dim = -1)
            self.draft_ids_pinned[:batch_size, idx:idx+1].copy_(new_ids)
            batch_ids.copy_(new_ids)
            cache_seqlens += 1

        self.draft_model.prefill(
            input_ids = batch_ids,
            params = {
                "attn_mode": "flash_attn",
                "block_table": block_index,
                "cache": self.draft_cache,
                "cache_seqlens": cache_seqlens,
            }
        )

        return self.draft_ids_pinned


    def iterate_gen(self, results: list, draft_tokens: torch.Tensor | None = None):

        # Get shape of active batch
        batch_size = 0
        max_seq_len = 0
        for job in self.active_jobs:
            if not job.is_prefill_done(): continue
            max_seq_len = max(max_seq_len, job.get_max_seq_len() + self.num_draft_tokens)
            batch_size += len(job.sequences)
        if batch_size == 0:
            return
        if draft_tokens is not None:
            max_seq_len += draft_tokens.shape[-1]

        # Create block index table for batch
        max_pages_batch = (max_seq_len + PAGE_SIZE - 1) // PAGE_SIZE
        block_index = torch.zeros((batch_size, max_pages_batch), dtype = torch.int32)
        cache_seqlens = torch.zeros((batch_size,), dtype = torch.int32)
        batch = 0
        for job in self.active_jobs:
            if not job.is_prefill_done(): continue
            for seq in job.sequences:
                seq_block_index = seq.block_index_tensor[:, :max_pages_batch]
                block_index[batch:batch+1, :seq_block_index.shape[-1]].copy_(seq_block_index)
                cache_seqlens[batch] = seq.kv_position
                batch += 1

        # Collect input IDs and indexed embeddings
        input_ids_list = []
        active_embeddings = []  # TODO (embeddings)
        logit_mapping = []
        # rope_offsets_list = [] if self.model.config.arch.lm.mrope else None  # TODO (embeddings)
        for job in self.active_jobs:
            logit_mapping.append(len(input_ids_list))
            if not job.is_prefill_done(): continue
            if job.time_first_token is None:
                cuda_sync_active()
                job.time_first_token = time.time()
            job_ids = job.get_input_ids_list(draft_tokens, len(input_ids_list), add_to_cache = True)
            input_ids_list += job_ids
            # active_embeddings += job.embeddings  # TODO (embeddings)
            # if rope_offsets_list is not None:
            #     rope_offsets_list += [job.alt_rope_offset] * len(job_ids)
        logit_mapping.append(len(input_ids_list))
        batch_ids = torch.cat(input_ids_list, dim = 0)

        # GPU workload is scheduled here, so launch any sampling filters that can run in the background
        # TODO: (filters)
        # if self.filter_queue:
        #     for f, p in self.filter_queue:
        #         if p:
        #             f.background_prepare_logit_mask(self.filter_pool)
        #         else:
        #             f.background_next(self.filter_pool)
        #     # time.sleep(0)
        #     self.filter_queue.clear()

        # Get logit batch from model
        batch_logits = self.model.forward(
            input_ids = batch_ids,
            params = {
                "attn_mode": "flash_attn",
                "block_table": block_index,
                "cache": self.cache,
                "cache_seqlens": cache_seqlens,
            }
        )

        # Prepare past IDs (for sequences that need them for repetition penalty etc.)
        for job in self.active_jobs:
            job.prepare_sampling_past_ids()

        # TODO: Batch sampling

        # Pass to jobs to sample
        completed_jobs = []
        j = 0
        for job, a, b in zip(self.active_jobs, logit_mapping[:-1], logit_mapping[1:]):
            if a == b: continue
            job_logits = batch_logits[a:b, :, :]

            for i in range(batch_logits.shape[1]):
                token_logits = job_logits[:, i:i + 1, :]
                next_token, next_k_tokens, next_k_probs, next_prob = job.receive_logits(
                    token_logits,
                )
                eos, sampled_token = job.receive_sample(
                    token_logits,
                    next_token,
                    next_k_tokens,
                    next_k_probs,
                    next_prob,
                    results,
                )

                # EOS
                if eos:
                    completed_jobs.append(job)
                    break

                # Continue sampling from logit batch as long as result matches draft
                if draft_tokens is not None and i < batch_logits.shape[1] - 1:
                    if draft_tokens[j, i].item() != sampled_token.item():
                        rejected = batch_logits.shape[1] - 1 - i
                        job.rejected_draft_tokens += rejected
                        for seq in job.sequences:
                            r = rejected
                            while r:
                                pos = seq.kv_position + r
                                page = seq.allocated_pages[(pos - 1) // PAGE_SIZE]
                                rp = min(page.kv_position, r)
                                page.kv_position -= rp
                                r -= rp
                        break
                    else:
                        job.accepted_draft_tokens += 1
            j += 1

        # if self.max_sampling_threads > 1 and len(self.active_jobs) >= self.min_sampling_threads:
        #     mt_sample = True
        #     futures = deque()
        #     for job, a, b in zip(self.active_jobs, logit_mapping[:-1], logit_mapping[1:]):
        #         if a == b: continue
        #         job_logits = batch_logits[a:b, :1, :]
        #         futures.append(self.sampling_pool.submit(job.receive_logits, job_logits))
        # else:
        #     mt_sample = False

        # Release pages for completed jobs
        num_jobs = self.num_remaining_jobs()
        for job in completed_jobs:
            job.deallocate_pages()
            self.active_jobs.remove(job)
        if num_jobs and not self.num_remaining_jobs():
            self.pagetable.defrag()


    def iterate_start_jobs(self, results: list):

        # Get current max batch
        current_max_batch = 0
        for job in self.active_jobs:
            current_max_batch += len(job.sequences)

        # Start new jobs if possible
        if (self.pagetable.num_unreferenced_pages() and
            len(self.pending_jobs) and
            current_max_batch < self.max_batch_size):

            skipped_jobs = []
            for job in self.pending_jobs.copy():

                if (len(job.sequences) + current_max_batch > self.max_batch_size or
                        job.current_new_pages_required() > self.pagetable.num_unreferenced_pages()):
                    skipped_jobs.append(job)
                    continue

                # Make sure the job we're about to add doesn't skip a job that's been skipped too many times
                for j in skipped_jobs:
                    if j.skips >= j.max_skips:
                        return
                for j in skipped_jobs:
                    j.skips += 1

                # Add job to active list
                self.pending_jobs.remove(job)
                self.active_jobs.append(job)

                # Allocate pages for job
                job.allocate_pages()
                current_max_batch += len(job.sequences)

                r = {
                    "job": job,
                    "stage": "started",
                    "eos": False,
                    "serial": job.serial_number,
                }
                if job.identifier is not None:
                    r.update({ "identifier": job.identifier })
                results.append(r)


    def generate(
        self,
        prompt: list[tuple] | list[str] | tuple | str,
        max_new_tokens: int | None = None,
        min_new_tokens: int = 0,
        seed: int | None = None,
        sampler: Sampler | list[Sampler] | None = None,
        token_healing: bool = False,
        encode_special_tokens: bool = False,
        decode_special_tokens: bool = False,
        stop_conditions: list[int | str] | None = None,
        add_bos: bool = False,
        abort_event: threading.Event | None = None,
        completion_only: bool = False,
        # filters: list[list[Filter]] | list[Filter] | None = None,
        # filter_prefer_eos: bool = False,
        return_last_results: bool = False,
        # embeddings: list[MMEmbedding] | list[list[MMEmbedding]] | None = None,
        **kwargs
    ):
        """
        This is a utility function for easily generating one or more completions from one or more prompt strings. For
        more versatility and streaming functionality, use the async wrapper or create Job objects directly, enqueue()
        them and call iterate() to receive one token at a time

        :param prompt:
            If this argument is a list, its length determines the batch size, and the output will be a list of strings
            as well. Each prompt is either a string or a pair of prompts for CFG sampling. If CFG is used, sampler
            settings must contain cfg_scale.

        :param sampler:
            Sampler stack settings for all prompts in batch or list of samplers for each prompt.

        :param max_new_tokens:
            Max number of tokens to generate.

        :param min_new_tokens:
            Minimum number of tokens to generate before stop tokens become active. Until this number have been
            sampled, stop tokens are suppressed but stop strings will still end response.

        :param seed:
            Seed for the sampling RNG. Doesn't guarantee perfect determinism from the implementation.

        :param token_healing:
            Apply token healing by regenerating the last token of the input sequence with prefix
            constraint.

        :param encode_special_tokens:
            Encode special tokens (BOS etc.) represented as text in the input. If False, special tokens are
            interpreted as text by the tokenizer.

        :param decode_special_tokens:
            Decode special tokens output by the model. If False, tokens marked as special in the tokenizer
            are decoded as empty strings.

        :param stop_conditions:
            List of strings and/or token IDs that will end generation. The stop condition is not included
            in the output.

        :param add_bos:
            Prepend the tokenizer's specified BOS token to the input.

        :param abort_event:
            Forwarded to the model during generation. Will abort prefill/context ingestion if triggered.

        :param completion_only:
            Only return completion. If False, returned string will include the input prompt.

        # :param filters:
        #     (List of) list of ExLlamaV2Filters to apply during generation. Each prompt in a batch needs
        #     its own filter list, or a value of None to disable filters for individual prompts. TODO

        # :param filter_prefer_eos:
        #     If True, always sample the tokenizer's defined EOS token as soon as it's allowed by the filters TODO

        :param return_last_results:
            If True, returns the last results dict for each job

        # :param embeddings:
        #     Optional list of ExLlamaV2MMEmbeddings to use for, or list of lists for batched generation TODO

        :return:
            Completion(s): (str or list[str] depending on the type of the input prompt argument)
            Optionally, last results: (dict or list[dict] depending on the type of the input prompt argument)
        """

        order = {}
        if isinstance(prompt, list):
            prompts = prompt
        else:
            prompts = [prompt]
            # filters = [filters]  # TODO: (filters)
            # embeddings = [embeddings]

        # if not filters:
        #     filters = [None] * len(prompts)
        # else:
        #     assert len(filters) == len(prompts) and \
        #         all((f is None or isinstance(f, list)) for f in filters), \
        #         "If using filters, must provide one filter list (or None-value) per prompt."

        # if not embeddings:
        #     embeddings = [None] * len(prompts)
        # else:
        #     assert len(embeddings) == len(prompts) and all((isinstance(f, list) or not f) for f in embeddings), \
        #         "Must provide one list of embeddings per prompt."

        prompts = prompt if isinstance(prompt, list) else [prompt]
        batch_size = len(prompts)
        for idx, p in enumerate(prompts):
            if isinstance(p, str):
                input_ids = self.tokenizer.encode(
                    p,
                    encode_special_tokens = encode_special_tokens,
                    add_bos = add_bos,
                    # embeddings = embeddings[idx]
                )
            elif isinstance(p, tuple):
                input_ids = [self.tokenizer.encode(
                    p_,
                    encode_special_tokens = encode_special_tokens,
                    add_bos = add_bos,
                    # embeddings = embeddings[idx]
                ) for p_ in p]
            else:
                assert False, "Unexpected type in prompt"

            if sampler is None or isinstance(sampler, Sampler):
                p_sampler = sampler
            elif isinstance(sampler, list):
                assert len(sampler) == len(prompts)
                p_sampler = sampler[idx]
            else:
                assert False, "Unexpected sampler type"

            job = Job(
                input_ids = input_ids,
                max_new_tokens = max_new_tokens,
                min_new_tokens = min_new_tokens,
                seed = seed,
                stop_conditions = stop_conditions,
                sampler = p_sampler,
                # filters = filters[idx] or [],
                # filter_prefer_eos = filter_prefer_eos,
                token_healing = token_healing,
                decode_special_tokens = decode_special_tokens,
                # embeddings = embeddings[idx] or []
            )

            if seed is not None: seed += 1

            serial = self.enqueue(job)
            order[serial] = idx

        # Collect outputs until all jobs finish
        completions = [""] * batch_size
        last_results = [None] * batch_size

        while self.num_remaining_jobs():
            results = self.iterate()

            for r in results:
                idx = order[r["serial"]]
                if r["stage"] == "streaming":
                    text = r.get("text", "")
                    completions[idx] += text
                if r["eos"]:
                    last_results[idx] = r
            if abort_event is not None and abort_event.is_set():
                self.clear_queue()
                return None

        # Return results
        if not completion_only:
            completions = [(p if isinstance(p, str) else p[0]) + c for p, c in zip(prompts, completions)]

        if not isinstance(prompt, list):
            completions = completions[0]
            last_results = last_results[0]

        if return_last_results:
            return completions, last_results
        else:
            return completions
</content>

<content full_path="exllamav3/generator/pagetable.py">
from __future__ import annotations
from functools import lru_cache
import torch
import hashlib
from dataclasses import dataclass
from typing import TYPE_CHECKING
from ..cache.cache import Cache
if TYPE_CHECKING:
    from .generator import Generator
from ..constants import PAGE_SIZE
from collections import deque, defaultdict
from itertools import pairwise
from ..util.tensor import SeqTensor
from exllamav3.ext import exllamav3_ext as ext
import time
from ..util import profile_opt


def _tensor_blake2b_checksum(tensor: torch.Tensor, prev_hash: bytes | None) -> bytes:
    hasher = hashlib.blake2b(digest_size = 16)
    if prev_hash is not None:
        hasher.update(prev_hash)
    hasher.update(tensor.numpy().tobytes())
    return hasher.digest()

_uniquehash = 0
def _randomhash():
    global _uniquehash
    _uniquehash += 1
    return _uniquehash.to_bytes(16, byteorder = 'big')

tensor_hash_checksum = _tensor_blake2b_checksum
random_hash = _randomhash


@dataclass
class CachePage:

    pagetable: PageTable
    page_index: int

    # Hash of this page if kv_position == PAGE_SIZE, else random hash. Also used to index (un)referenced_pages
    phash: bytes
    phash_revert: bytes

    # Hash of previous page in chain
    prev_hash: bytes | None
    prev_hash_revert: bytes | None

    # Number of active jobs referencing page
    ref_count: int

    # Last time this page was assigned to a job
    access_serial: int
    access_serial_revert: int

    # Number of tokens in page for which KV is valid assuming prev_hash
    kv_position: int
    kv_position_revert: int

    # Specific tokens for which KV is valid assuming prev_hash
    sequence: torch.Tensor
    can_revert: bool

    # Used by defragmenter
    new_page_index: int
    children: list[CachePage]
    longest_chain: int

    def __repr__(self):
        return (
            f"CachePage: idx = {self.page_index}, ref_count = {self.ref_count}, "
            f"phash: ..{str(self.phash)[8:24]}.., prev_hash: ..{str(self.prev_hash)[8:24]}.., "
            f"kvp {self.kv_position}"
        )

    # Copy page state so page can be reverted
    def backup(self):
        self.phash_revert = self.phash
        self.prev_hash_revert = self.prev_hash
        self.access_serial_revert = self.access_serial
        self.kv_position_revert = self.kv_position
        self.can_revert = True

    # Reuse unreferenced page
    def revert(self):
        assert self.can_revert
        self.phash = self.phash_revert
        self.prev_hash = self.prev_hash_revert
        self.access_serial = self.access_serial_revert
        self.kv_position = self.kv_position_revert
        self.can_revert = False

    # Increase reference count
    def add_ref(self, serial):
        if self.ref_count == 0:
            del self.pagetable.unreferenced_pages[self.phash]
            assert self.phash not in self.pagetable.referenced_pages
            self.pagetable.referenced_pages[self.phash] = self
        self.ref_count += 1
        self.access_serial = max(serial, self.access_serial)
        self.can_revert = False

    # Increase reference count and clear page
    def add_ref_clear(self, serial, newhash):
        assert self.ref_count == 0
        del self.pagetable.unreferenced_pages[self.phash]
        self.phash = newhash
        assert self.phash not in self.pagetable.referenced_pages
        self.pagetable.referenced_pages[self.phash] = self
        self.ref_count += 1
        self.access_serial = serial
        self.prev_hash = None
        self.can_revert = False
        self.kv_position = 0

    # Add reference to (currently) unique page
    def add_ref_unique(self, serial):
        self.backup()
        assert self.ref_count == 0
        del self.pagetable.unreferenced_pages[self.phash]
        self.phash = _randomhash()
        assert self.phash not in self.pagetable.referenced_pages
        self.pagetable.referenced_pages[self.phash] = self
        self.ref_count += 1
        self.access_serial = serial
        self.prev_hash = None
        self.kv_position = 0

    # Decrease reference count
    def sub_ref(self):
        self.ref_count -= 1
        if self.ref_count == 0:
            del self.pagetable.referenced_pages[self.phash]
            if self.can_revert:
                self.revert()
            if self.phash in self.pagetable.referenced_pages or self.phash in self.pagetable.unreferenced_pages:
                self.phash = _randomhash()
                self.prev_hash = None
            assert self.phash not in self.pagetable.unreferenced_pages
            self.pagetable.unreferenced_pages[self.phash] = self

    # Clear page
    def clear(self):
        assert self.ref_count == 0
        del self.pagetable.unreferenced_pages[self.phash]
        self.phash = _randomhash()
        self.prev_hash = None
        self.kv_position = 0
        self.can_revert = False
        self.sequence[:, :] = 0
        assert self.phash not in self.pagetable.unreferenced_pages
        self.pagetable.unreferenced_pages[self.phash] = self

    # Update hash
    def update_hash(self, newhash):
        assert self.ref_count > 0
        assert self.kv_position == PAGE_SIZE
        del self.pagetable.referenced_pages[self.phash]
        self.phash = newhash
        self.can_revert = False
        assert self.phash not in self.pagetable.referenced_pages
        self.pagetable.referenced_pages[self.phash] = self


class Sequence:

    def __init__(self, ids: torch.Tensor, seq_ids: torch.Tensor):
        self.input_ids = SeqTensor.from_tensor(ids, seq_dim = -1)
        self.sequence_ids = SeqTensor.from_tensor(seq_ids, seq_dim = -1)
        self.kv_position = 0
        self.page_hashes = None
        self.new_unique_pages = 0
        self.allocated_pages = None
        self.block_index_tensor = None
        self.live = True
        self.prefill_complete = False


    def prepare(self, has_prefix_token: bool, max_new_tokens: int):
        self.page_hashes = []
        unique_hashes = set()

        max_len = len(self.sequence_ids) + max_new_tokens
        if has_prefix_token: max_len += 1
        context_pages = (len(self.sequence_ids) - 1) // PAGE_SIZE
        total_pages = (max_len + PAGE_SIZE - 1) // PAGE_SIZE

        r_hash = None
        for i in range(context_pages):
            # TODO: profile/optimize hash function
            page_ids = self.sequence_ids.torch_slice(i * PAGE_SIZE, (i + 1) * PAGE_SIZE)
            assert page_ids.shape[-1] == PAGE_SIZE
            r_hash = tensor_hash_checksum(page_ids, r_hash)
            self.page_hashes.append(r_hash)
            unique_hashes.add(r_hash)

        self.new_unique_pages = total_pages - context_pages
        return unique_hashes, self.new_unique_pages

    def build_block_index_tensor(self):
        self.block_index_tensor = torch.tensor(
            [[page.page_index for page in self.allocated_pages]],
            dtype = torch.int32,
        )

    def allocate_pages(self, pagetable: PageTable):
        self.allocated_pages, self.kv_position, cached_pages, non_sequential_pages = \
            pagetable.allocate_pages(self.page_hashes, self.new_unique_pages)
        self.build_block_index_tensor()
        return len(self.allocated_pages), cached_pages, non_sequential_pages


class PageTable:

    def __init__(
        self,
        generator: Generator,
        cache: Cache
    ):
        self.generator = generator
        self.cache = cache
        self.max_pages = cache.max_num_tokens // PAGE_SIZE

        self.access_serial = self.max_pages
        self.referenced_pages = {}
        self.unreferenced_pages = {}
        self.all_pages = []
        self.reset_page_table()
        self.last_defrag_serial = self.max_pages


    def reset_page_table(self):
        """
        Reset the page table.
        """
        self.referenced_pages = {}
        self.unreferenced_pages = {}
        self.all_pages = []
        for idx in range(self.max_pages):
            h = _randomhash()
            cp = CachePage(
                pagetable = self,
                page_index = idx,
                phash = h,
                phash_revert = h,
                prev_hash = None,
                prev_hash_revert = None,
                sequence = torch.empty((1, PAGE_SIZE), dtype = torch.long),
                ref_count = 0,
                access_serial = idx,
                access_serial_revert = idx,
                kv_position = 0,
                kv_position_revert = 0,
                can_revert = False,
                new_page_index = 0,
                children = [],
                longest_chain = 1,
            )
            self.all_pages.append(cp)
            self.unreferenced_pages[h] = cp
        self.access_serial = self.max_pages
        self.last_defrag_serial = self.access_serial


    def print_page_list(self, short: bool = True):
        for cp in self.all_pages:
            if cp.phash in self.referenced_pages:
                assert cp.ref_count > 0
                ref = str(cp.ref_count) if cp.ref_count < 10 else "+"
            elif cp.phash in self.unreferenced_pages:
                assert cp.ref_count == 0
                ref = "."
            else:
                ref = "#"
            if short: print(ref, end = "")
            else: print(str(cp) + f", ref {ref}")
        print()


    def allocate_pages(
        self,
        page_hashes: list,
        new_unique_pages: int
    ):
        allocated_pages = []
        available_pages = None

        # Allocate whole pages
        for h in page_hashes:
            self.access_serial += 1

            # Find matching referenced page
            rp = self.referenced_pages.get(h)
            if rp:
                rp.add_ref(self.access_serial)
                allocated_pages.append(rp)

            # If possible, reuse an unreferenced page with matching hash
            else:
                up = self.unreferenced_pages.get(h)
                if up:
                    up.add_ref(self.access_serial)
                    allocated_pages.append(up)

                # No matching pages
                else:

                    # Get list of unreferenced pages in order of oldest to newest
                    if available_pages is None:
                        available_pages = list(self.unreferenced_pages.values())
                        available_pages.sort(key = lambda x: x.access_serial)
                        available_pages = deque(available_pages)
                    else:
                        while available_pages[0].ref_count:
                            available_pages.popleft()

                    # Allocate oldest unreferenced page
                    op = available_pages.popleft()
                    op.add_ref_clear(self.access_serial, h)
                    allocated_pages.append(op)

        # Allocate unique pages
        for npi in range(new_unique_pages):
            self.access_serial += 1

            # Get list of unreferenced pages in order of oldest to newest
            if available_pages is None:
                available_pages = list(self.unreferenced_pages.values())
                available_pages.sort(key = lambda x: x.access_serial)
                available_pages = deque(available_pages)
            else:
                while available_pages[0].ref_count:
                    available_pages.popleft()

            op = available_pages.popleft()
            op.add_ref_unique(self.access_serial)
            allocated_pages.append(op)

        # Advance cache over prefilled pages
        kv_position = 0
        cached_pages = 0
        for page in allocated_pages:
            if page.kv_position == PAGE_SIZE:
                kv_position += PAGE_SIZE
                cached_pages += 1
            else:
                break

        non_sequential_pages = 0
        for page_a, page_b in pairwise(allocated_pages):
            if page_b.page_index != page_a.page_index + 1:
                non_sequential_pages += 1

        return allocated_pages, kv_position, cached_pages, non_sequential_pages


    def deallocate_pages(self, allocated_pages: list):
        for page in allocated_pages:
            page.sub_ref()


    def num_unreferenced_pages(self):
        return len(self.unreferenced_pages)


    def defrag(self, debug = False):

        if not self.generator.enable_defrag:
            return

        # Defragment once job queue is empty and all pages have been touched at least once
        if self.access_serial < self.last_defrag_serial + self.max_pages:
            return
        self.last_defrag_serial = self.access_serial

        assert not self.referenced_pages

        if debug:
            torch.cuda.synchronize()
            time_begin = time.time()

        # Build page index
        page_index = {}
        def build_page_index():
            nonlocal page_index
            page_index = {}
            for page in self.all_pages:
                page_index[page.phash] = page
                page.children = []
                page.longest_chain = 1
        build_page_index()

        # Find cached sequences that can be recovered
        root_pages = []
        def build_root_pages():
            nonlocal root_pages
            root_pages = []
            for page in self.all_pages:
                if page.prev_hash is None:
                    root_pages.append(page)
                else:
                    parent = page_index.get(page.prev_hash)
                    if parent is not None:
                        parent.children.append(page)
        build_root_pages()

        # Measure recoverable sequence length
        def measure(p):
            p.longest_chain = 1
            if p.children:
                p.longest_chain += max([measure(pc) for pc in p.children])
            return p.longest_chain

        for page in root_pages:
            measure(page)

        # Recursively sort branches by length
        def sort_seq(p):
            if len(p.children) > 1:
                p.children = sorted(p.children, key = lambda x: x.longest_chain, reverse = True)
            for pc in p.children:
                sort_seq(pc)

        for page in root_pages:
            sort_seq(page)

        # Process roots in order of increasing age
        root_pages = sorted(root_pages, key = lambda x: x.access_serial)

        # Maintain the longest sequence for each tree and create new root nodes from trimmed branches
        index = 0
        while index < len(root_pages):
            page = root_pages[index]
            while page.children:
                root_pages += page.children[1:]
                page.children = page.children[:1]
                page = page.children[0]
            index += 1

        # Reorder partial sequences into the longest possible contiguous strings
        new_page_index = 0
        shift_counts = defaultdict(int)
        non_orphaned_pages = []
        orphans = page_index
        for page in root_pages:
            while True:
                non_orphaned_pages.append(page)
                del orphans[page.phash]
                page.new_page_index = new_page_index
                shift = page.new_page_index - page.page_index
                shift_counts[shift] += 1
                new_page_index += 1
                if not page.children:
                    break
                page = page.children[0]

        # Move orphans to end of cache, ordered by last access
        if orphans:
            orphans = list(orphans.values())
            orphans = sorted(orphans, key = lambda x: x.page_index)
            access_serials = [page.access_serial for page in orphans]
            access_serials = sorted(access_serials)
            for page, access_serial in zip(orphans, access_serials):
                page.access_serial = access_serial
                page.new_page_index = new_page_index
                shift = page.new_page_index - page.page_index
                shift_counts[shift] += 1
                new_page_index += 1

        assert new_page_index == self.max_pages

        # Adjust overall shift to minimize page copies
        shift_adjust = max(shift_counts, key = shift_counts.get)

        # Order of operations
        if debug:
            print("Page shifts")

        defrag_map = {}
        for page in self.all_pages:
            page.new_page_index = (page.new_page_index - shift_adjust + self.max_pages) % self.max_pages
            if page.page_index != page.new_page_index:
                defrag_map[page.new_page_index] = page.page_index
                if debug:
                    print(f"{page.new_page_index:2} ← {page.page_index:2}")

        # Don't bother if less than 10% of cache is fragmented
        if len(defrag_map) <= max(self.max_pages // 10, 2):
            return

        # Get all tensors to reshuffle
        cache_tensors = self.cache.get_all_tensors()

        if debug:
            print("Page rotations")

        # Find page rotations
        all_rotations = []
        while defrag_map:

            # Get first dst,src pair in new loop
            dst = next(iter(defrag_map))
            src = defrag_map[dst]
            del defrag_map[dst]
            rotation = [dst, src]

            # Walk around loop
            while True:
                if src == rotation[0]:
                    rotation = [-1, src] + rotation[:-1] + [-1]
                    all_rotations += rotation
                    break
                dst = src
                src = defrag_map[dst]
                del defrag_map[dst]
                rotation += [dst, src]

            if debug:
                print(" ← ".join([".."] + [f"{rotation[i + 1]:2}" for i in range(0, len(rotation) - 2, 2)] + [".."]))

        # Rotate pages
        all_rotations_cpu = torch.tensor(all_rotations, dtype = torch.int)
        @lru_cache
        def get_all_rotations(device):
            nonlocal all_rotations_cpu
            return all_rotations_cpu.to(device)

        @lru_cache
        def get_buffer(shape, device, dtype):
            return torch.empty(shape, device = device, dtype = dtype)

        for cache in cache_tensors:
            buffer = get_buffer(cache[0].shape, cache.device, cache.dtype)
            all_rotations = get_all_rotations(cache.device)
            ext.cache_rotate(cache, all_rotations, buffer)

        # Write new page indices
        for page in self.all_pages:
            page.page_index = page.new_page_index

        # Debug stuff
        if debug:
            build_page_index()
            build_root_pages()

            def dbg_walk(l, p):
                nonlocal walks
                l = l + [p]
                if not p.children:
                    walks.append(l)
                else:
                    for p in p.children:
                        dbg_walk(l, p)

            print("Cache seqs")
            for page in root_pages:
                walks = []
                dbg_walk([], page)
                for pp in walks:
                    print(" → ".join([f"{p.page_index:2}" for p in pp]))

            torch.cuda.synchronize()
            elapsed = time.time() - time_begin
            print(f"Defrag latency: {elapsed:.5f} s")
</content>

<content full_path="exllamav3/generator/visualizer.py">
from __future__ import annotations
from collections import deque
import math

"""
Quick and dirty visualizer for the paged cache, for debug purposes. Horribly slow and should probably be
rewritten to just draw on a bitmap. 
"""

job_colors = [
    "#00DDFF",
    "#9800FF",
    "#D8FF00",
    "#00FFA5",
    "#FF00E4",
    "#FF8800",
    "#057DFF",
    "#FF008C",
    "#00FFE1",
    "#FFFA00",
    "#B6FF00",
    "#D400FF",
    "#FF1900",
    "#FFCC00",
]

empty_color = "#505050"
empty_color_outline = "#707070"

class CacheVisualizer:

    def __init__(
            self,
            num_pages: int,
            window_size: int = (800, 600),
            gap: float = 0.75,
            margin: int = 25
    ):
        import tkinter as tk

        self.num_pages = num_pages
        self.window_size = window_size
        self.gap = gap
        self.margin = margin
        self.chains = []
        self.usage = []

        w, h = self.window_size
        self.root = tk.Tk()
        self.root.title("Cache Map")
        self.canvas = tk.Canvas(
            self.root,
            width = w,
            height = h,
            bg = "#242424",
            highlightthickness = 0,
            borderwidth = 0
        )
        self.canvas.pack(fill = "both", expand = True)

        self.page_rects = []
        for i in range(num_pages):
            rid = self.canvas.create_rectangle(0, 0, 10, 10, fill = empty_color, outline = empty_color_outline)
            self.page_rects.append(rid)

        self._page_grid_layout()
        self.canvas.after_idle(lambda: self.root.bind("<Configure>", self._on_resize))

        self.elements = []
        self.root.update()


    def _page_grid_layout(self):
        w, h = self.window_size
        w -= 2 * self.margin
        h -= 2 * self.margin
        ratio = w / h
        self.w_pages = min(int(math.ceil(math.sqrt(self.num_pages * ratio))), self.num_pages)
        self.h_pages = int(math.ceil(self.num_pages / self.w_pages))
        self.page_bboxes = []
        cell_size_a = w / (self.w_pages + (self.w_pages - 1) * self.gap)
        cell_size_b = h / (self.h_pages + (self.h_pages - 1) * self.gap)
        cell_size = min(cell_size_a, cell_size_b)
        cell_step = cell_size * (1 + self.gap)
        self.cell_step = cell_step
        self.cell_size = cell_size
        self.gap_size = self.gap * cell_size
        for y in range(self.h_pages):
            for x in range(self.w_pages):
                if len(self.page_bboxes) >= self.num_pages:
                    break
                x0 = self.margin + x * cell_step
                y0 = self.margin + y * cell_step
                x1 = x0 + cell_size
                y1 = y0 + cell_size
                rid = self.page_rects[len(self.page_bboxes)]
                self.page_bboxes.append((x0, y0, x1, y1))
                self.canvas.coords(rid, x0, y0, x1, y1)


    def _on_resize(self, event):
        if event.width <= 1 or event.height <= 1:
            return
        w, h = self.window_size
        if (w, h) == (event.width, event.height):
            return
        self.window_size = (event.width, event.height)
        self._page_grid_layout()
        self._update_chains()


    def _update_chains(self):
        for e in self.elements:
            self.canvas.delete(e)
        self.elements.clear()

        cols = [list() for _ in range(self.num_pages)]
        for index, chain in self.chains:
            col = job_colors[index % len(job_colors)]
            for page in chain:
                cols[page] += [col]

        in_handles = []
        out_handles = []
        for page, col in enumerate(cols):
            if not col:
                in_handles.append([])
                out_handles.append([])
                continue
            bbox = self.page_bboxes[page]
            inh = deque()
            outh = deque()
            for i, c in enumerate(col):
                c_br = self.root.tk.call("tk::Darken", c,  135)
                c_dk = self.root.tk.call("tk::Darken", c,  60)
                a = i / len(col)
                b = (i + 1) / len(col)
                x0, y0, x1, y1 = bbox
                h = y1 - y0
                y0, y1 = y0 + a * h, y0 + b * h
                rid = self.canvas.create_rectangle(x0, y0, x1, y1, fill = c, outline = c_br)
                self.elements.append(rid)
                u = self.usage[page] or 0.0
                if u < 1.0:
                    mx = x0 + (x1 - x0) * u
                    rid = self.canvas.create_rectangle(mx, y0, x1, y1, fill = c_dk, outline = c_dk)
                    self.elements.append(rid)
                inh.append((x0, (y0 + y1) * 0.5))
                outh.append((x1, (y0 + y1) * 0.5))
            in_handles.append(inh)
            out_handles.append(outh)

        x = 0
        y = 0
        def start(_x, _y):
            nonlocal x, y
            x, y = _x, _y

        def line(_x, _y):
            nonlocal x, y, col
            aid = self.canvas.create_line(x, y, _x, _y, fill = col, width = 2.0)
            self.elements.append(aid)
            x, y = _x, _y

        def arrow(_x, _y):
            nonlocal x, y, col
            aid = self.canvas.create_line(x, y, _x, _y, arrow = 'last', tags = ("arrow",), fill = col, width = 2.0)
            self.elements.append(aid)
            x, y = _x, _y

        for l_index, (index, chain) in enumerate(self.chains):
            bcol = job_colors[index % len(job_colors)]
            bias = -self.gap_size / 6 + \
                   ((self.gap_size / 3) * l_index + (self.gap_size / 3) * (l_index + 1)) * 0.5 / len(self.chains)
            for page_a, page_b in zip(chain[:-1], chain[1:]):
                if self.usage[page_b]:
                    col = bcol
                else:
                    col = self.root.tk.call("tk::Darken", bcol, 60)

                x0, y0 = out_handles[page_a].popleft()
                x1, y1 = in_handles[page_b].popleft()
                ax0, ay0, ax1, ay1 =  self.page_bboxes[page_a]
                bx0, by0, bx1, by1 =  self.page_bboxes[page_b]
                dy = y1 - y0
                dx = x1 - x0
                cs = self.cell_size
                gs = self.gap_size
                if 0 < dx < cs and abs(dy) < cs:
                    start(x0, y0)
                    line(x0 + gs / 2 + bias, y0)
                    line(x0 + gs / 2 + bias, y1)
                    arrow(x1, y1)
                elif 0 < dx and abs(dy) < cs:
                    start(x0, y0)
                    line(x0 + gs / 2 + bias, y0)
                    line(x0 + gs / 2 + bias, ay1 + gs / 2 + bias)
                    line(x1 - gs / 2 + bias, ay1 + gs / 2 + bias)
                    line(x1 - gs / 2 + bias, y1)
                    arrow(x1, y1)
                elif dy > 0:
                    start(x0, y0)
                    line(x0 + gs / 2 + bias, y0)
                    line(x0 + gs / 2 + bias, by0 - gs / 2 + bias)
                    line(x1 - gs / 2 + bias, by0 - gs / 2 + bias)
                    line(x1 - gs / 2 + bias, y1)
                    arrow(x1, y1)
                else:
                    start(x0, y0)
                    line(x0 + gs / 2 + bias, y0)
                    line(x0 + gs / 2 + bias, by1 + gs / 2 + bias)
                    line(x1 - gs / 2 + bias, by1 + gs / 2 + bias)
                    line(x1 - gs / 2 + bias, y1)
                    arrow(x1, y1)


    def update(self, chains: list[tuple[int, list]], usage: list[float]):
        self.chains = chains
        self.usage = usage
        self._update_chains()
        self.root.update()

</content>

<content full_path="exllamav3/generator/sampler/custom.py">
from .sampler import Sampler
import torch
from typing_extensions import override
from ...tokenizer import Tokenizer
from ...ext import exllamav3_ext as ext
from ...util import next_power_of_2
from ...util.tensor import buffered_arange
import random
from dataclasses import dataclass
from enum import Enum
from ...util import profile_opt

class SS(Enum):
    INIT = 0  # only state.in_logits is valid
    DONE = 1  # finished, state.sample is valid
    LOGITS = 2  # state.logits is valid
    PROBS = 3  # state.probs is valid
    LOGITS_S = 4  # state.logits is valid, state.indices is valid
    PROBS_S = 5  # state.probs is valid but not normalized, indices are valid
    PROBS_N = 6  # state.probs is valid and normalized
    PROBS_N_S = 7  # state.probs is valid and normalized, indices are valid

@dataclass
class SamplingState:
    rand_u32: int
    bsz: int
    dim: int
    in_logits: torch.Tensor | None = None
    logits: torch.Tensor | None = None
    sample: torch.Tensor | None = None
    probs: torch.Tensor | None = None
    indices: torch.Tensor | None = None
    past_ids: torch.Tensor | None = None
    state: SS = SS.INIT

    def empty_sample(self):
        assert self.sample is None
        return torch.empty((self.bsz, 1), dtype = torch.long, device = self.in_logits.device)

    def empty_probs(self, reuse = True):
        if reuse and self.probs is not None:
            return self.probs
        return torch.empty((self.bsz, self.dim), dtype = torch.float, device = self.in_logits.device)

    def empty_logits(self, reuse = True):
        if reuse and self.logits is not None:
            return self.logits
        return torch.empty((self.bsz, self.dim), dtype = torch.float, device = self.in_logits.device)


class SS_Base:
    def run(self, state: SamplingState):
        raise NotImplementedError()
    def prep(self, in_state: SS):
        return None
    def alt(self):
        return None
    def reqs_past_ids(self):
        return False
    def reqs_torch_seed(self):
        return False


class SS_NoOp(SS_Base):
    """
    Empty sampling step
    """
    def run(self, state: SamplingState):
        pass


class SS_Argmax(SS_Base):
    """
    Final sampling step: select most likely token
    """
    def run(self, state: SamplingState):
        match state.state:
            case SS.INIT:
                state.sample = torch.argmax(state.in_logits, dim = -1)
            case SS.LOGITS:
                state.sample = torch.argmax(state.logits, dim = -1)
            case SS.PROBS | SS.PROBS_N:
                state.sample = torch.argmax(state.probs, dim = -1)
            case SS.LOGITS_S:
                temp = torch.argmax(state.logits, dim = -1)
                state.state = state.indices[temp]
            case SS.PROBS_S | SS.PROBS_N_S:
                temp = torch.argmax(state.probs, dim = -1)
                state.state = state.indices[temp]
        state.state = SS.DONE


class SS_Sample(SS_Base):
    """
    Final sampling step: categorical sampling, randomly sample from (truncated and/or modified) distribution
    """
    def run(self, state: SamplingState):
        # TODO: Fused Gumbel noise + argmax kernel
        # TODO: Evaluate if multinomial sampling from sorted prob. distribution is more efficient
        match state.state:
            case SS.INIT:
                state.logits = torch.empty_like(state.in_logits)
                ext.gumbel_noise_f16(state.in_logits, state.logits, state.rand_u32)
                state.sample = torch.argmax(state.logits, dim = -1)
            case SS.LOGITS:
                ext.gumbel_noise_f32(state.logits, state.logits, state.rand_u32)
                state.sample = torch.argmax(state.logits, dim = -1)
            case SS.PROBS | SS.PROBS_N:
                ext.gumbel_noise_log(state.probs, state.probs, state.rand_u32)
                state.sample = torch.argmax(state.probs, dim = -1)
            case SS.LOGITS_S:
                ext.gumbel_noise_f32(state.logits, state.logits, state.rand_u32)
                temp = torch.argmax(state.logits, dim = -1)
                state.sample = state.indices[buffered_arange(state.bsz, state.in_logits.device), temp]
            case SS.PROBS_S | SS.PROBS_N_S:
                ext.gumbel_noise_log(state.probs, state.probs, state.rand_u32)
                temp = torch.argmax(state.probs, dim = -1)
                state.sample = state.indices[buffered_arange(state.bsz, state.in_logits.device), temp]
        state.state = SS.DONE


class SS_Sample_mn(SS_Sample):
    """
    Categorical sampling, but only using torch.multinomial (for testing/validation)
    """
    def run(self, state: SamplingState):
        match state.state:
            case SS.PROBS_N_S | SS.PROBS_N:
                state.sample = torch.multinomial(state.probs, num_samples = 1)
            case _:
                raise ValueError("Sampling logic error")
        state.state = SS.DONE

    def prep(self, in_state: SS):
        match in_state:
            case SS.INIT | SS.LOGITS | SS.PROBS | SS.LOGITS_S | SS.PROBS_S:
                return [SS_Normalize]
            case _:
                return None

    def reqs_torch_seed(self):
        return True


class SS_Temperature(SS_Base):
    """
    Modify distribution with temperature scaling
    """
    def __init__(self, temperature: float):
        self.temperature = temperature

    def run(self, state: SamplingState):
        match state.state:
            case SS.INIT:
                state.logits = state.in_logits.float()
                state.logits /= self.temperature
                state.state = SS.LOGITS
            case SS.LOGITS:
                state.logits /= self.temperature
            case SS.PROBS | SS.PROBS_N:
                state.probs.pow_(1.0 / self.temperature)
                state.state = SS.PROBS
            case SS.LOGITS_S:
                state.logits /= self.temperature
            case SS.PROBS_S | SS.PROBS_N_S:
                state.probs.pow_(1.0 / self.temperature)
                state.state = SS.PROBS_S

    def alt(self):
        if self.temperature == 1.0:
            return SS_NoOp()
        return None


class SS_Normalize(SS_Base):
    """
    Normalize distribution
    """
    def run(self, state: SamplingState):
        match state.state:
            case SS.INIT:
                state.probs = torch.softmax(state.in_logits.float(), dim = -1)
                state.state = SS.PROBS_N
            case SS.LOGITS:
                state.probs = torch.softmax(state.logits, dim = -1)
                state.state = SS.PROBS_N
            case SS.PROBS:
                state.probs /= state.probs.sum(dim = -1, keepdim = True)
                state.state = SS.PROBS_N
            case SS.LOGITS_S:
                state.probs = torch.softmax(state.logits, dim = -1)
                state.state = SS.PROBS_N_S
            case SS.PROBS_S:
                state.probs /= state.probs.sum(dim = -1, keepdim = True)
                state.state = SS.PROBS_N_S
            case SS.PROBS_N | SS.PROBS_N_S:
                pass


class SS_Sort(SS_Base):
    """
    Sort tokens by descending probability.
    """
    def run(self, state: SamplingState):
        match state.state:
            case SS.INIT:
                logits = state.in_logits.to(torch.float, copy = True)
                state.logits, state.indices = torch.sort(logits, dim = -1, descending = True)
                state.state = SS.LOGITS_S
            case SS.LOGITS:
                state.logits, state.indices = torch.sort(state.logits, dim = -1, descending = True)
                state.state = SS.LOGITS_S
            case SS.PROBS:
                state.probs, state.indices = torch.sort(state.probs, dim = -1, descending = True)
                state.state = SS.PROBS_S
            case SS.PROBS_N:
                state.probs, state.indices = torch.sort(state.probs, dim = -1, descending = True)
                state.state = SS.PROBS_N_S
            case SS.LOGITS_S | SS.PROBS_S | SS.PROBS_N_S:
                pass


class SS_TopK(SS_Base):
    """
    Mask out all but the top K most likely tokens
    """
    def __init__(self, top_k: int):
        assert isinstance(top_k, int) or top_k.is_integer(), "top_k value must be integer"
        self.top_k = int(top_k)

    def run(self, state: SamplingState):
        match state.state:
            case SS.PROBS_S | SS.PROBS_N_S:
                state.probs[..., self.top_k:] = 0.0
                state.state = SS.PROBS_S
            case SS.LOGITS_S:
                state.logits[..., self.top_k:] = -float("inf")
            case _:
                raise ValueError("Sampling logic error")

    def prep(self, in_state: SS):
        match in_state:
            case SS.INIT | SS.LOGITS | SS.PROBS | SS.PROBS_N:
                return [SS_Sort]
            case _:
                return None

    def alt(self):
        if self.top_k < 1:
            return SS_NoOp()
        return None


class SS_TopP(SS_Base):
    """
    Identify the smallest set of top tokens with a cumulative probability greater than P, mask out all
    remainig tokens
    """
    def __init__(self, top_p: float):
        self.top_p = top_p
        assert 0.0 <= top_p <= 1.0

    def run(self, state: SamplingState):
        match state.state:
            case SS.PROBS_N_S:
                cumsum = state.probs.cumsum(dim = -1)
                mask = cumsum <= self.top_p
                state.probs[..., 1:] *= mask[..., 1:]
                state.state = SS.PROBS_S
            case _:
                raise ValueError("Sampling logic error")

    def prep(self, in_state: SS):
        match in_state:
            case SS.PROBS_N:
                return [SS_Sort]
            case SS.INIT | SS.LOGITS | SS.PROBS:
                return [SS_Normalize, SS_Sort]
            case SS.LOGITS_S | SS.PROBS_S:
                return [SS_Normalize]
            case _:
                return None

    def alt(self):
        if self.top_p == 1.0:
            return SS_NoOp()
        return None


class SS_MinP(SS_Base):
    """
    Mask out all tokens whose probability is less than the top token's probability times min_p
    """
    def __init__(self, min_p: float):
        self.min_p = min_p
        assert 0.0 <= min_p <= 1.0

    def run(self, state: SamplingState):
        match state.state:
            case SS.PROBS_N:
                threshold = state.probs.amax(dim = -1, keepdim = True) * self.min_p
                mask = state.probs >= threshold
                state.probs *= mask
                state.state = SS.PROBS
            case SS.PROBS_N_S:
                threshold = state.probs[:, :1] * self.min_p
                mask = state.probs >= threshold
                state.probs *= mask
                state.state = SS.PROBS_S
            case _:
                raise ValueError("Sampling logic error")

    def prep(self, in_state: SS):
        match in_state:
            case SS.INIT | SS.LOGITS | SS.PROBS | SS.LOGITS_S | SS.PROBS_S:
                return [SS_Normalize]
            case _:
                return None

    def alt(self):
        if self.min_p == 0.0:
            return SS_NoOp()
        return None


class SS_RepP(SS_Base):
    """
    Apply Transformers style repetition penalties based on past token IDs. Must be the first step in sampler
    chain.
    """
    def __init__(
        self,
        rep_p: float = 1.0,
        sustain_range: int = int(10e7),
        decay_range: int = 0
    ):
        """
        :param rep_p:
            Multiplicative penalty. rep_p = 1.0 means no penalty. Positive logits are divided by this value and
            negative ones are multiplied by it. Recreates the method from the Transformers generate() pipeline,
            following https://arxiv.org/pdf/1909.05858.pdf which relies on the assumption that logits output
            straight from the model are "centered" around zero.
         :param sustain_range:
            Number of most recent past tokens over which to apply full penalty
        :param decay_range:
            Number tokens (after sustain_range) over which the penalty gradually fades out
        """
        self.rep_p = rep_p
        self.sustain_range = sustain_range
        self.decay_range = decay_range

    def run(self, state: SamplingState):
        match state.state:
            case SS.INIT:
                state.logits = torch.empty_like(state.in_logits, dtype = torch.float)
                ext.apply_rep_pens(
                    state.in_logits,
                    state.logits,
                    state.past_ids,
                    self.rep_p,
                    self.sustain_range,
                    self.decay_range
                )
            case SS.LOGITS:
                ext.apply_rep_pens(
                    state.logits,
                    state.logits,
                    state.past_ids,
                    self.rep_p,
                    self.sustain_range,
                    self.decay_range
                )
            case _:
                raise ValueError("Sampling logic error")
        state.state = SS.LOGITS

    def alt(self):
        if self.rep_p == 1.0 or self.sustain_range + self.decay_range <= 0:
            return SS_NoOp()
        return None

    def reqs_past_ids(self):
        return True


class SS_PresFreqP(SS_Base):
    """
    Apply OAI-style presence and frequency penalties based on past token IDs. Must be the first step in the
    sampler chain.
    """
    def __init__(
        self,
        pres_p: float = 0.0,
        freq_p: float = 0.0,
        sustain_range: int = int(10e7),
        decay_range: int = 0
    ):
        """
        :param pres_p:
            Additive penalty, OAI style. 0.0 means no penalty. Added to logit once if a token appears in
            past_ids
        :param freq_p:
            Additive penalty, OAI style. 0.0 means no penalty. Added to logit for every time a token is
            encountered in past_ids
         :param sustain_range:
            Number of most recent past tokens over which to apply full penalty
        :param decay_range:
            Number tokens (after sustain_range) over which the penalty gradually fades out
        """
        self.pres_p = pres_p
        self.freq_p = freq_p
        self.sustain_range = sustain_range
        self.decay_range = decay_range

    def run(self, state: SamplingState):
        match state.state:
            case SS.INIT:
                state.logits = torch.empty_like(state.in_logits, dtype = torch.float)
                ext.apply_pres_freq_pens(
                    state.in_logits,
                    state.logits,
                    state.past_ids,
                    self.pres_p,
                    self.freq_p,
                    self.sustain_range,
                    self.decay_range
                )
            case SS.LOGITS:
                ext.apply_pres_freq_pens(
                    state.logits,
                    state.logits,
                    state.past_ids,
                    self.pres_p,
                    self.freq_p,
                    self.sustain_range,
                    self.decay_range
                )
            case _:
                raise ValueError("Sampling logic error")
        state.state = SS.LOGITS

    def alt(self):
        if (self.pres_p == 0.0 and self.freq_p == 0.0) or self.sustain_range + self.decay_range <= 0:
            return SS_NoOp()
        return None

    def reqs_past_ids(self):
        return True


class CustomSampler(Sampler):
    def __init__(
        self,
        steps: list[SS_Base]
    ):
        super().__init__()

        self.steps = []
        state = SS.INIT
        for step in steps:
            self.reqs_past_ids = self.reqs_past_ids or step.reqs_past_ids()
            self.reqs_torch_seed = self.reqs_torch_seed or step.reqs_torch_seed()
            alt = step.alt()
            if alt:
                step = alt
            prep_steps = step.prep(state)
            if prep_steps:
                for prep_step in prep_steps:
                    self.steps.append(prep_step())
            self.steps.append(step)

        # TODO: Identify and remove redundant sampling steps, add rules for fusing steps where possible

    @override
    @torch.inference_mode
    def forward(
        self,
        logits,
        sequence_ids: torch.Tensor | None = None,
        rand_u32: int | None = None,
        tokenizer: Tokenizer | None = None,
        blocked_tokens: list[int] | None = None,
        allowed_tokens: list[int] | None = None,
        return_state: bool = False
    ):
        out_shape = logits.shape[:-1]

        if tokenizer is not None:
            logits[..., tokenizer.actual_vocab_size:] = -float("inf")

        if rand_u32 is None:
            rand_u32 = random.randint(0, (1<<32) - 1)
        else:
            if self.reqs_torch_seed:
                torch.manual_seed(rand_u32)
                random.seed(rand_u32)

        dim = logits.shape[-1]
        bsz = logits.numel() // dim

        # Prepare logit bias tensor

        # TODO: Extension function for this, combine with filter API when it's added
        if blocked_tokens is not None or allowed_tokens is not None:
            logits = logits.clone()
        if blocked_tokens is not None:
            logits[..., blocked_tokens] = float('-inf')
        if allowed_tokens is not None:
            mask = torch.zeros(logits.shape[-1], dtype = torch.bool, device = logits.device)
            mask[allowed_tokens] = True
            logits[..., ~mask] = float('-inf')

        state = SamplingState(
            rand_u32 = rand_u32,
            dim = dim,
            bsz = bsz,
            in_logits = logits.view(bsz, dim),
            past_ids = sequence_ids,
        )

        for ss in self.steps:
            assert state.state != SS.DONE, "Sampling logic error"
            ss.run(state)
        assert return_state or state.state == SS.DONE, "Sampling logic error"

        return state if return_state else state.sample.view(out_shape)
</content>

<content full_path="exllamav3/generator/sampler/presets.py">
from .custom import *

class DefaultSampler(CustomSampler):
    """
    Sensible default for most models
    """
    def __init__(self):
        super().__init__([
            SS_MinP(0.08),
            SS_Temperature(0.8),
            SS_Sample()
        ])

class ArgmaxSampler(CustomSampler):
    """
    Returns top token
    """
    def __init__(self):
        super().__init__([
            SS_Argmax()
        ])

GreedySampler = ArgmaxSampler

class CategoricalSampler(CustomSampler):
    """
    Samples from unmodified categorical distribution
    """
    def __init__(self, temperature: float = 1.0):
        if temperature == 0:
            super().__init__([
                SS_Argmax()
            ])
        else:
            super().__init__([
                SS_Temperature(temperature),
                SS_Sample()
            ])

GumbelSampler = CategoricalSampler

class TopKSampler(CustomSampler):
    """
    Truncates distribution to top_k values before sampling
    """
    def __init__(self, top_k: int, temperature: float = 1.0):
        assert top_k >= 1
        if top_k == 1 or temperature == 0:
            super().__init__([
                SS_Argmax()
            ])
        else:
            super().__init__([
                SS_Temperature(temperature),
                SS_TopK(top_k),
                SS_Sample()
            ])

class TopPSampler(CustomSampler):
    """
    Truncates distribution to the top probabilities <= top_p (at least 1 candidate) before sampling
    """
    def __init__(self, top_p: float, temperature: float = 1.0, temperature_last = False):
        if top_p == 0 or temperature == 0:
            super().__init__([
                SS_Argmax()
            ])
        else:
            if temperature_last:
                super().__init__([
                    SS_TopP(top_p),
                    SS_Temperature(temperature),
                    SS_Sample()
                ])
            else:
                super().__init__([
                    SS_Temperature(temperature),
                    SS_TopP(top_p),
                    SS_Sample()
                ])

class ComboSampler(CustomSampler):
    """
    Single class with an argument for each sampling step
    """
    def __init__(
        self,
        rep_p: float = 1.0,
        freq_p: float = 0.0,
        pres_p: float = 0.0,
        rep_sustain_range: int = int(10e7),
        rep_decay_range: int = 0,
        temperature: float = 1.0,
        min_p: float = 0.0,
        top_k: int = 0,
        top_p: float = 1.0,
        temp_last: bool = False,
    ):
        # Steps with default parameters become no-ops
        stack = [
            SS_RepP(rep_p, rep_sustain_range, rep_decay_range),
            SS_PresFreqP(pres_p, freq_p, rep_sustain_range, rep_decay_range),
        ]

        if temperature == 0.0 or top_k == 1:
            stack += [
                SS_Argmax()
            ]
        else:
            stack += [
                SS_Temperature(temperature if not temp_last else 1.0),
                SS_MinP(min_p),
                SS_TopK(top_k),
                SS_TopP(top_p),
                SS_Temperature(temperature if temp_last else 1.0),
                SS_Sample()
            ]

        super().__init__(stack)
</content>

<content full_path="exllamav3/generator/sampler/__init__.py">

from .sampler import Sampler
from .custom import (
    CustomSampler,
    SS_Base,
    SS_Argmax,
    SS_Sample,
    SS_Sample_mn,
    SS_Temperature,
    SS_Normalize,
    SS_Sort,
    SS_MinP,
    SS_TopK,
    SS_TopP,
    SS_NoOp,
    SS_RepP,
    SS_PresFreqP,
)
from .presets import (
    DefaultSampler,
    ArgmaxSampler,
    GreedySampler,
    CategoricalSampler,
    GumbelSampler,
    TopKSampler,
    TopPSampler,
    ComboSampler,
)
</content>

<content full_path="exllamav3/generator/sampler/sampler.py">
from abc import abstractmethod
import torch
from ...tokenizer import Tokenizer

class Sampler:
    def __init__(self):
        self.reqs_past_ids = False
        self.reqs_torch_seed = False

    @abstractmethod
    def forward(
        self,
        logits,
        sequence_ids: torch.Tensor | None = None,
        rand_u32: int | None = None,
        tokenizer: Tokenizer | None = None,
        blocked_tokens: list[int] | None = None,
        allowed_tokens: list[int] | None = None,
        return_state: bool = False
    ):
        pass
</content>

<content full_path="exllamav3/loader/__init__.py">
from .safetensors import SafetensorsCollection, VariantSafetensorsCollection
</content>

<content full_path="exllamav3/loader/safetensors.py">
from __future__ import annotations

from dataclasses import dataclass

import torch
import os, glob
import numpy as np
import json
import mmap
from ..util import Timer, cuda_sync_active
from ..ext import exllamav3_ext as ext
from functools import lru_cache

MAX_DEFERRED_LOAD_CHUNK = 2*1024**2

def convert_dtype(dt: str):
    if dt == "I32": return torch.int, np.int32, 4
    elif dt == "I16": return torch.short, np.int16, 2
    elif dt == "F16": return torch.float16, np.float16, 2
    elif dt == "BF16": return torch.bfloat16, np.float16, 2
    elif dt == "F32": return torch.float, np.float32, 4
    else:
        raise ValueError(f"Unknown dtype {dt}")


def read_header(filename: str) -> dict:
    with open(filename, "rb") as fp:
        header_size = np.fromfile(fp, dtype = np.int64, count = 1).item()
        header_json = fp.read(header_size)
        header = json.loads(header_json.decode("utf-8"))
        header["_header_offset"] = fp.tell()
        return header


@dataclass
class STCMetrics:
    bytes_loaded: int = 0
    time_elapsed: float = 0.0
    deferred_tensors: int = 0
    deferred_passes: int = 0
    direct_tensors: int = 0
    total_chunks: int = 0
    def bandwidth(self):
        return self.bytes_loaded / (1024 ** 3) / self.time_elapsed
    def print(self):
        print(f" -- Total size: {self.bytes_loaded:,} bytes, {self.bytes_loaded / 1024**3:.2f} GB")
        print(f" -- Load time: {self.time_elapsed:.3f} seconds")
        print(f" -- Bandwidth: {self.bandwidth():.3f} GB / s")
        print(f" -- Deferred: {self.deferred_tensors:,} tensors in {self.deferred_passes:,} passes, {self.total_chunks:,} chunks")
        print(f" -- Direct: {self.direct_tensors:,} tensors")


class SafetensorsCollection:

    def __init__(
        self,
        directory: str,
        load_method: str | None = None
    ):
        """
        Scan directory for .safetensors files and build collection, preparing to load tensors indexed by key.

        :param directory:
            Directory to scan.

        :param load_method:
            - "mt_fread": multithreaded C++ loader using fread
            - "python": use fp.seek() and fp.read() to load tensor data via bytearray and torch.frombuffer
        """

        self.directory = directory
        self.tensor_file_map = {}
        self.file_headers = {}
        self.handles: dict[str, list | None] = {}
        self.load_method = load_method or "mt_fread"

        self.metrics = STCMetrics()

        self.tensor_files = []
        self.add_tensor_files(directory)

        self.new_tensors = None
        self.deferred_mode = False
        self.deferred_loads = []


    def add_tensor_files(
        self,
        directory: str,
        warn_if_override: bool = True
    ):
        st_pattern = os.path.join(directory, "*.safetensors")
        new_tensor_files = glob.glob(st_pattern)
        self.tensor_files += new_tensor_files

        overrides = 0
        for st_file in new_tensor_files:
            self.handles[st_file] = None
            header = read_header(st_file)
            self.file_headers[st_file] = header
            for key in header.keys():
                if key in ["__metadata__", "_header_offset"]:
                    continue
                if key in self.tensor_file_map and warn_if_override:
                    # print(f" !! Overriding {key} from {self.tensor_file_map[key]} with f{st_file}")
                    overrides += 1
                self.tensor_file_map[key] = st_file
        if overrides:
            print(f" !! Replaced {overrides} tensors from {directory}")


    def has_tensor(
        self,
        key: str,
    ):
        if self.new_tensors and key in self.new_tensors:
            return True
        return key in self.tensor_file_map


    def has_tensor_group(
        self,
        key: str,
        subkeys: list,
    ):
        sources = [self.tensor_file_map]
        if self.new_tensors:
            sources += [self.new_tensors]
        return any(
            all(
                (
                    f"{key}.{subkey}" in source if isinstance(subkey, str) else
                    any(f"{key}.{sk}" in source for sk in subkey)
                ) for subkey in subkeys
            ) for source in sources
        )


    def get_tensor_sizes(
        self,
        prefix: str,
    ):
        assert self.new_tensors is None  # TODO
        keys = [
            key for key in self.tensor_file_map.keys()
            if key == prefix or key.startswith(prefix + ".")
        ]
        sizes = [self.get_tensor_size(key) for key in keys]
        return sizes


    def get_tensor_size(
        self,
        key: str,
        optional: bool = False
    ):
        assert self.new_tensors is None  # TODO
        if not key in self.tensor_file_map:
            if not optional:
                raise ValueError(f"Required tensor {key} not found in any *.safetensors file in {self.directory}")
            else:
                return 0

        filename = self.tensor_file_map[key]
        header = self.file_headers[filename]
        h = header[key]
        # _, _, esize = convert_dtype(h["dtype"])
        # bytesize = np.prod(h["shape"]) * esize
        beg, end = h["data_offsets"]
        bytesize = end - beg
        return bytesize


    def list_tensors(
        self,
        prefix: str,
    ) -> dict:
        assert self.new_tensors is None  # TODO
        keys = [
            key for key in self.tensor_file_map.keys()
            if key == prefix or key.startswith(prefix + ".")
        ]
        results = {}
        for key in keys:
            filename = self.tensor_file_map[key]
            header = self.file_headers[filename]
            h = header[key]
            dtype, np_dtype, esize = convert_dtype(h["dtype"])
            beg, end = h["data_offsets"]
            results[key] = {
                "shape": h["shape"],
                "n_bytes": end - beg,
                "dtype": str(dtype),
            }
        return results


    # TODO: deferred load
    def get_tensors(
        self,
        prefix: str,
        device: torch.device | None = None,
        allow_bf16: bool = False,
    ) -> dict:
        assert self.new_tensors is None  # TODO
        keys = [
            key for key in self.tensor_file_map.keys()
            if key == prefix or key.startswith(prefix + ".")
        ]
        result = {key: self.get_tensor(key, device, allow_bf16 = allow_bf16) for key in keys}
        return result


    # TODO: deferred load
    def get_tensor(
        self,
        key: str,
        device: torch.device | None = None,
        optional: bool = False,
        allow_bf16: bool = False,
        float2half: bool = False,
        no_defer: bool = False,
        transpose: bool = False,
        pad_to: tuple = None,
    ) -> torch.Tensor | None:

        if device is None:
            device = torch.device("cpu")

        if self.new_tensors and key in self.new_tensors:
            tensor = self.new_tensors[key].to(device)
            if transpose:
                tensor = tensor.T.contiguous()
            return tensor

        if not key in self.tensor_file_map:
            if not optional:
                raise ValueError(f"Required tensor {key} not found in any *.safetensors file in {self.directory}")
            else:
                return None

        filename = self.tensor_file_map[key]
        header = self.file_headers[filename]
        h = header[key]
        offset = header["_header_offset"]

        dtype, np_dtype, esize = convert_dtype(h["dtype"])
        beg, end = h["data_offsets"]
        bytesize = end - beg
        shape = h["shape"]
        numel = np.prod(shape)
        assert numel * esize == bytesize, \
            f"Incorrect size of {key} in {filename}"

        load_method = self.load_method
        if load_method == "mt_fread" and self.deferred_mode and not no_defer:
            load_method = "defer"

        with (Timer() as timer):
            match load_method:
                case "defer":
                    h = self.handles[filename]
                    if not h:
                        try:
                            h = ext.stloader_open_file(filename)
                            self.handles[filename] = h
                        except RuntimeError as e:
                            print(f" ## Error opening {filename}")
                            raise e
                    bf16_to_fp16 = (dtype == torch.bfloat16 and not allow_bf16)
                    fp32_to_fp16 = (dtype == torch.float and float2half)
                    load_shape = tuple(shape)
                    load_shape_t = load_shape if not transpose else (shape[1], shape[0])
                    load_dtype = dtype
                    if bf16_to_fp16 and load_dtype == torch.bfloat16:
                        load_dtype = torch.half
                    final_shape = pad_to if pad_to is not None else load_shape_t
                    final_dtype = dtype if not (bf16_to_fp16 or fp32_to_fp16) else torch.float16
                    if final_shape == load_shape_t:
                        tensor = torch.empty(final_shape, dtype = final_dtype, device = device)
                    else:
                        tensor = torch.zeros(final_shape, dtype = final_dtype, device = device)
                    if transpose or fp32_to_fp16 or final_shape != load_shape_t:
                        temp_tensor = torch.empty(load_shape, dtype = load_dtype, device = device)
                    else:
                        temp_tensor = None
                    self.deferred_loads.append({
                        "filename": filename,
                        "file_offset": offset + beg,
                        "bytesize": bytesize,
                        "temp_tensor": temp_tensor,
                        "dest_tensor": tensor,
                        "bf16_to_fp16": bf16_to_fp16,
                        "fp32_to_fp16": fp32_to_fp16,
                        "cuda": tensor.is_cuda,
                        "device_id": tensor.device.index if tensor.is_cuda else -1,
                        "transpose": transpose,
                    })
                    self.metrics.deferred_tensors += 1

                case "mt_fread":
                    h = self.handles[filename]
                    if not h:
                        try:
                            h = ext.stloader_open_file(filename)
                            self.handles[filename] = h
                        except RuntimeError as e:
                            print(f" ## Error opening {filename}")
                            raise e
                    tensor = torch.empty(shape, dtype = dtype, device = device)
                    assert tensor.is_contiguous()
                    if device != "cpu":
                        cuda_sync_active()
                    ext.stloader_read(
                        h,
                        offset + beg,
                        bytesize,
                        tensor,
                    )
                    if tensor.dtype == torch.bfloat16 and not allow_bf16:
                        tensor = tensor.to(torch.float16)
                    if tensor.dtype == torch.float and float2half:
                        tensor = tensor.to(torch.float16)
                    if transpose:
                        tensor = tensor.T
                    if pad_to is not None:
                        padded = torch.zeros(pad_to, dtype = tensor.dtype, device = tensor.device)
                        padded[tuple(slice(0, s) for s in tensor.shape)].copy_(tensor)
                        tensor = padded
                    tensor = tensor.contiguous()
                    self.metrics.direct_tensors += 1


                case "python":
                    with open(filename, "rb") as fp:
                        fp.seek(offset + beg)
                        buffer = bytearray(fp.read(bytesize))
                        tensor = torch.frombuffer(buffer, dtype = dtype, count = numel).reshape(shape)
                        if tensor.dtype == torch.bfloat16 and not allow_bf16:
                            tensor = tensor.to(torch.float16)
                        if tensor.dtype == torch.float and float2half:
                            tensor = tensor.to(torch.float16)
                        if transpose:
                            tensor = tensor.T
                        if pad_to is not None:
                            padded = torch.zeros(pad_to, dtype = tensor.dtype, device = tensor.device)
                            padded[tuple(slice(0, s) for s in tensor.shape)].copy_(tensor)
                            tensor = padded
                        tensor = tensor.to(device).contiguous()
                    self.metrics.direct_tensors += 1

                case _:
                    raise ValueError(f"Invalid load_method: {load_method}")

        self.metrics.bytes_loaded += bytesize
        self.metrics.time_elapsed += timer.interval

        return tensor


    def close(self):
        assert self.new_tensors is None
        for filename, h in self.handles.items():
            if h:
                ext.stloader_close_file(h)
                self.handles[filename] = None


    @lru_cache
    def max_key_len(self):
        l = max(len(k) for k in self.tensor_file_map.keys())
        return l


    def set_new_tensors(self, new_tensors):
        self.new_tensors = new_tensors


    def begin_deferred_load(self):
        assert not self.deferred_mode
        self.deferred_mode = True


    def end_deferred_load(self):
        assert self.deferred_mode

        with (Timer() as timer):

            cpu_loads = {}
            cuda_loads = {}
            for load in self.deferred_loads:
                filenmame = load["filename"]
                cuda = load["cuda"]
                if cuda:
                    if not filenmame in cuda_loads:
                        cuda_loads[filenmame] = []
                    cuda_loads[filenmame].append(load)
                else:
                    if not filenmame in cpu_loads:
                        cpu_loads[filenmame] = []
                    cpu_loads[filenmame].append(load)

            def make_workload(l):
                wl = []
                for w in l:
                    if w["temp_tensor"] is not None:
                        dst = w["temp_tensor"].data_ptr()
                    else:
                        # Not transposing, padding or converting fp32->fp16, load directly
                        dst = w["dest_tensor"].data_ptr()
                    bytesize = w["bytesize"]
                    src = w["file_offset"]
                    while bytesize > 0:
                        j = ext.TensorLoadJob(
                            self.handles[w["filename"]],
                            src,
                            min(bytesize, MAX_DEFERRED_LOAD_CHUNK),
                            dst,
                            w["bf16_to_fp16"],
                            w["fp32_to_fp16"],
                            w["cuda"],
                            w["device_id"]
                        )
                        src += MAX_DEFERRED_LOAD_CHUNK
                        dst += MAX_DEFERRED_LOAD_CHUNK
                        bytesize -= MAX_DEFERRED_LOAD_CHUNK
                        wl.append(j)
                return wl

            for filename, loads in cpu_loads.items():
                loads = sorted(loads, key = lambda c: -c["bytesize"])
                workload = make_workload(loads)
                self.metrics.total_chunks += len(workload)
                ext.stloader_deferred_cpu(workload)
                for w in loads:
                    if w["temp_tensor"] is not None:
                        src = w["temp_tensor"]
                        if w["transpose"]:
                            src = src.T
                        unpadded_idx = tuple(slice(0, s) for s in src.shape)
                        w["dest_tensor"][unpadded_idx].copy_(src)

            for filename, loads in cuda_loads.items():
                loads = sorted(loads, key = lambda c: -c["bytesize"])
                workload = make_workload(loads)
                self.metrics.total_chunks += len(workload)
                ext.stloader_deferred_cuda(workload, MAX_DEFERRED_LOAD_CHUNK)
                for w in loads:
                    if w["temp_tensor"] is not None:
                        src = w["temp_tensor"]
                        if w["transpose"]:
                            src = src.T
                        unpadded_idx = tuple(slice(0, s) for s in src.shape)
                        w["dest_tensor"][unpadded_idx].copy_(src)

        self.metrics.time_elapsed += timer.interval
        self.metrics.deferred_passes += 1

        self.deferred_mode = False
        self.deferred_loads = []


    def abort_deferred_load(self):
        self.deferred_mode = False
        self.deferred_loads = []


class VariantSafetensorsCollection:

    def __init__(
        self,
        tensor_map: dict[str, str],
        **kwargs
    ):
        self.tensor_map = None
        self.tensor_map_sort = None
        self.all_dirs = None
        self.stcs = {}
        self.kwargs = kwargs
        self.update_map(tensor_map)


    def update_map(
        self,
        tensor_map: dict[str, str]
    ):
        self.tensor_map = tensor_map
        self.tensor_map_sort = sorted(tensor_map.items(), key = lambda kv: len(kv[0]), reverse = True)
        all_dirs = list(set(tensor_map.values()))

        for d in all_dirs:
            if d not in self.stcs:
                self.stcs[d] = SafetensorsCollection(directory = d, **self.kwargs)


    def has_tensor(
        self,
        key: str,
    ):
        return any(key in stc.tensor_file_map for stc in self.stcs.values())


    def has_tensor_group(
        self,
        key: str,
        subkeys: list[str],
    ):
        return all(
            any(f"{key}.{subkey}" in stc.tensor_file_map for stc in self.stcs.values())
            for subkey in subkeys
        )


    def get_tensor(
        self,
        key: str,
        device: torch.device | None = None,
        optional: bool = False,
        allow_bf16: bool = False
    ) -> torch.Tensor | None:

        file = None
        for k, v in self.tensor_map_sort:
            if key.startswith(k):
                file = v
                break
        if file is None:
            if not optional:
                raise ValueError(f"No prefix found in variants map with the matching key: {key}")
            else:
                return None

        return self.stcs[file].get_tensor(key, device, optional, allow_bf16)


    def close(self):
        for stc in self.stcs.values():
            stc.close()


    def get_metrics(self):
        res = [stc.get_metrics() for stc in self.stcs.values()]
        bytes_loaded = sum(r[0] for r in res)
        time_elapsed = sum(r[1] for r in res)
        bandwidth = bytes_loaded / (1024**3) / time_elapsed
        return bytes_loaded, time_elapsed, bandwidth

</content>

<content full_path="exllamav3/conversion/__init__.py">

</content>

<content full_path="exllamav3/conversion/quant_config.py">
from ..models import Config, Model
import os, json
import torch

def update_config(
    config_dict: dict
):
    """
    Make necessary updates to config.json
    """
    if "tied_word_embeddings" in config_dict:
        config_dict["tied_word_embeddings"] = True


def create_quantization_config_json(
    model_dir: str
):
    # Create model instance without loading
    config = Config.from_directory(model_dir)
    model = Model.from_config(config)

    # Create tensor map
    storage_dict = {}
    for module in model:
        # Only list leaf nodes
        if len(module.modules) > 0:
            continue

        module_dict = {}
        stored_tensors = config.stc.list_tensors(module.key)
        module_dict["stored_tensors"] = stored_tensors

        qformat = module.quant_format_id()
        if qformat == "exl3":
            shape = stored_tensors[f"{module.key}.trellis"]["shape"]

            mul1 = config.stc.get_tensor(f"{module.key}.mul1", optional = True, no_defer = True)
            mul1_mult = mul1.view(torch.uint32).item() if mul1 is not None else 0
            mcg = config.stc.get_tensor(f"{module.key}.mcg", optional = True, no_defer = True)
            mcg_mult = mcg.view(torch.uint32).item() if mcg is not None else 0

            module_dict["quant_format"] = "exl3"
            module_dict["bits_per_weight"] = shape[-1] // 16
            if mul1_mult:
                module_dict["mul1_multiplier"] = mul1_mult
            if mcg_mult:
                module_dict["mcg_multiplier"] = mcg_mult

        storage_dict[module.key] = module_dict

    # Grab quantization_config from config.json
    with open(os.path.join(model_dir, "config.json"), "r") as f:
        config_dict = json.load(f)
        assert "quantization_config" in config_dict, f"{model_dir} does not appear to be a quantized model"
        quantization_config = config_dict["quantization_config"]

    # Update config with storage data
    quantization_config["tensor_storage"] = storage_dict

    # Write
    with open(os.path.join(model_dir, "quantization_config.json"), "w") as f:
        f.write(json.dumps(quantization_config, indent = 4))

</content>

<content full_path="exllamav3/conversion/compile.py">
import os
import shutil
import json
from ..loader.safetensors import SafetensorsCollection
from ..version import __version__
from safetensors.torch import save_file
from ..util.memory import free_mem
from ..modules import Module
from .quant_config import update_config, create_quantization_config_json

def tsize(t):
    return t.nelement() * t.element_size()

def dsize(d):
    size = 0
    for _, v in d.items(): size += tsize(v)
    return size

def compile_model(args, model, config, tokenizer):

    in_dir = args["in_dir"]
    out_dir = args["out_dir"]
    work_dir = args["work_dir"]
    qtensors_dir = os.path.join(work_dir, "qtensors")
    qtensors_stc = SafetensorsCollection(qtensors_dir)

    # Prepare output directory
    if not os.path.exists(out_dir):
        print(f" -- Creating directory {out_dir}")
        os.makedirs(out_dir)
    else:
        print(f" -- Writing into {out_dir}")
        if len(os.listdir(out_dir)) != 0:
            print(f" !! Warning, output directory is not empty")

    # Allocate shards
    total_size = 0
    max_shard_bytes = args["shard_size"] * 1024**2
    out_map = []
    out_map.append([])
    current_shard_size = 0
    for module in model.modules:
        prefix = module.key
        sizes = qtensors_stc.get_tensor_sizes(prefix)
        if len(sizes) == 0:
            continue
        size = sum(sizes)
        if size > max_shard_bytes:
            print(f" !! Warning, unable to fit module {module.key} in single shard of {args['shard_size']} MB")
        if current_shard_size + size > max_shard_bytes and current_shard_size > 0:
            current_shard_size = 0
            out_map.append([])
        current_shard_size += size
        total_size += size
        out_map[-1].append(module)

    # Additional tensors
    extra_tensors = {}
    for _, cls in config.model_classes.items():
        extra_tensors.update(cls.get_additional_compiled_tensors(config))
    for key, data in extra_tensors.items():
        size = data["n_bytes"]
        if size > max_shard_bytes:
            print(f" !! Warning, unable to fit module {module.key} in single shard of {args['shard_size']} MB")
        if current_shard_size + size > max_shard_bytes and current_shard_size > 0:
            current_shard_size = 0
            out_map.append([])
        current_shard_size += size
        total_size += size
        out_map[-1].append(key)

    # Write model tensors
    map_dict = {}
    num_files = len(out_map)
    for file_idx, modules in enumerate(out_map):
        filename = (
            "model.safetensors" if num_files == 1 else
            f"model-{file_idx+1:05}-of-{num_files:05}.safetensors"
        )
        print(f" -- Writing {filename}")
        file_dict = {}
        for m in modules:
            if isinstance(m, Module):
                prefix = m.key
                tensors = qtensors_stc.get_tensors(prefix, allow_bf16 = True)
                tensors = {k: v.contiguous() for k, v in tensors.items()}
                qtensors_stc.close()
            elif isinstance(m, str):
                tensor = config.stc.get_tensor(m, allow_bf16 = True)
                tensors = {m: tensor.contiguous()}
            file_dict.update(tensors)
        for name in file_dict.keys():
            map_dict[name] = filename
        save_file(file_dict, os.path.join(out_dir, filename))
        del file_dict
        free_mem()

    # Copy non-tensor files
    print(f" -- Copying non-tensor files from {in_dir}")
    filtered_files = []
    ignored_files = []
    for f in os.listdir(in_dir):
        if not os.path.isfile(os.path.join(in_dir, f)):
            continue
        if f.endswith(".safetensors"):
            continue
        if f == "config.json":
            continue
        if f == "model.safetensors.index.json":
            continue
        if any(f.endswith(x) for x in [".bin", ".ckpt", ".pth", ".pt"]):
            ignored_files.append(f)
            continue
        filtered_files.append(f)
    for f in filtered_files:
        print(f"     - {f}")
        source_file_path = os.path.join(in_dir, f)
        target_file_path = os.path.join(out_dir, f)
        shutil.copy(source_file_path, target_file_path)
    if ignored_files:
        print(f" !! Warning, the following file(s) will not be included in output model:")
        for f in ignored_files[:10]:
            print(f"     - {f}")
        if len(ignored_files) > 10:
            print(f"     - (+ {len(ignored_files) - 10} more)")

    # Write new model.safetensors.index.json maybe
    if num_files > 1:
        print(f" -- Writing model.safetensors.index.json")
        safetensors_index = {
            "metadata": {
                "total_size": total_size,
            },
            "weight_map": map_dict
        }
        with open(os.path.join(out_dir, "model.safetensors.index.json"), "w") as f:
            f.write(json.dumps(safetensors_index, indent = 4))

    # Update and write config.json
    print(f" -- Writing config.json")
    with open(os.path.join(in_dir, "config.json"), "r") as f:
        config_dict = json.load(f)
    qcfg = {
        "quant_method": "exl3",
        "version": __version__,
        "bits": args["bits"],
        "head_bits": args["head_bits"],
        "calibration": {
            "rows": args["cal_rows"],
            "cols": args["cal_cols"],
        },
        "out_scales": {True: "always", False: "never", None: "auto"}[args["apply_out_scales"]],
    }
    if any(args.get(x) for x in ["mcg_multiplier", "mul1_multiplier"]):
        exp_qcfg = {}
        if args.get("mcg_multiplier"):
            exp_qcfg["mcg_multiplier"] = args.get("mcg_multiplier")
        if args.get("mul1_multiplier"):
            exp_qcfg["mul1_multiplier"] = args.get("mul1_multiplier")
        qcfg["experimental_options"] = exp_qcfg

    update_config(config_dict)
    config_dict["quantization_config"] = qcfg
    with open(os.path.join(out_dir, "config.json"), "w") as f:
        f.write(json.dumps(config_dict, indent = 4))

    # Add extra metadata to quant_config
    print(f" -- Creating quantization_config.json")
    create_quantization_config_json(out_dir)

    print(f" -- Finished compiling model to {out_dir}")



</content>

<content full_path="exllamav3/conversion/allocation.py">
from __future__ import annotations
import math
import bisect
from functools import lru_cache
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from ..modules.linear import Linear

def allocate_transformer(
    bpw: float,
    surplus_bits: int,
    q: Linear | None,
    k: Linear | None,
    v: Linear | None,
    o: Linear | None,
    g: Linear | list[Linear] | None,
    u: Linear | list[Linear] | None,
    d: Linear | list[Linear] | None,
) -> (dict, int):

    # Submodules
    keys = []
    out_keys = {}
    numels = []
    perms_qkvo = []
    perms_gud = []

    if q is not None:
        assert k and v and o
        keys += [m.key for m in (q, k, v, o)]
        numels += [m.weights_numel() for m in (q, k, v, o)]
        for m in (q, k, v, o):
            out_keys[m.key] = m.key
        perms_qkvo = [
            [0, 0, 0, 0],
            [0, 1, 1, 0],
            [0, 2, 2, 0],
            [0, 1, 1, 1],
            [0, 1, 2, 1],
            [1, 2, 2, 1],
        ]

    if g is not None and u is not None:
        assert d
        if isinstance(g, list):
            for m in (g, u, d):
                key_ = m[0].key.replace(".slice.0", ".slice.*").replace(".experts.0.", ".experts.*.")
                keys += [key_]
                numels += [sum(mm.weights_numel() for mm in m)]
                for mm in m:
                    out_keys[mm.key] = key_
        else:
            keys += [m.key for m in (g, u, d)]
            numels += [m.weights_numel() for m in (g, u, d)]
            for m in (g, u, d):
                out_keys[m.key] = m.key
        perms_gud = [
            [0, 0, 0],
            [0, 0, 1],
            [0, 1, 1],
            [1, 1, 1],
        ]

    elif g is None and u is not None:
        assert d
        if isinstance(u, list):
            for m in (u, d):
                key_ = m[0].key.replace(".slice.0", ".slice.*").replace(".experts.0.", ".experts.*.")
                keys += [m]
                numels += [sum(mm.weights_numel() for mm in m)]
                for mm in m:
                    out_keys[mm.key] = key_
        else:
            keys += [m.key for m in (u, d)]
            numels += [m.weights_numel() for m in (u, d)]
            for m in (u, d):
                out_keys[m.key] = m.key
        perms_gud = [
            [0, 0],
            [0, 1],
            [1, 1],
        ]

    # Bits per weight from budget
    numel = sum(numels)
    budget = int(bpw * numel) + surplus_bits + 1
    bpw = budget / numel

    # Permutations to consider
    base_bpw = max(int(math.floor(bpw)), 1)
    if perms_qkvo and perms_gud:
        perms = [qkvo + gud for qkvo in perms_qkvo for gud in perms_gud]
        perms = [[min(8, p1 + base_bpw) for p1 in p2] for p2 in perms]
    elif perms_qkvo:
        perms = perms_qkvo
        perms = [[min(8, p1 + base_bpw) for p1 in p2] for p2 in perms]
    elif perms_gud:
        perms = perms_gud
        perms = [[min(8, p1 + base_bpw) for p1 in p2] for p2 in perms]
    else:
        assert False, "Logic error"

    # Find largest option within budget
    options = [(sum(a * b for a, b in zip(p, numels)), p) for p in perms]
    options.sort()
    idx = bisect.bisect_right(options, (budget,))
    idx = max(0, idx - 1)
    used_budget, selected = options[idx]

    # Output
    strategy = {k: v for k, v in zip(keys, selected)}
    strategy = {k: strategy[v] for k, v in out_keys.items()}
    surplus = budget - used_budget
    return strategy, surplus


def allocate_linear(
    bpw: float,
    surplus_bits: int,
    l: Linear,
) -> (dict, int):

    numel = l.weights_numel()
    budget = int(bpw * numel) + surplus_bits + 1
    bpw = budget / numel
    bpw = max(int(math.floor(bpw)), 1)
    used_budget = bpw * numel

    strategy = {l.key: bpw}
    surplus = budget - used_budget
    return strategy, surplus

</content>

<content full_path="exllamav3/conversion/calibration_data.py">
import torch
import os
import random

def split_art(articles, rows, columns, tokenizer):
    t_rows = []
    idx = 0
    empty = torch.empty((1, 0), dtype = torch.long)
    t_row = empty
    while len(t_rows) < rows:
        add_special_tokens = (len(t_rows) % 2 == 0)
        t_art = tokenizer.encode(articles[idx], add_bos = add_special_tokens, add_eos = add_special_tokens)
        t_row = torch.cat((t_row, t_art), dim = -1)
        t_row = t_row[:, :columns]
        if t_row.shape[-1] == columns:
            t_rows.append(t_row)
            t_row = empty
        idx += 1
    return t_rows


def split_wiki(text, rows, columns, tokenizer):
    articles = [a[a.find("\n") + 1:] for a in text.split("</doc>\n")]
    articles = [a for a in articles if len(a) > 50]
    return split_art(articles, rows, columns, tokenizer)


def split_tiny(text, rows, columns, tokenizer):
    articles = [a.strip() for a in text.split("<|endoftext|>")]
    return split_art(articles, rows, columns, tokenizer)


def shuffle_lines(text, rows, columns, tokenizer):
    articles = text.split("\n")
    articles = [a for a in articles if not a.isspace()]
    random.seed(0)
    random.shuffle(articles)
    return split_art(articles, rows, columns, tokenizer)


def split_raw(text, rows, columns, tokenizer):
    t_all = tokenizer.encode(text)
    t_rows = []
    for i in range(rows):
        a = i * columns
        b = a + columns
        t_rows.append(t_all[:, a:b])
    return t_rows


def random_data(text, rows, columns, tokenizer):
    vocab_size = tokenizer.actual_vocab_size
    torch.manual_seed(0)
    t_rows = []
    for i in range(rows):
        t_row = torch.randint(0, vocab_size, (1, columns), dtype = torch.long)
        t_rows.append(t_row)
    return t_rows


def get_default_calibration(args, tokenizer):
    columns = args["cal_cols"]
    rows = args["cal_rows"]

    data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "standard_cal_data")
    files = [
        ("c4.utf8", 10, shuffle_lines),
        ("code.utf8", 15, split_raw),
        ("multilingual.utf8", 15, shuffle_lines),
        ("technical.utf8", 10, split_raw),
        ("wiki.utf8", 48, split_wiki),
        ("tiny.utf8", 10, split_tiny),
        (None, 20, random_data),
    ]

    dist_sum = sum(x for (_, x, _) in files)
    cal_data = []

    for filename, weight, processor in files:
        target_rows = max(1, int(weight / dist_sum * rows))
        if filename:
            path = os.path.join(data_dir, filename)
            with open(path, "r", encoding = "utf8") as f:
                file_text = f.read()
        else:
            file_text = None
            target_rows = max(1, rows - len(cal_data))
        r = processor(file_text, target_rows, columns, tokenizer)
        cal_data += r

    # cal_data = torch.cat(cal_data, dim = 0)
    return cal_data
</content>

<content full_path="exllamav3/conversion/convert_model.py">
import argparse
import torch
import time
import sys
from .. import Config, Model, Tokenizer
from ..modules import Linear
from ..modules.quant import LinearFP16, LinearEXL3
from ..util.progress import ProgressBar
from ..util.memory import free_mem
from ..util import Timer, human_time
from ..util.tensor import save_tensor_image
from ..util.measures import cosine_error, sqnr
from .calibration_data import get_default_calibration
from .compile import compile_model, dsize
from safetensors.torch import save_file
from safetensors import safe_open
import os, shutil
import json

col_default = "\u001b[0m"
col_red = "\u001b[31;1m"

torch.set_printoptions(precision = 5, sci_mode = False, linewidth = 200)

parser = argparse.ArgumentParser()
parser.add_argument("-i", "--in_dir", type = str, default = None, help = "Input (model) directory")
parser.add_argument("-w", "--work_dir", type = str, default = None, help = "Working directory")
parser.add_argument("-o", "--out_dir", type = str, default = None, help = "Output directory")
parser.add_argument("-ss", "--shard_size", type = int, help = "Max shard size in MB, default: 8192")
parser.add_argument("-b", "--bits", type = float, help = "Bits per weight")
parser.add_argument("-hb", "--head_bits", type = int, default = None, help = "Bits per weight, output (head) layer, default: 6")
parser.add_argument("-resume", "--resume", action = "store_true", help = "Resume interrupted job from working directory")
parser.add_argument("-cr", "--cal_rows", type = int, help = "Calibration data size, rows, default: 100")
parser.add_argument("-cc", "--cal_cols", type = int, help = "Calibration data size, columns, default: 2048")
parser.add_argument("-cpi", "--checkpoint_interval", type = int, default = 120, help = "Minimum checkpoint interval, in seconds")
parser.add_argument("-lcpi", "--last_checkpoint_index", type = int, default = None, help = "Last module index to checkpoint (for debug purposes)")
parser.add_argument("-v", "--verbose", action = "store_true", help = "Verbose mode")
parser.add_argument("-d", "--devices", type = str, default = "0", help = "List of devices to use for quantization, e.g. --devices 0,1,2")
parser.add_argument("-dr", "--device_ratios", type = str, default = "", help = "Split ratio for devices, e.g. --device_ratio 2,2,4")
parser.add_argument("-img", "--image_dump", action = "store_true", help = "Save model tensors as images (saved to working directory)")
parser.add_argument("-mcg", "--mcg_multiplier", type = str, default = None, help = "MCG multiplier - EXPERIMENTAL, DO NOT USE")
parser.add_argument("-mul1", "--mul1_multiplier", type = str, default = None, help = "MUL1 multiplier - EXPERIMENTAL, DO NOT USE")
parser.add_argument("-strat", "--strategy", type = str, default = None, help = "Modifiers for quantization strategy - EXPERIMENTAL")

group = parser.add_mutually_exclusive_group()
group.add_argument("--out_scales", dest = "out_scales_", action = "store_true", help = "Always enable out channel scales  (for debug purposes)")
group.add_argument("--no_out_scales", dest = "out_scales_", action = "store_false", help = "Never enable out channel scales  (for debug purposes)")
parser.set_defaults(out_scales_ = None)

parser.add_argument("--override_anyway", action = "store_true", help = "Allow resuming even when overriding settings that will break the existing job.")

num_ref_states = 5

def save_dict(filename, dict_, args):
    path = os.path.join(args["work_dir"], filename)
    with open(path, "w", encoding = "utf8") as f:
        f.write(json.dumps(dict_, indent = 4))


def load_dict(filename, args):
    path = os.path.join(args["work_dir"], filename)
    with open(path, "r", encoding = "utf8") as f:
        return json.load(f)


def load_tensor(filename, args):
    path = os.path.join(args["work_dir"], filename)
    with safe_open(path, framework = "pt", device = "cpu") as f:
        if "tensor" in f.keys():
            return f.get_tensor("tensor")
        else:
            tensors = []
            i = 0
            while f"tensor.{i}" in f.keys():
                tensors.append(f.get_tensor(f"tensor.{i}"))
                i += 1
            return tensors


def save_tensor(tensor, filename: str, args):
    path = os.path.join(args["work_dir"], filename)
    if isinstance(tensor, dict):
        save_file({
            k: v for k, v in tensor.items()
        }, path)
    elif isinstance(tensor, list):
        save_file({
            f"tensor.{i}": t for i, t in enumerate(tensor)
        }, path)
    else:
        save_file({
            f"tensor": tensor
        }, path)


def prepare_env(args):
    qtensors_dir = os.path.join(args["work_dir"], "qtensors")
    ckpt_dir = os.path.join(args["work_dir"], "ckpt")
    images_dir = os.path.join(args["work_dir"], "images")
    os.makedirs(args["work_dir"], exist_ok = True)
    os.makedirs(qtensors_dir, exist_ok = True)
    os.makedirs(ckpt_dir, exist_ok = True)
    os.makedirs(images_dir, exist_ok = True)


def prepare(args) -> (dict, dict, bool, str):
    if not args.work_dir:
        return None, None, False, "Must specify --work_dir"
    if not args.in_dir and not args.resume:
        return None, None, False, "Specify either --in_dir to start a new job or --resume to resume an interrupted job"
    if not args.out_dir and not args.resume:
        return None, None, False, "Must specify --out_dir or --resume"
    if args.mcg_multiplier and args.mul1_multiplier:
        return None, None, False, "Cannot specify both MCG and MUL1 arguments"

    in_args = { "work_dir": args.work_dir }
    if args.resume:
        in_args = load_dict("args.json", in_args)
        in_args["work_dir"] = args.work_dir

    prepare_env(in_args)

    def override(arg, can_override, default):
        if (arg not in args or vars(args)[arg] is None) and arg not in in_args:
            if default is not None:
                in_args[arg] = default
            else:
                raise ValueError(f" ## Missing required argument: {arg}")
        if arg in args and vars(args)[arg] is not None:
            if arg in in_args and vars(args)[arg] and in_args[arg] != vars(args)[arg]:
                if can_override:
                    print(
                        f" !! Warning: Overriding {arg} from existing job, was: {in_args[arg]}, "
                        f"new value: {vars(args)[arg]}"
                    )
                else:
                    raise ValueError(
                        f" ## Error: Resuming job with {arg} = {in_args[arg]}, "
                        f"cannot override with new value of {vars(args)[arg]}. "
                        f"Please start a new job to change this value."
                    )
            in_args[arg] = vars(args)[arg]

    for arg_, can_override, default in [
        ("in_dir", True, None),
        ("out_dir", True, None),
        ("shard_size", True, 8192),
        ("bits", False, None),
        ("head_bits", False, 6),
        ("cal_rows", False, 100),
        ("cal_cols", False, 2048),
        ("checkpoint_interval", True, None),
        ("last_checkpoint_index", True, -1),
        ("devices", True, None),
        ("device_ratios", True, None),
        ("mcg_multiplier", True, ""),
        ("mul1_multiplier", True, ""),
        ("strategy", False, ""),
    ]:
        override(arg_, can_override if not args.override_anyway else True, default)

    # Momentary args
    in_args["image_dump"] = args.image_dump
    in_args["verbose"] = args.verbose
    in_args["apply_out_scales"] = args.out_scales_

    if args.resume:
        job_state = load_dict("ckpt/job.json", in_args)
        print(f" -- Resuming existing job")
    else:
        print(f" -- Creating new job")
        job_state = {
            "next_module_idx": 0,
            "surplus_bits": 0,
        }
        save_dict("args.json", in_args, in_args)
        save_dict("ckpt/job.json", job_state, in_args)

    warn_experimental = False
    print(f"    Input directory: {in_args['in_dir']}")
    print(f"    Output directory: {in_args['out_dir']}")
    print(f"    Working directory: {in_args['work_dir']}")
    print(f"    Calibration size: {in_args['cal_rows']} rows, {in_args['cal_cols']} columns")
    print(f"    Target bitrate: {in_args['bits']} (decoder), {in_args['head_bits']} (head)")
    print(f"    Output scales: " + {True: "always", False: "never", None: "auto"}[in_args["apply_out_scales"]])
    if in_args.get("mcg_multiplier"):
        warn_experimental = True
        print(f"    {col_red}MCG multiplier (experimental): {in_args.get('mcg_multiplier')} {col_default}")
    if in_args.get("mul1_multiplier"):
        warn_experimental = True
        print(f"    {col_red}MUL1 multiplier (experimental): {in_args.get('mul1_multiplier')} {col_default}")

    if warn_experimental:
        print(
            f" !! {col_red}WARNING, experimental options are selected. The quantized model may not work in future "
            f"versions of ExLlamaV3 {col_default}"
        )

    return in_args, job_state, True, None


def get_base_model(args):
    config = Config.from_directory(args["in_dir"])
    print(f" -- Loaded model config")
    print(f"    Architecture: {config.architecture}")
    model = Model.from_config(config)
    print(f" -- Created model instance:")
    print(model.get_layout_tree(4))
    tokenizer = Tokenizer.from_config(config)
    print(f" -- Loaded tokenizer")
    print(f"    Vocab size: {tokenizer.actual_vocab_size}")
    return config, model, tokenizer


def prepare_state(args, job_state, config, model, tokenizer):
    idx = job_state["next_module_idx"]
    if idx == 0:
        print(f" -- Preparing input state")
        state = get_default_calibration(args, tokenizer)
    else:
        if idx < len(model.modules):
            print(f" -- Resuming at: {model.modules[idx].key}")
        else:
            print(f" -- Resuming after: {model.modules[idx - 1].key}")
        state = load_tensor("ckpt/state.safetensors", args)
    return state


def get_state_error(x, ref):
     x = x.view(-1, x.shape[-1]).float()
     ref = ref.view(-1, ref.shape[-1]).float()
     err = torch.linalg.norm(x - ref, 'fro') / torch.linalg.norm(ref, 'fro')
     sq = sqnr(x, ref)
     cos = cosine_error(x, ref)
     return err.item(), cos, sq


def mod_strategy(args, module, strategy, idx):
    mod_arg = args.get("strategy")
    if not mod_arg:
        return strategy

    s_layers = [""] + mod_arg.split(";")
    if idx >= len(s_layers):
        return strategy

    s = s_layers[idx]
    mod = {}
    while s:
        l, m = s[0], s[1]
        s = s[2:]
        mod[l] = int(m)

    new_strategy = {}
    for key, bits in strategy.items():
        submodule = module.find_module(key)
        modifier = mod.get(submodule.qbits_mod_key, 0)
        new_strategy[key] = min(bits + modifier, 8)

    # TODO: Automate this, also calculate overall increase in bitrate, track in job.json across resumes
    return new_strategy


@torch.inference_mode()
def main(args, job_state):

    torch.set_printoptions(precision = 5, sci_mode = False, linewidth = 200)

    torch.set_grad_enabled(False)

    devices = [int(d) for d in args["devices"].split(",")]
    device = torch.device(devices[0])
    if args.get("device_ratios"):
        device_ratios = [int(d) for d in args["device_ratios"].split(",")]
        assert len(devices) == len(device_ratios), "--devices and --device_ratios must be same length"
    else:
        device_ratios = None

    last_checkpoint_time = time.time()
    start_time = time.time()
    timed_blocks = 0

    # Get model
    config, model, tokenizer = get_base_model(args)

    # Get initial state or resume state
    state = prepare_state(args, job_state, config, model, tokenizer)

    # Iterate over modules
    for idx, module in enumerate(model.modules):

        start_module_time = time.time()
        if idx == model.first_block_idx:
            start_time = time.time()
            timed_blocks = 0

        # If resuming, skip along to checkpoint index
        if idx < job_state["next_module_idx"]:
            continue

        # Collect output tensors
        q_tensors = {}

        # Quantization strategy
        strategy, surplus = module.allocate_q(
            {
                "bits": args["bits"],
                "head_bits": args["head_bits"],
            },
            job_state["surplus_bits"],
        )
        strategy = mod_strategy(args, module, strategy, idx)
        job_state["surplus_bits"] = surplus

        # Slice module if necessary
        slicing = module.num_slices > 1
        for current_slice in range(module.num_slices):

            # Load current module
            slice_str = f" (slice {current_slice + 1}/{module.num_slices})" if slicing else ""
            print(f" -- Loading unquantized module: {module.key}" + slice_str)
            module.load(
                torch.device("cpu") if module.caps.get("prefer_cpu") else device,
                load_slice = current_slice if slicing else None
            )
            for m in module:
                if m.used_alt_key and not slicing:
                    print(f"     - Cloned {m.key} from {m.alt_key}")
            module.config.stc.close()

            # Skip modules without quant targets
            qmaps = module.get_qmaps()
            if len(qmaps) > 0:

                # Capture calibration input states during forward pass. For block-sparse models, all expert layers
                # are activated to ensure all down projections capture at least some calibration data. When the
                # state is advanced later, only selected experts will be used.
                with ProgressBar(f" -- Capturing: {module.key}" + slice_str, len(state)) as progress:
                    capture_H = {}
                    ref_states = []
                    for i in range(len(state)):
                        progress.update(i)
                        params = {
                            "attn_mode": "flash_attn_nc",
                            "capture": capture_H,
                            "activate_all_experts": model.calibration_all_experts,
                        }
                        if slicing:
                             params["q_mlp_slice"] = current_slice
                        rs = module.prepare_for_device(state[i], params)
                        rs = module.forward(rs, params)
                        if i < num_ref_states:
                            if model.calibration_all_experts:
                                # Reference state for measuring error need, with only selected experts
                                params = { "attn_mode": "flash_attn_nc" }
                                if slicing:
                                    params["q_mlp_slice"] = current_slice
                                rs = module.prepare_for_device(state[i], params)
                                rs = module.forward(rs, params)
                            ref_states.append(rs.cpu())
                        rs = None
                print(f" -- Captured: {module.key}" + slice_str)
                sys.stdout.flush()

                # Swap captured H to system RAM
                for k, v in capture_H.items():
                    infs, nans = v["inf_nan"][0].item(), v["inf_nan"][1].item()
                    if infs or nans:
                        numel = v["num_total"]
                        print(f" !! Warning: {k} state has {infs:,} inf values and {nans:,} NaN values (out of {numel:,})")

                # Swap captured H to system RAM
                for k, v in capture_H.items():
                    v["H_swap_device"] = v["H"].device
                    v["H"] = v["H"].cpu()

            # Get submodules to quantize
            linears = [m for m in module if isinstance(m, Linear) and m.qmap and m.device is not None]
            assert all(linear.key in strategy for linear in linears), \
                f" ## Logic error, no quantization strategy for module"
            assert all(isinstance(linear.inner, LinearFP16) for linear in linears)

            # Write images
            if args["image_dump"]:
                for linear in linears:
                    filename = f"images/{linear.key}.jpg"
                    print(f" -- Saving image: {filename}")
                    w = linear.inner.get_weight_tensor()
                    assert w.dim() == 2
                    save_tensor_image(w, os.path.join(args["work_dir"], filename))

            # Move original tensors to system RAM (load to GPU one by one when quantizing)
            for linear in linears:
                linear.inner.swap_cpu()

            # Quantize module
            for linear in linears:
                quant_args = {
                    "seed": idx,
                    "K": strategy[linear.key],
                    "devices": devices,
                    "device_ratios": device_ratios,
                    "apply_out_scales": args["apply_out_scales"],
                }
                if args.get("mcg_multiplier"):
                    quant_args.update({
                        "mcg_mult": int(args["mcg_multiplier"], 0)
                    })
                if args.get("mul1_multiplier"):
                    quant_args.update({
                        "mul1_mult": int(args["mul1_multiplier"], 0)
                    })

                with Timer() as t:
                    sr = os.path.join(args["work_dir"], f"images/{linear.key}.reg.jpg") \
                        if args["image_dump"] else None
                    proxy_err = linear.convert_exl3(
                        capture_H[linear.qmap],
                        quant_args = quant_args,
                        progress_str = f" -- <step>: {linear.key}",
                        verbose = args["verbose"],
                        save_reg = sr
                    )
                    assert isinstance(linear.inner, LinearEXL3)
                    linear.inner.swap_cpu()
                flags = "o" if quant_args["apply_out_scales"] else "."
                proxy_err_str = f"{proxy_err:8.6f}" if proxy_err >= 0.0 else "(OoM)   "
                print(
                    f" -- Quantized: {linear.key:{config.stc.max_key_len() + 8}}"
                    f"  bpw: {quant_args['K']:5.2f}"
                    f"  proxy_err: {proxy_err_str}"
                    f"  {flags}"
                    f"  g_sc: {quant_args['g_scale']:.6f}"
                    f"  [{t.interval:4.2f} s]"
                )
                sys.stdout.flush()

            # Collect converted module tensors
            for m in module:
                q_tensors.update(m.get_tensors())

            # Unload module
            module.unload()
            config.stc.close()

        # Save layer tensors to working directory
        save_tensor(q_tensors, f"qtensors/{module.key}.safetensors", args)

        # Output final bpw for layer
        num_bytes = dsize(q_tensors)
        num_bits = num_bytes * 8
        final_bpw = num_bits / module.weights_numel() if module.weights_numel() else None

        # Reload module from memory
        config.stc.set_new_tensors(q_tensors)
        module.load(
            torch.device("cpu") if module.caps.get("prefer_cpu") else device,
            source = q_tensors
        )
        config.stc.set_new_tensors(None)
        del q_tensors

        # Advance state
        error = 0
        cos_error = 0
        sqnr_ = 0
        with ProgressBar(f" -- Forward pass: {module.key}", len(state)) as progress:
            for i in range(len(state)):
                progress.update(i)
                params = {
                    "attn_mode": "flash_attn_nc",
                }
                state[i] = module.prepare_for_device(state[i], params)
                if i < num_ref_states or idx < len(model.modules) - 1:
                    state[i] = module.forward(state[i], params).cpu()
                if i < num_ref_states and len(linears):
                    ref_states[i] = ref_states[i].to(state[i].device)
                    rfn, cos, sq = get_state_error(state[i], ref_states[i])
                    error += rfn
                    cos_error += cos
                    sqnr_ += sq
                    ref_states[i] = None
        error /= num_ref_states
        cos_error /= num_ref_states
        sqnr_ /= num_ref_states

        # Feedback after module
        module_time = time.time() - start_module_time
        print(
            f" -- Quantized: {module.key:{config.stc.max_key_len() + 8}}" +
            (f"  bpw: {final_bpw:5.2f}" if final_bpw else f"   no_weights") +
            (f"  rfn: {error:.6f}" if module.num_slices == 1 else "        rfn: N/A     ") +
            f"  cos: {cos_error:.6f}"
            f"  sqnr: {sqnr_:.6f}"
            f"  [{module_time:.2f} s]"
        )
        sys.stdout.flush()
        if idx >= model.first_block_idx:
            overall_time = time.time() - start_time
            timed_blocks += 1
            est_remaining = (overall_time / timed_blocks) * (len(model.modules) - idx)
            print(f" -- Estimated remaining time: {human_time(est_remaining)}")

        # Unload current module
        module.unload()
        # free_mem()

        # Checkpoint
        job_state["next_module_idx"] = idx + 1
        if time.time() > last_checkpoint_time + args["checkpoint_interval"] and \
            (args.get("last_checkpoint_index", -1) < 0 or idx <= args["last_checkpoint_index"]):
            print(f" -- Saving checkpoint")
            ckpt_dir = os.path.join(args["work_dir"], "ckpt")
            ckpt_dir_old = os.path.join(args["work_dir"], "ckpt_old")
            ckpt_dir_new = os.path.join(args["work_dir"], "ckpt_new")
            os.makedirs(ckpt_dir_new, exist_ok = True)
            save_dict("ckpt_new/job.json", job_state, args)
            save_tensor(state, "ckpt_new/state.safetensors", args)
            if os.path.exists(ckpt_dir_old):
                shutil.rmtree(ckpt_dir_old)
            os.rename(ckpt_dir, ckpt_dir_old)
            os.rename(ckpt_dir_new, ckpt_dir)
            last_checkpoint_time = time.time()

    # Compile model
    compile_model(args, model, config, tokenizer)

    # All done
    print(" -- All done")
</content>

<content full_path="exllamav3/modules/rmsnorm.py">
from __future__ import annotations
from typing_extensions import override
import torch
import torch.nn.functional as F
from torch import nn
from ..models import Config
from . import Module
from ..ext import exllamav3_ext as ext

class RMSNorm(Module):

    def __init__(
        self,
        config: Config,
        key: str,
        rms_norm_eps: float,
        out_dtype: torch.dtype | None = None,
        qmap: str | None = None,
        constant_bias: float = 0.0
    ):
        super().__init__(config, key, None)
        assert qmap is None, "No quant scheme for RMSNorm"
        self.module_name = "RMSNorm"

        self.weight = None
        self.rms_norm_eps = rms_norm_eps
        self.out_dtype = out_dtype
        self._numel = None
        self.constant_bias = constant_bias

    @override
    def load(self, device: torch.device, **kwargs):
        self.device = device
        weight = self.config.stc.get_tensor(f"{self.key}.weight", self.device, float2half = True)
        self._numel = weight.numel()
        self.weight = nn.Parameter(weight, requires_grad = False)

    @override
    def unload(self):
        self.device = None
        self.weight = None

    @override
    def get_tensors(self):
        return {
            f"{self.key}.weight": self.weight.data
        }

    def forward_torch(
        self,
        x: torch.Tensor,
        params: dict,
        out_dtype: torch.dtype | None = None,
    ) -> torch.Tensor:
        dtype = x.dtype
        x = x.float()
        var = x.pow(2).mean(dim = -1, keepdim = True) + self.rms_norm_eps
        x = x * torch.rsqrt(var)
        x = x.to(dtype)
        x = x * self.weight if self.constant_bias == 0.0 else x * (self.weight + self.constant_bias)
        x = x.to(out_dtype or self.out_dtype)
        return x

    @override
    def weights_numel(self):
        return self._numel

    @override
    def forward(
        self,
        x: torch.Tensor,
        params,
        out_dtype: torch.dtype | None = None,
    ) -> torch.Tensor:

        y = torch.empty_like(x, dtype = out_dtype or self.out_dtype)
        ext.rms_norm(x, self.weight, y, self.rms_norm_eps, self.constant_bias)
        return y
</content>

<content full_path="exllamav3/modules/layernorm.py">
from __future__ import annotations
from typing_extensions import override
import torch
import torch.nn.functional as F
from torch import nn
from ..models import Config
from . import Module
from ..ext import exllamav3_ext as ext

class LayerNorm(Module):

    def __init__(
        self,
        config: Config,
        key: str,
        layernorm_eps: float,
        out_dtype: torch.dtype | None = None,
        qmap: str | None = None,
    ):
        super().__init__(config, key, None)
        assert qmap is None, "No quant scheme for LayerNorm"
        self.module_name = "LayerNorm"

        self.weight = None
        self.weight_f = None
        self.bias = None
        self.bias_f = None
        self.layernorm_eps = layernorm_eps
        self.out_dtype = out_dtype
        self._numel = None

    @override
    def load(self, device: torch.device, **kwargs):
        self.device = device
        weight = self.config.stc.get_tensor(f"{self.key}.weight", self.device, float2half = True)
        bias = self.config.stc.get_tensor(f"{self.key}.bias", self.device, optional = True, float2half = True)
        self._numel = weight.numel() + (bias.numel() if bias is not None else 0)
        self.weight = weight
        self.weight_f = None
        self.bias = bias
        self.bias_f = None

    @override
    def unload(self):
        self.device = None
        self.weight = None
        self.weight_f = None
        self.bias = None
        self.bias_f = None

    @override
    def get_tensors(self):
        t = {}
        t[f"{self.key}.weight"] = self.weight.contiguous()
        if self.bias is not None:
            t[f"{self.key}.bias"] = self.bias.contiguous()
        return t

    def _weight_f(self):
        if self.weight_f is None:
            self.weight_f = self.weight.to(torch.float)
        return self.weight_f

    def _bias_f(self):
        if self.bias is None:
            return None
        if self.bias_f is None:
            self.bias_f = self.bias.to(torch.float)
        return self.bias_f

    def forward_torch(
        self,
        x: torch.Tensor,
        params: dict,
        out_dtype: torch.dtype | None = None,
    ) -> torch.Tensor:
        w, b = (self._weight_f(), self._bias_f()) if x.dtype == torch.float else (self.weight, self.bias)
        x = F.layer_norm(x, x.shape[-1:], w, b, eps = self.layernorm_eps)
        return x.to(out_dtype or self.out_dtype)

    @override
    def weights_numel(self):
        return self._numel

    @override
    def forward(
        self,
        x: torch.Tensor,
        params: dict,
        out_dtype: torch.dtype | None = None,
    ) -> torch.Tensor:
        w, b = (self._weight_f(), self._bias_f()) if x.dtype == torch.float else (self.weight, self.bias)
        d = w.dim()
        if d == 1:
            x = F.layer_norm(x, x.shape[-1:], w, b, eps = self.layernorm_eps)
        else:
            x = F.layer_norm(x, x.shape[-1:], None, None, eps = self.layernorm_eps)
            x *= w
            if b is not None:
                x += b
        return x.to(out_dtype or self.out_dtype)
</content>

<content full_path="exllamav3/modules/block_sparse_mlp.py">
from __future__ import annotations
from typing_extensions import override
import torch
import torch.nn.functional as F
from torch import nn
from ..models import Config
from ..util.tensor import to2
from . import Module, Linear
from .multilinear import MultiLinear
from ..ext import exllamav3_ext as ext
from ..constants import MAX_MLP_INTERMEDIATE
from ..util import first_not_none
from ..util import profile_opt
from dataclasses import dataclass


@dataclass
class RoutingCFG:
    gate_tensor: torch.Tensor
    num_experts: int
    num_experts_per_tok: int
    router_logits_bsz1: torch.Tensor
    routing_weights_bsz1: torch.Tensor
    selected_experts_bsz1: torch.Tensor

def routing(bsz, cfg, y, params):
    activate_all_experts = params.get("activate_all_experts")

    if bsz == 1 and not activate_all_experts:
        torch.matmul(y, cfg.gate_tensor, out = cfg.router_logits_bsz1)
        torch.topk(
            cfg.router_logits_bsz1,
            cfg.num_experts_per_tok,
            dim = -1,
            out = (cfg.routing_weights_bsz1, cfg.selected_experts_bsz1),
            sorted = False
        )
        torch.softmax(cfg.routing_weights_bsz1, dim = -1, out = cfg.routing_weights_bsz1)
        return cfg.selected_experts_bsz1, cfg.routing_weights_bsz1

    else:
        router_logits = torch.matmul(y, cfg.gate_tensor)
        routing_weights, selected_experts = torch.topk(
            router_logits,
            cfg.num_experts if activate_all_experts else cfg.num_experts_per_tok,
            dim = -1
        )
        routing_weights = torch.softmax(routing_weights, dim = -1)
        return selected_experts, routing_weights


@dataclass
class ExpertsCFG:
    yh: torch.Tensor
    interm_g: torch.Tensor
    interm_u: torch.Tensor
    interm_a: torch.Tensor
    out_d: torch.Tensor


class BlockSparseMLP(Module):

    def __init__(
        self,
        config: Config,
        key: str,
        hidden_size: int,
        intermediate_size: int,
        num_experts: int,
        num_experts_per_tok: int,
        key_up: str | None = None,
        key_gate: str | None = None,
        key_down: str | None = None,
        key_routing_gate: str | None = None,
        qmap: str | None = None,
        out_dtype: torch.dtype = None,
        activation_fn: str = "silu",
        interm_dtype: torch.dtype = None,
    ):
        super().__init__(config, key, None)

        self.out_dtype = out_dtype
        self.interm_dtype = interm_dtype
        self.activation_fn = activation_fn
        self.intermediate_size = intermediate_size
        self.num_experts = num_experts
        self.num_experts_per_tok = num_experts_per_tok
        self.hidden_size = hidden_size

        self.routing_gate = Linear(
            config = config,
            key = f"{key}.{key_routing_gate}",
            in_features = hidden_size,
            out_features = num_experts,
            qmap = None,
            out_dtype = torch.half,
            pad_to = 1,
        )
        self.register_submodule(self.routing_gate)

        self.gates = []
        self.ups = []
        self.downs = []

        for idx in range(num_experts):

            gate = Linear(
                config = config,
                key = f"{key}.{key_gate}".replace("{expert_idx}", str(idx)),
                in_features = hidden_size,
                out_features = intermediate_size,
                qmap = qmap + ".input",
                out_dtype = self.interm_dtype
            )
            up = Linear(
                config = config,
                key = f"{key}.{key_up}".replace("{expert_idx}", str(idx)),
                in_features = hidden_size,
                out_features = intermediate_size,
                qmap = qmap + ".input",
                out_dtype = self.interm_dtype
            )
            down = Linear(
                config = config,
                key = f"{key}.{key_down}".replace("{expert_idx}", str(idx)),
                in_features = intermediate_size,
                out_features = hidden_size,
                qmap = qmap + f".{idx}.down",
                out_dtype = torch.half,
                allow_input_padding = True,
            )

            self.ups.append(up)
            self.gates.append(gate)
            self.downs.append(down)

            self.register_submodule(up)
            self.register_submodule(gate)
            self.register_submodule(down)

        match activation_fn:
            case "silu": self.activation_fn_call = ext.silu_mul
            case "gelu": self.activation_fn_call = ext.gelu_mul

        self.is_quantized = False
        self.multi_gate = None
        self.multi_up = None
        self.multi_down = None

        self.routing_cfg = None
        self.experts_cfg = None

    @override
    def load(self, device: torch.Device, **kwargs):
        super().load(device, **kwargs)

        # Test if experts can be fused
        num_exl3_tensors = 0
        num_nonexl3_tensors = 0
        for l in self.gates + self.ups + self.downs:
            if l.quant_type == "exl3":
                num_exl3_tensors += 1
            else:
                num_nonexl3_tensors += 1
        if num_exl3_tensors and num_nonexl3_tensors:
            print(f" !! Warning, partially quantized block-sparse MLP layer: {self.key}")
        self.is_quantized = (num_exl3_tensors > 0 and num_nonexl3_tensors == 0)

        # Make fused modules
        if self.is_quantized:
            self.multi_gate = MultiLinear(self. device, self.gates)
            self.multi_up = MultiLinear(self. device, self.ups)
            self.multi_down = MultiLinear(self. device, self.downs)

        router_logits_bsz1 = torch.empty((1, self.num_experts), dtype = torch.half, device = self.device)
        routing_weights_bsz1 = torch.empty((1, self.num_experts_per_tok), dtype = torch.half, device = self.device)
        selected_experts_bsz1 = torch.empty((1, self.num_experts_per_tok), dtype = torch.long, device = self.device)

        self.routing_cfg = RoutingCFG(
            gate_tensor = self.routing_gate.inner.weight,
            num_experts = self.num_experts,
            num_experts_per_tok = self.num_experts_per_tok,
            router_logits_bsz1 = router_logits_bsz1,
            routing_weights_bsz1 = routing_weights_bsz1,
            selected_experts_bsz1 = selected_experts_bsz1
        )

        yh = torch.empty(
            (self.num_experts_per_tok, 1, self.hidden_size),
            dtype = torch.half,
            device = self.device
        )
        interm_g = torch.empty(
            (self.num_experts_per_tok, 1, self.intermediate_size),
            dtype = self.interm_dtype,
            device = self.device
        )
        interm_u = torch.empty_like(interm_g)
        interm_a = torch.empty_like(interm_u, dtype = torch.half) if self.interm_dtype != torch.half else interm_u
        out_d = torch.empty(
            (self.num_experts_per_tok, 1, self.hidden_size),
            dtype = self.out_dtype or torch.half,
            device = self.device
        )

        self.experts_cfg = ExpertsCFG(
            yh = yh,
            interm_g = interm_g,
            interm_u = interm_u,
            interm_a = interm_a,
            out_d = out_d
        )

    @override
    def unload(self):
        if self.multi_gate is not None:
            self.multi_gate.unload()
            self.multi_gate = None
        if self.multi_up is not None:
            self.multi_up.unload()
            self.multi_up = None
        if self.multi_down is not None:
            self.multi_down.unload()
            self.multi_down = None
        self.routing_cfg = None
        self.experts_cfg = None
        super().unload()


    @override
    def forward(
        self,
        x: torch.Tensor,
        params: dict,
        out_dtype: torch.dtype | None = None
    ) -> torch.Tensor:

        y = x.view(-1, self.hidden_size)
        bsz = y.shape[0]

        # selected_experts, routing_weights = routing(bsz, self.routing_cfg, y, params)
        selected_experts, routing_weights = ext.blocksparse_mlp_routing(bsz, self.routing_cfg, y, params)

        # Torch path
        if bsz > 1 or not self.is_quantized:
            final_hidden_states = torch.zeros_like(y)

            expert_mask = torch.nn.functional.one_hot(
                selected_experts,
                num_classes = self.num_experts
            )
            expert_count = expert_mask.view(-1, self.num_experts).sum(dim = 0).cpu()
            expert_mask = expert_mask.permute(2, 1, 0)

            def mlp(exp_i, xc):
                g = self.gates[exp_i].forward(xc, params)
                u = self.ups[exp_i].forward(xc, params)
                self.activation_fn_call(g, u, u)
                return self.downs[exp_i].forward(u, params)

            for expert_idx in range(self.num_experts):
                if expert_count[expert_idx] == 0:
                    continue
                idx, top_x = torch.where(expert_mask[expert_idx])
                current_state = y[None, top_x].reshape(-1, self.hidden_size)
                current_state = mlp(expert_idx, current_state) * routing_weights[top_x, idx, None]
                final_hidden_states.index_add_(0, top_x, current_state)

            final_hidden_states = final_hidden_states.reshape(x.shape)
            return to2(final_hidden_states, out_dtype, self.out_dtype)

        # Fused path
        # TODO: Find good solution for 1 < bsz < 32
        else:
            y = y.unsqueeze(0)

            cfg = self.experts_cfg

            # Gate
            ext.exl3_mgemm(
                y,
                self.multi_gate.ptrs_trellis,
                cfg.interm_g,
                self.multi_gate.ptrs_suh,
                cfg.yh,
                self.multi_gate.ptrs_svh,
                selected_experts,
                None,
                self.multi_gate.K,
                -1,
                self.multi_gate.mcg_mult,
                self.multi_gate.mul1_mult,
            )

            # Up
            ext.exl3_mgemm(
                y,
                self.multi_up.ptrs_trellis,
                cfg.interm_u,
                self.multi_up.ptrs_suh,
                cfg.yh,
                self.multi_up.ptrs_svh,
                selected_experts,
                None,
                self.multi_up.K,
                -1,
                self.multi_up.mcg_mult,
                self.multi_up.mul1_mult,
            )

            # Activation
            self.activation_fn_call(cfg.interm_g, cfg.interm_u, cfg.interm_a)

            # Down
            ext.exl3_mgemm(
                cfg.interm_a,
                self.multi_down.ptrs_trellis,
                cfg.out_d,
                self.multi_down.ptrs_suh,
                cfg.interm_a,
                self.multi_down.ptrs_svh,
                selected_experts,
                routing_weights,
                self.multi_down.K,
                -1,
                self.multi_down.mcg_mult,
                self.multi_down.mul1_mult,
            )

            final_hidden_states = cfg.out_d[:1, ...]
            return final_hidden_states.view(x.shape)
</content>

<content full_path="exllamav3/modules/embedding.py">
from __future__ import annotations
from typing_extensions import override
import torch
import torch.nn.functional as F
from torch import nn
from ..models import Config
from ..util.tensor import to2
from . import Module

class Embedding(Module):

    def __init__(
        self,
        config: Config,
        key: str,
        vocab_size: int,
        hidden_size: int,
        out_dtype: torch.dtype | None = torch.float,
        qmap: str | None = None,
        normalize: bool = False
    ):
        super().__init__(config, key, None)
        assert qmap is None, "No quant scheme for Embedding"

        self.key = key
        self.embedding = None
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.out_dtype = out_dtype
        self._numel = vocab_size * hidden_size
        self.normalize = normalize

        self.caps.update({
            "prefer_cpu": True,
        })

    @override
    def load(self, device: torch.device, **kwargs):
        self.device = device
        weight = self.config.stc.get_tensor(self.key + ".weight", self.device, float2half = True)
        self._numel = weight.numel()
        self.embedding = nn.Embedding(
            self.vocab_size,
            self.hidden_size,
            device = "meta"
        )
        self.embedding.weight = nn.Parameter(weight)

    @override
    def unload(self):
        self.device = None
        self.embedding = None

    @override
    def get_tensors(self):
       return {
            f"{self.key}.weight": self.embedding.weight.data.contiguous()
        }

    @override
    def weights_numel(self):
        return self._numel
        
    @override
    def forward(
        self,
        x: torch.Tensor,
        params: dict,
        out_dtype: torch.dtype | None = None
    ) -> torch.Tensor:

        x = self.embedding.forward(x)
        x = to2(x, out_dtype, self.out_dtype)
        if self.normalize:
            x *= x.shape[-1] ** 0.5
        return x
</content>

<content full_path="exllamav3/modules/linear.py">
from __future__ import annotations
from typing_extensions import override
import torch
import torch.nn.functional as F
import numpy as np
from torch import nn
from ..models import Config
from . import Module
from .quant import LinearFP16, LinearFP16_torch, LinearEXL3
from .quant.exl3_lib import quantize_exl3
from ..ext import exllamav3_ext as ext
from ..conversion.allocation import allocate_linear
from ..util.memory import free_mem


class Linear(Module):

    def __init__(
        self,
        config: Config,
        key: str,
        in_features: int,
        out_features: int,
        qmap: str | None = None,
        alt_key: str | None = None,
        qbits_key: str = "bits",
        qbits_mod_key: str = "",
        fkey : str | None = None,
        frange: tuple[int, int] | None = None,
        caps: dict = None,
        softcap: float = 0.0,
        pad_to: int = 128,
        full_in_features: int | None = None,
        full_out_features: int | None = None,
        first_in_feature: int | None = None,
        first_out_feature: int | None = None,
        out_dtype: torch.dtype | None = None,
        allow_input_padding: bool = False,
        post_scale: float = 1.0,
    ):
        super().__init__(config, key, qmap)

        self.alt_key = alt_key
        self.in_features_unpadded = in_features
        self.in_features = (in_features + pad_to - 1) // pad_to * pad_to
        self.out_features_unpadded = out_features
        self.out_features = (out_features + pad_to - 1) // pad_to * pad_to
        self.full_in_features = full_in_features if full_in_features is not None else self.in_features
        self.full_out_features = full_out_features if full_out_features is not None else self.out_features
        self.first_in_feature = first_in_feature if first_in_feature is not None else 0
        self.first_out_feature = first_out_feature if first_out_feature is not None else 0
        self.inner = None
        self.qbits_key = qbits_key
        self.qbits_mod_key = qbits_mod_key
        self.fkey = fkey
        self.frange = frange
        self.quant_type = None
        self.softcap = softcap
        self.is_sliced = self.in_features != self.full_in_features or self.out_features != self.full_out_features
        self.out_dtype = out_dtype
        self.post_scale = post_scale

        assert self.in_features_unpadded == self.in_features or allow_input_padding, \
            f"Input padding is not allowed for {self.key}, in_dim: {self.in_features_unpadded}, pad_to: {pad_to}"

        if caps is not None:
            self.caps.update(caps)


    def pad_out(self, w: torch.Tensor | None) -> torch.Tensor | None:
        if w is None or self.out_features == self.out_features_unpadded and self.in_features == self.in_features_unpadded:
            return w
        if w.dim() == 2:
            padded = torch.zeros((self.in_features, self.out_features), dtype = w.dtype, device = w.device)
            if self.out_features != self.out_features_unpadded:
                padded[:, :w.shape[1]] = w
            elif self.in_features != self.in_features_unpadded:
                padded[:w.shape[0], :] = w
        else:
            assert w.dim() == 1
            padded = torch.zeros((self.out_features,), dtype = w.dtype, device = w.device)
            padded[:w.shape[0]] = w
        return padded.contiguous()


    def apply_fp8_scales_(self, weight: torch.Tensor, scale_inv: torch.Tensor):
        ws = weight.shape
        ss = scale_inv.shape
        assert len(ws) == len(ss) == 2
        assert all(w == s * 128 for w, s in zip(ws, ss))
        weight = weight.view(ss[0], 128, ss[1], 128)
        scale_inv = scale_inv.view(ss[0], 1, ss[1], 1)
        weight *= scale_inv
        return weight


    def load_fp16(self, key: str) -> bool:
        if self.config.stc.has_tensor_group(key, ["weight"]):
            self.used_alt_key = key == self.alt_key
            dev = "cpu" if self.is_sliced else self.device
            pad1 = (self.out_features,) if not self.is_sliced else None
            pad2 = (self.in_features, self.out_features) if not self.is_sliced else None
            weight = self.config.stc.get_tensor(key + ".weight", dev, float2half = True, transpose = True, pad_to = pad2)
            bias = self.config.stc.get_tensor(key + ".bias", dev, float2half = True, optional = True, pad_to = pad1)
            scale_inv = self.config.stc.get_tensor(key + ".weight_scale_inv", dev, float2half = True, optional = True)
            if scale_inv is not None:
                self.apply_fp8_scales_(weight, scale_inv)
            self.inner = LinearFP16(
                self.in_features,
                self.out_features,
                weight,
                bias,
                self.full_in_features,
                self.full_out_features,
                self.first_in_feature,
                self.first_out_feature,
                self.out_dtype
            )
            if self.is_sliced:
                self.inner.swap_device = self.device
                self.inner.unswap_cpu()
            self.quant_type = "fp16"
            return True
        elif self.fkey and self.config.stc.has_tensor_group(self.fkey, ["weight"]):
            weight = self.config.stc.get_tensor(
                self.fkey + ".weight",
                self.device,
                no_defer = True
            )
            weight = weight[self.frange[0] : self.frange[1]].contiguous()
            weight = self.pad_out(weight)
            bias = self.config.stc.get_tensor(key + ".bias", self.device, optional = True, no_defer = True)
            bias = self.pad_out(bias)
            if bias is not None:
                bias = bias[self.frange[0] : self.frange[1]].contiguous()
            self.inner = LinearFP16(
                self.in_features,
                self.out_features,
                weight.T,
                bias,
                out_dtype = self.out_dtype
            )
            self.quant_type = "fp16"
            return True
        return False


    def is_exl3_storage(self, key: str):
        return self.config.stc.has_tensor_group(
            key,
            [["sv", "svh"], ["su", "suh"], "trellis"]
        )

    def load_exl3(self, key: str) -> bool:
        if not self.is_exl3_storage(key):
            return False
        self.used_alt_key = key == self.alt_key
        scale = self.config.stc.get_tensor(key + ".scale", self.device, optional = True)
        su = self.config.stc.get_tensor(key + ".su", self.device, optional = True, no_defer = True)
        suh = self.config.stc.get_tensor(key + ".suh", self.device, optional = True)
        sv = self.config.stc.get_tensor(key + ".sv", self.device, optional = True, no_defer = True)
        svh = self.config.stc.get_tensor(key + ".svh", self.device, optional = True)
        trellis = self.config.stc.get_tensor(key + ".trellis", self.device)
        mcg = self.config.stc.get_tensor(key + ".mcg", "cpu", no_defer = True, optional = True)
        mul1 = self.config.stc.get_tensor(key + ".mul1", "cpu", no_defer = True, optional = True)
        bias = self.config.stc.get_tensor(key + ".bias", self.device, optional = True)
        self.inner = LinearEXL3(
            self.config,
            self.in_features,
            self.out_features,
            scale,
            su,
            sv,
            suh,
            svh,
            trellis,
            mcg,
            mul1,
            bias,
            self.out_dtype
        )
        self.quant_type = "exl3"
        return True


    @override
    def can_defer_load(self):
        if self.is_sliced: return False
        return super().can_defer_load()


    @override
    def load(self, device: torch.device, **kwargs):
        self.device = device
        keys = [self.key]
        if self.alt_key:
            keys += [self.alt_key]
        if any(self.load_exl3(k) for k in keys): return
        if any(self.load_fp16(k) for k in keys): return
        raise ValueError(f"No tensors found for {self.key} matching supported quantization format.")


    @override
    def unload(self):
        self.device = None
        self.inner = None


    @override
    def get_tensors(self):
        if self.device:
            return self.inner.get_tensors(self.key)
        else:
            return {}


    def convert_exl3(
        self,
        H_data: dict,
        quant_args: dict,
        progress_str: str | None = None,
        return_weight_q: bool = False,
        verbose: bool = False,
        save_reg: str = None
    ):
        assert isinstance(self.inner, LinearFP16), \
            "Inner layer is already quant type"

        # Destroy original layer here to save VRAM, we only need weights
        swap_to_device = self.inner.swap_device  # in case weights are swapped to CPU
        orig_weight = self.inner.get_weight_tensor().float()
        orig_bias = self.inner.get_bias_tensor()
        self.inner = None

        weight_q, proxy_err, out_tensors = quantize_exl3(
            orig_weight,
            H_data,
            quant_args,
            return_weight_q,
            progress_str,
            verbose,
            swap_to_device,
            save_reg = save_reg
        )

        self.inner = LinearEXL3(
            self.config,
            self.in_features,
            self.out_features,
            out_tensors.get("scale"),
            out_tensors.get("su"),
            out_tensors.get("sv"),
            out_tensors.get("suh"),
            out_tensors.get("svh"),
            out_tensors.get("trellis"),
            out_tensors.get("mcg"),
            out_tensors.get("mul1"),
            orig_bias,
            self.out_dtype
        )

        if return_weight_q:
            return proxy_err, weight_q
        else:
            return proxy_err


    def capture_H(self, x: torch.Tensor, params: dict):
        if self.qmap not in params["capture"]:
            params["capture"][self.qmap] = {
                "H": torch.zeros(self.in_features, self.in_features, dtype = torch.float32, device = self.device),
                "first_key": self.key,
                "count": 0,
                "finalized": False,
                "num_total": 0,
                "inf_nan": torch.zeros(2, dtype = torch.long, device = self.device),
            }

        params["capture"][self.qmap]["num_total"] += x.numel()
        ext.count_inf_nan(x, params["capture"][self.qmap]["inf_nan"])

        if params["capture"][self.qmap]["first_key"] == self.key:
            rows = np.prod(x.shape[:-1])
            dim = x.shape[-1]
            x = x.view((rows, dim)).to(torch.float, copy = True)

            params["capture"][self.qmap]["H"].addmm_(x.T, x)
            params["capture"][self.qmap]["count"] += rows


    @override
    def weights_numel(self):
        return self.in_features * self.out_features


    @override
    def forward(
        self,
        x: torch.Tensor,
        params: dict,
        out_dtype: torch.dtype | None = None,
    ) -> torch.Tensor:

        if "capture" in params and self.qmap:
            self.capture_H(x, params)

        x = self.inner.forward(x, params, out_dtype)
        if self.softcap != 0.0:
            ext.softcap(x, x, self.softcap)
        if self.post_scale != 1.0:
            x *= self.post_scale
        return x


    def allocate_q(self, quant_args: dict, surplus_bits: int):
        return allocate_linear(
            quant_args[self.qbits_key],
            surplus_bits,
            self
        )


    def quant_format_id(self):
        # alt_key is only used when loading unquantized model
        if self.is_exl3_storage(self.key):
            return "exl3"
        else:
            return None

</content>

<content full_path="exllamav3/modules/__init__.py">
from .module import Module
from .linear import Linear
from .mlp import MLP, GatedMLP
from .block_sparse_mlp import BlockSparseMLP
from .rmsnorm import RMSNorm
from .layernorm import LayerNorm
from .embedding import Embedding
from .attn import Attention
from .transformer import TransformerBlock, ParallelDecoderBlock

</content>

<content full_path="exllamav3/modules/mlp.py">
from __future__ import annotations
from typing_extensions import override
import torch
import torch.nn.functional as F
from torch import nn
from ..models import Config
from ..util.tensor import to2
from . import Module, Linear
from ..ext import exllamav3_ext as ext
from ..constants import MAX_MLP_INTERMEDIATE

class MLP(Module):

    def __init__(
        self,
        config: Config,
        key: str,
        hidden_size: int,
        intermediate_size: int,
        key_up: str | None = None,
        key_down: str | None = None,
        key_fused_gate_up: str | None = None,
        qmap: str | None = None,
        out_dtype: torch.dtype = None
    ):
        super().__init__(config, key, None)
        assert key_fused_gate_up is None

        self.out_dtype = out_dtype

        self.up = Linear(config, f"{key}.{key_up}", hidden_size, intermediate_size, qmap = qmap + ".up", qbits_mod_key = "u")
        self.down = Linear(config, f"{key}.{key_down}", intermediate_size, hidden_size, qmap = qmap + ".down", qbits_mod_key = "d")

        self.register_submodule(self.up)
        self.register_submodule(self.down)

    @override
    def forward(
        self,
        x: torch.Tensor,
        params: dict,
        out_dtype: torch.dtype | None = None
    ) -> torch.Tensor:

        x = self.up.forward(x, params)
        x = F.silu(x)
        x = self.down.forward(x, params)

        return to2(x, out_dtype, self.out_dtype)


class GatedMLP(Module):

    def __init__(
        self,
        config: Config,
        key: str,
        hidden_size: int,
        intermediate_size: int,
        key_up: str | None = None,
        key_gate: str | None = None,
        key_down: str | None = None,
        key_fused_gate_up: str | None = None,
        qmap: str | None = None,
        out_dtype: torch.dtype = None,
        activation_fn: str = "silu",
        intermediate_split_size: int | None = MAX_MLP_INTERMEDIATE,
        interm_dtype: torch.dtype = None,
    ):
        super().__init__(config, key, None)

        self.out_dtype = out_dtype
        self.interm_dtype = interm_dtype
        self.activation_fn = activation_fn
        self.intermediate_size = intermediate_size

        if key_fused_gate_up:
            assert not intermediate_split_size or intermediate_size <= intermediate_split_size, \
                "Cannot combine fused gate/up layers with MLP slicing"
            fkey = f"{key}.{key_fused_gate_up}"
            frange_gate = (0, intermediate_size)
            frange_up = (intermediate_size, 2 * intermediate_size)
        else:
            fkey, frange_gate, frange_up = None, None, None

        if intermediate_split_size and intermediate_size > intermediate_split_size:
            num_slices = (intermediate_size + intermediate_split_size - 1) // intermediate_split_size
            interm_slice = intermediate_size // num_slices // 128 * 128
            interm_split = [interm_slice for _ in range(num_slices)]
            interm_split[-1] += intermediate_size - sum(interm_split)
            self.num_slices = num_slices
        else:
            interm_split = [intermediate_size]
            self.num_slices = 1

        self.gates = []
        self.ups = []
        self.downs = []

        a = 0
        for idx, sp in enumerate(interm_split):
            b = a + sp

            if self.num_slices > 1:
                s_key_g = f"{key}.{key_gate}.slice.{idx}"
                s_key_u = f"{key}.{key_up}.slice.{idx}"
                s_key_d = f"{key}.{key_down}.slice.{idx}"
                a_key_g = f"{key}.{key_gate}"
                a_key_u = f"{key}.{key_up}"
                a_key_d = f"{key}.{key_down}"
            else:
                s_key_g = f"{key}.{key_gate}"
                s_key_u = f"{key}.{key_up}"
                s_key_d = f"{key}.{key_down}"
                a_key_g = None
                a_key_u = None
                a_key_d = None

            gate = Linear(
                config = config,
                key = s_key_g,
                in_features = hidden_size,
                out_features = b - a,
                full_in_features = hidden_size,
                full_out_features = intermediate_size,
                first_in_feature = 0,
                first_out_feature = a,
                qmap = qmap + ".input",
                fkey = fkey,
                frange = frange_gate,
                alt_key = a_key_g,
                out_dtype = self.interm_dtype,
                qbits_mod_key = "g"
            )
            up = Linear(
                config = config,
                key = s_key_u,
                in_features = hidden_size,
                out_features = b - a,
                full_in_features = hidden_size,
                full_out_features = intermediate_size,
                first_in_feature = 0,
                first_out_feature = a,
                qmap = qmap + ".input",
                fkey = fkey,
                frange = frange_up,
                alt_key = a_key_u,
                out_dtype = self.interm_dtype,
                qbits_mod_key = "u"
            )
            down = Linear(
                config = config,
                key = s_key_d,
                in_features = b - a,
                out_features = hidden_size,
                full_in_features = intermediate_size,
                full_out_features = hidden_size,
                first_in_feature = a,
                first_out_feature = 0,
                qmap = qmap + ".down",
                alt_key = a_key_d,
                out_dtype = self.out_dtype,
                allow_input_padding = True,
                qbits_mod_key = "d"
            )

            self.ups.append(up)
            self.gates.append(gate)
            self.downs.append(down)

            self.register_submodule(up)
            self.register_submodule(gate)
            self.register_submodule(down)

            a = b

        match activation_fn:
            case "silu": self.activation_fn_call = ext.silu_mul
            case "gelu": self.activation_fn_call = ext.gelu_mul


    @override
    def can_defer_load(self):
        if self.num_slices > 1: return False
        return super().can_defer_load()


    @override
    def load(self, device: torch.Device, load_slice: int | None = None, **kwargs):
        if load_slice is None:
            super().load(device, **kwargs)
        else:
            self.gates[load_slice].load(device, **kwargs)
            self.ups[load_slice].load(device, **kwargs)
            self.downs[load_slice].load(device, **kwargs)


    @override
    def forward(
        self,
        x: torch.Tensor,
        params: dict,
        out_dtype: torch.dtype | None = None
    ) -> torch.Tensor:

        qs = params.get("q_mlp_slice")
        r = [qs] if qs is not None else range(0, self.num_slices)
        d = None

        for s in r:
            g = self.gates[s].forward(x, params)
            u = self.ups[s].forward(x, params)
            self.activation_fn_call(g, u, u)
            d_ = self.downs[s].forward(u, params)
            if d is None: d = d_
            else: d += d_
            del d_

        return to2(d, out_dtype, self.out_dtype)

</content>

<content full_path="exllamav3/modules/transformer.py">
from __future__ import annotations
from typing_extensions import override
import torch
import torch.nn.functional as F
from torch import nn
from ..util.tensor import to2
from ..models import Config
from . import Module, RMSNorm, LayerNorm, Attention, GatedMLP, MLP, BlockSparseMLP
from ..conversion.allocation import allocate_transformer
from ..util import profile_opt

class TransformerBlock(Module):

    def __init__(
        self,
        config: Config,
        key: str,
        attn_norm: RMSNorm | None = None,
        attn: Attention | None = None,
        attn_post_norm: RMSNorm | None = None,
        mlp_norm: RMSNorm | LayerNorm | None = None,
        mlp: MLP | GatedMLP | BlockSparseMLP | None = None,
        mlp_post_norm: RMSNorm | None = None,
        qmap: str | None = None,
        qbits_key: str = "bits",
        out_dtype: torch.dtype = None
    ):
        super().__init__(config, key, None)

        self.attn_norm = attn_norm
        self.attn = attn
        self.attn_post_norm = attn_post_norm
        self.mlp_norm = mlp_norm
        self.mlp = mlp
        self.mlp_post_norm = mlp_post_norm
        self.qbits_key = qbits_key
        self.out_dtype = out_dtype

        self.register_submodule(self.attn_norm)
        self.register_submodule(self.attn)
        self.register_submodule(self.attn_post_norm)
        self.register_submodule(self.mlp_norm)
        self.register_submodule(self.mlp)
        self.register_submodule(self.mlp_post_norm)

        self.num_slices = mlp.num_slices if mlp else 1


    @override
    def forward(
        self,
        x: torch.Tensor,
        params: dict,
        out_dtype: torch.dtype | None = None
    ) -> torch.Tensor:

        if self.attn:
            if self.attn_norm:
                y = self.attn_norm.forward(x, params, out_dtype = torch.half)
            y = self.attn.forward(y, params)
            if params.get("prefill"): return x
            if self.attn_post_norm:
                y = self.attn_post_norm.forward(y, params)
            x += y

        if self.mlp:
            if self.mlp_norm:
                y = self.mlp_norm.forward(x, params, out_dtype = torch.half)
            y = self.mlp.forward(y, params)
            if self.mlp_post_norm:
                y = self.mlp_post_norm.forward(y, params)
            x += y

        return to2(x, out_dtype, self.out_dtype)


    def allocate_q(self, quant_args: dict, surplus_bits: int):
        if not self.attn and not self.mlp:
            return {}, surplus_bits
        return allocate_transformer(
            quant_args[self.qbits_key],
            surplus_bits,
            self.attn.q_proj if self.attn else None,
            self.attn.k_proj if self.attn else None,
            self.attn.v_proj if self.attn else None,
            self.attn.o_proj if self.attn else None,
            self.mlp.gates if any(isinstance(self.mlp, x) for x in [GatedMLP, BlockSparseMLP]) else None,
            self.mlp.ups if self.mlp else None,
            self.mlp.downs if self.mlp else None,
        )


    def get_name(self):
        name = super().get_name()
        if not self.attn and not self.mlp:
            name += " (no-op)"
        return name


class ParallelDecoderBlock(Module):

    def __init__(
        self,
        config: Config,
        key: str,
        input_norm: RMSNorm | LayerNorm | None = None,
        attn: Attention | None = None,
        mlp: MLP | GatedMLP | None = None,
        qmap: str | None = None,
        qbits_key: str = "bits",
        out_dtype: torch.dtype = None
    ):
        super().__init__(config, key, None)

        self.input_norm = input_norm
        self.attn = attn
        self.mlp = mlp
        self.qbits_key = qbits_key
        self.out_dtype = out_dtype

        self.register_submodule(self.input_norm)
        self.register_submodule(self.attn)
        self.register_submodule(self.mlp)

        self.num_slices = mlp.num_slices if mlp else 1


    @override
    def forward(
        self,
        x: torch.Tensor,
        params: dict,
        out_dtype: torch.dtype | None = None
    ) -> torch.Tensor:

        y = self.input_norm.forward(x, params, out_dtype = torch.half)
        y1 = self.attn.forward(y, params)
        x += y1
        if params.get("prefill"): return x
        y2 = self.mlp.forward(y, params)
        x += y2

        return to2(x, out_dtype, self.out_dtype)


    def allocate_q(self, quant_args: dict, surplus_bits: int):
        if not self.attn and not self.mlp:
            return {}, surplus_bits
        return allocate_transformer(
            quant_args[self.qbits_key],
            surplus_bits,
            self.attn.q_proj if self.attn else None,
            self.attn.k_proj if self.attn else None,
            self.attn.v_proj if self.attn else None,
            self.attn.o_proj if self.attn else None,
            self.mlp.gates if any(isinstance(self.mlp, x) for x in [GatedMLP, BlockSparseMLP]) else None,
            self.mlp.ups if self.mlp else None,
            self.mlp.downs if self.mlp else None,
        )


    def get_name(self):
        name = super().get_name()
        if not self.attn and not self.mlp:
            name += " (no-op)"
        return name
</content>

<content full_path="exllamav3/modules/module.py">
from __future__ import annotations
from abc import ABC, abstractmethod
import torch
import torch.nn.functional as F
from torch import nn
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from ..models import Config

class Module(ABC):

    def __init__(
        self,
        config: Config,
        key: str,
        qmap: str | None,
    ):
        """
        :param config:
            Model config

        :param key:
            Tensor key, reflects name in .safetensors collection

        :param qmap:
            Label for the hidden state upon entry into the forward function. Used to collect states/Hessian data
            in linear layers during quantization, e.g. to allow sharing between Q/K/V projections that have the same
            input state.
        """
        self.config = config
        self.key = key
        self.alt_key = None
        self.used_alt_key = False
        self.device = None
        self.modules = []
        self.caps = {}
        self.qmap = qmap
        self.num_slices = 1
        self.qbits_mod_key = ""

    def __iter__(self):
        yield self
        for module in self.modules:
            yield from module

    def find_module(self, key: str):
        for module in self:
            if module.key == key:
                return module

    def can_defer_load(self):
        if len(self.modules) == 0: return True
        return all(module.can_defer_load() for module in self.modules)

    def load(self, device: torch.Device, **kwargs):
        self.device = device
        for module in self.modules:
            module.load(device, **kwargs)

    def unload(self):
        self.device = None
        for module in self.modules:
            module.unload()

    def prepare_for_device(self, x: torch.Tensor, params: dict) -> torch.Tensor:
        if x.device != self.device:
            x = x.to(self.device)
        return x

    def get_qmaps(self):
        sq = set()
        if self.qmap:
            sq.add(self.qmap)
        for m in self.modules:
            sq.update(m.get_qmaps())
        return sq

    def get_tensors(self):
        return {}

    def weights_numel(self):
        return sum(m.weights_numel() for m in self.modules)

    @abstractmethod
    def forward(
        self,
        x: torch.Tensor,
        params: dict,
        out_dtype: torch.dtype
    ) -> torch.Tensor:
        pass

    def allocate_q(self, quant_args: dict, surplus_bits: int):
        return {}, surplus_bits

    def register_submodule(self, module: Module | None):
        if module is not None:
            self.modules.append(module)

    def quant_format_id(self):
        return None

    def get_name(self):
        return self.__class__.__name__
</content>

<content full_path="exllamav3/modules/multilinear.py">
from __future__ import annotations
import torch
from . import Linear

class MultiLinear:
    def __init__(
        self,
        device: torch.Device,
        linears: list[Linear]
    ):
        self.device = device
        self.linears = linears
        self.num_linears = len(linears)

        assert all(l.quant_type == "exl3" for l in linears)
        assert all(l.inner.bias is None for l in linears)
        assert all(not l.softcap for l in linears)
        assert all(l.post_scale == 1.0 for l in linears)

        self.in_features = linears[0].in_features
        self.out_features = linears[0].out_features
        self.K = linears[0].inner.K
        assert all(l.inner.K == self.K for l in linears)
        assert all(l.in_features == self.in_features for l in linears)
        assert all(l.out_features == self.out_features for l in linears)

        self.ptrs_suh = torch.tensor([l.inner.suh.data_ptr() for l in linears], dtype = torch.long, device = device)
        self.ptrs_svh = torch.tensor([l.inner.svh.data_ptr() for l in linears], dtype = torch.long, device = device)
        self.ptrs_trellis = torch.tensor([l.inner.trellis.data_ptr() for l in linears], dtype = torch.long, device = device)

        self.mcg_mult = linears[0].inner.mcg_mult
        assert all(l.inner.mcg_mult == self.mcg_mult for l in linears[1:])
        self.mul1_mult = linears[0].inner.mul1_mult
        assert all(l.inner.mul1_mult == self.mul1_mult for l in linears[1:])

    def unload(self):
        pass

</content>

<content full_path="exllamav3/modules/attn.py">
from __future__ import annotations
from typing_extensions import override
import torch
import torch.nn.functional as F
from torch import nn
from ..models import Config
from ..util.rope import RopeSettings, RoPE
from ..util.tensor import get_for_device, to2
from . import Module, Linear, RMSNorm, LayerNorm
from ..device import get_device_context, release_device_context
from ..constants import PAGE_SIZE
from ..cache import Cache
from flash_attn import flash_attn_func, flash_attn_with_kvcache
from ..util import profile_opt
from .multilinear import MultiLinear
from ..ext import exllamav3_ext as ext

"""
SDPA:
    
    attn_mode: "sdpa_nc"
    position (optional, default = 0): int *OR*
    positions: shape (bsz) *OR*
    position_ids: shape (bsz, seq_len)    
    - no cache
    - no chunking
    - batch shape is determined by shape of input_ids
    - no logit softcap support (Gemma)
                    
Flash Attention:
                
    attn_mode: "flash_attn"
    batch_shape: tuple of (bsz, max_seq_len)
    cache: Cache with capacity of at least bsz*max_seq_len tokens
    past_len: int, *OR*
    cache_seqlens: shape (bsz) 
    position: int (overrides past_len for position emb)
    positions: shape (bsz) (overrides cache_seqlens for position emb) *OR*
    position_ids: shape (bsz, seq_len) (overrides cache_seqlens for position emb)
    - max_seq_len must be divisible by 256
    
    attn_mode: "flash_attn"
    block_table: list of page indices, shape (bsz, pages_per_seq)
    cache: Paged cache
    cache_seqlens: shape (bsz)
    positions: shape (bsz) (overrides cache_seqlens for position emb) *OR*
    position_ids: shape (bsz, seq_len) (overrides cache_seqlens for position emb)

    attn_mode: "flash_attn_nc"
    position (optional, default = 0): int *OR*
    positions: shape (bsz) *OR*
    position_ids: shape (bsz, seq_len)    
    - no cache
    - no chunking
    - batch shape is determined by shape of input_ids
"""

def prepare_sdpa_nc(input_ids: torch.Tensor, params: dict) -> torch.Tensor:
    assert "cache" not in params, \
        f"Cache provided for attn_mode: sdpa_nc"
    return input_ids


def prepare_flash_attn_nc(input_ids: torch.Tensor, params: dict) -> torch.Tensor:
    assert "cache" not in params, \
        f"Cache provided for attn_mode: sdpa_nc"
    return input_ids


def prepare_flash_attn(input_ids: torch.Tensor, params: dict) -> torch.Tensor:
    bsz, seq_len = input_ids.shape

    if "batch_shape" in params:
        cache = params["cache"]
        cache_bsz, cache_max_seq_len = params["batch_shape"]
        past_len = params.get("past_len")
        cache_seqlens = params.get("cache_seqlens")
        position = params.get("position")
        positions = params.get("positions")
        position_ids = params.get("position_ids")
        assert cache_bsz >= bsz, "batch size too large for cache"
        assert cache_max_seq_len % PAGE_SIZE == 0, f"cache seq len must be a multiple of {PAGE_SIZE}"
        assert (past_len is not None) ^ (cache_seqlens is not None), "Need either past_len or cache_seqlens"
        assert bsz * cache_max_seq_len <= cache.max_num_tokens, "Cache too small for batch shape"
        cache_bsz = min(bsz, cache_bsz)
        num_pages = cache_bsz * cache_max_seq_len // PAGE_SIZE
        block_table = torch.arange(num_pages, dtype = torch.int32).view(cache_bsz, cache_max_seq_len // PAGE_SIZE)
        if past_len is not None:
            cache_seqlens = torch.tensor([past_len], dtype = torch.int32).repeat(bsz)
            if position is None: position = past_len
        else:
            if positions is None and position_ids is None: positions = cache_seqlens
        if position is None: position = 0
        params["block_table"] = block_table
        params["cache_seqlens"] = cache_seqlens
        params["position"] = position
        params["positions"] = positions
        params["position_ids"] = position_ids

    elif "block_table" in params:
        positions = params.get("positions")
        position_ids = params.get("position_ids")
        cache_seqlens = params.get("cache_seqlens")
        if positions is None and position_ids is None: positions = cache_seqlens
        params["cache_seqlens"] = cache_seqlens
        params["positions"] = positions
        params["position_ids"] = position_ids

    return input_ids


def prepare_for_attn(input_ids: torch.Tensor, params: dict) -> torch.Tensor:
    """
    Add attn parameters to state
    """
    attn_mode = params.get("attn_mode", "flash_attn_nc")
    match attn_mode:
        case "sdpa_nc":
            return prepare_sdpa_nc(input_ids, params)
        case "flash_attn":
            return prepare_flash_attn(input_ids, params)
        case "flash_attn_nc":
            return prepare_flash_attn_nc(input_ids, params)
        case _:
            raise ValueError(f"Unknown attn_mode: {attn_mode}")


class Attention(Module):

    def __init__(
        self,
        config: Config,
        key: str,
        layer_idx: int,
        hidden_size: int,
        head_dim: int,
        num_q_heads: int,
        num_kv_heads: int,
        rope_settings: RopeSettings | None,
        sm_scale: float | None = None,
        key_q: str | None = None,
        key_k: str | None = None,
        key_v: str | None = None,
        key_o: str | None = None,
        key_fused_qkv: str | None = None,
        qmap: str | None = None,
        out_dtype: torch.dtype | None = None,
        sliding_window: int  = -1,
        logit_softcapping: float = 0.0,
        q_norm: RMSNorm | LayerNorm | None = None,
        k_norm: RMSNorm | LayerNorm | None = None,
    ):
        super().__init__(config, key, None)

        self.device_context = None
        self.layer_idx = layer_idx
        self.hidden_size = hidden_size
        self.head_dim = head_dim
        self.num_q_heads = num_q_heads
        self.num_kv_heads = num_kv_heads
        self.gqa = (num_q_heads != num_kv_heads)
        self.sm_scale = sm_scale
        self.rope_settings = rope_settings
        self.rope = None
        self.out_dtype = out_dtype
        self.sliding_window = sliding_window
        self.logit_softcapping = logit_softcapping

        if key_fused_qkv:
            fkey = f"{key}.{key_fused_qkv}"
            frange_q = (0, num_q_heads * head_dim)
            frange_k = (frange_q[1], frange_q[1] + num_kv_heads * head_dim)
            frange_v = (frange_k[1], frange_k[1] + num_kv_heads * head_dim)
        else:
            fkey, frange_q, frange_k, frange_v = None, None, None, None

        self.q_proj = Linear(config, f"{key}.{key_q}", hidden_size, num_q_heads * head_dim, qmap = qmap + ".input", fkey = fkey, frange = frange_q, qbits_mod_key = "q")
        self.k_proj = Linear(config, f"{key}.{key_k}", hidden_size, num_kv_heads * head_dim, qmap =  qmap + ".input", fkey = fkey, frange = frange_k, qbits_mod_key = "k")
        self.v_proj = Linear(config, f"{key}.{key_v}", hidden_size, num_kv_heads * head_dim, qmap =  qmap + ".input", fkey = fkey, frange = frange_v, qbits_mod_key = "v")
        self.o_proj = Linear(config, f"{key}.{key_o}", num_q_heads * head_dim, hidden_size, qmap =  qmap + ".o", out_dtype = out_dtype, qbits_mod_key = "o")

        self.register_submodule(self.q_proj)
        self.register_submodule(self.k_proj)
        self.register_submodule(self.v_proj)
        self.register_submodule(self.o_proj)

        if q_norm:
            assert k_norm, "Must have both Q and K norms, or neither"
            self.q_norm = q_norm
            self.k_norm = k_norm
            self.register_submodule(self.q_norm)
            self.register_submodule(self.k_norm)
            if isinstance(q_norm, RMSNorm):
                self.norm_eps = q_norm.rms_norm_eps
                self.norm_constant_bias = q_norm.constant_bias
                assert self.norm_eps == k_norm.rms_norm_eps
            else:
                self.norm_eps = q_norm.layernorm_eps
                self.norm_constant_bias = 0.0
        else:
            self.q_norm = None
            self.k_norm = None
            self.norm_eps = 1e-6
            self.norm_constant_bias = 0.0

        self.caps.update({
            "kv_cache": True
        })

        self.cache_layers = []
        self.multi_kv = None

        self.q_norm_tensor = None
        self.k_norm_tensor = None


    @override
    def load(self, device: torch.Device, **kwargs):
        self.device_context = get_device_context(self.config, device)
        super().load(device)

        for cl in self.cache_layers:
            cl.alloc(device)

        if self.rope_settings:
            self.rope = RoPE(
                device,
                self.rope_settings,
            )

        # Test if K and V proj can be fused
        if (
            self.k_proj.quant_type == "exl3" and
            self.v_proj.quant_type == "exl3" and
            self.k_proj.out_features == self.v_proj.out_features and
            self.k_proj.inner.K == self.v_proj.inner.K and
            self.k_proj.inner.bias is None and
            self.v_proj.inner.bias is None
        ):
            self.multi_kv = MultiLinear(self. device, [self.k_proj, self.v_proj])

        # Head norm
        if self.q_norm and isinstance(self.q_norm, RMSNorm):
            self.q_norm_tensor = self.q_norm.weight.data
            self.k_norm_tensor = self.k_norm.weight.data


    @override
    def unload(self):
        self.device_context = release_device_context(self.config, self.device)
        super().unload()

        for cl in self.cache_layers:
            cl.free()

        self.rope = None

        if self.multi_kv is not None:
            self.multi_kv.unload()
            self.multi_kv = None

        self.q_norm_tensor = None
        self.k_norm_tensor = None


    @override
    def forward(
        self,
        x: torch.Tensor,
        params: dict,
        out_dtype: torch.dtype | None = None
    ) -> torch.Tensor:

        bsz, seqlen, _ = x.shape
        attn_mode = params.get("attn_mode", "flash_attn_nc")
        match attn_mode:
            case "sdpa_nc":
                x = self.decode_sdpa_nc(x, bsz, seqlen, params)
            case "flash_attn":
                x = self.decode_flash_attn(x, bsz, seqlen, params)
            case "flash_attn_nc":
                x = self.decode_flash_attn_nc(x, bsz, seqlen, params)
            case _:
                raise ValueError(f"Unknown attn_mode: {attn_mode}")

        return to2(x, out_dtype, self.out_dtype)


    def project_qkv(self, x: torch.Tensor, params: dict) -> tuple:
        bsz, q_len, dim = x.shape
        q = self.q_proj.forward(x, params)

        if self.multi_kv is None or bsz * q_len > 32:
            k = self.k_proj.forward(x, params)
            v = self.v_proj.forward(x, params)

        else:
            x = x.view(1, bsz * q_len, dim)
            kvh = torch.empty((2, bsz * q_len, dim), dtype = torch.half, device = x.device)
            kv = torch.empty((2, bsz * q_len, self.num_kv_heads * self.head_dim), dtype = torch.half, device = x.device)
            ext.exl3_mgemm(
                x,
                self.multi_kv.ptrs_trellis,
                kv,
                self.multi_kv.ptrs_suh,
                kvh,
                self.multi_kv.ptrs_svh,
                None,
                None,
                self.multi_kv.K,
                -1,
                self.multi_kv.mcg_mult,
                self.multi_kv.mul1_mult,
            )
            k = kv[0].view(bsz, q_len, self.num_kv_heads * self.head_dim)
            v = kv[1].view(bsz, q_len, self.num_kv_heads * self.head_dim)

        return q, k, v


    def project_o(self, o: torch.Tensor, bsz: int, seqlen: int, params: dict) -> torch.Tensor:
        o = o.reshape(bsz, seqlen, self.num_q_heads * self.head_dim)
        x = self.o_proj.forward(o, params)
        return x


    def decode_sdpa_nc(
        self,
        x: torch.Tensor,
        bsz: int,
        seqlen: int,
        params: dict,
    ):
        causal = params.get("causal", True)
        position = params.get("position", 0)
        positions = get_for_device(params, "positions", self.device, None)
        position_ids = get_for_device(params, "position_ids", self.device, None)

        q, k, v = self.project_qkv(x, params)
        q = q.view(bsz, seqlen, self.num_q_heads, self.head_dim)
        k = k.view(bsz, seqlen, self.num_kv_heads, self.head_dim)
        v = v.view(bsz, seqlen, self.num_kv_heads, self.head_dim)

        assert self.sliding_window < 0, \
            "Torch SDPA does not support sliding window attention (SWA)"
        assert self.logit_softcapping == 0.0, \
            "Torch SDPA does not support logit softcapping"

        if self.q_norm and (not self.rope or self.q_norm_tensor is None):
            q = self.q_norm.forward(q, params, out_dtype = torch.half)
            k = self.k_norm.forward(k, params, out_dtype = torch.half)

        if self.rope:
            q, k = self.rope.apply(
                q, k,
                position,
                positions,
                position_ids,
                True,
                self.q_norm_tensor,
                self.k_norm_tensor,
                self.norm_eps,
                self.norm_constant_bias
            )

        q = q.transpose(1, 2)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)
        o = F.scaled_dot_product_attention(q, k, v, is_causal = causal, enable_gqa = self.gqa)
        o = o.transpose(1, 2)

        o = self.project_o(o, bsz, seqlen, params)
        return o


    def decode_flash_attn_nc(
        self,
        x: torch.Tensor,
        bsz: int,
        seqlen: int,
        params: dict,
    ):
        causal = params.get("causal", True)
        position = params.get("position", 0)
        positions = get_for_device(params, "positions", self.device, None)
        position_ids = get_for_device(params, "position_ids", self.device, None)

        q, k, v = self.project_qkv(x, params)
        q = q.view(bsz, seqlen, self.num_q_heads, self.head_dim)
        k = k.view(bsz, seqlen, self.num_kv_heads, self.head_dim)
        v = v.view(bsz, seqlen, self.num_kv_heads, self.head_dim)

        if self.q_norm and (not self.rope or self.q_norm_tensor is None):
            q = self.q_norm.forward(q, params, out_dtype = torch.half)
            k = self.k_norm.forward(k, params, out_dtype = torch.half)

        if self.rope:
            q, k = self.rope.apply(
                q, k,
                position,
                positions,
                position_ids,
                True,
                self.q_norm_tensor,
                self.k_norm_tensor,
                self.norm_eps,
                self.norm_constant_bias
            )

        o = flash_attn_func(
            q = q,
            k = k,
            v = v,
            causal = causal,
            softmax_scale = self.sm_scale,
            window_size = (self.sliding_window, self.sliding_window),
            softcap = self.logit_softcapping
        )
        o = o.view((bsz, seqlen, self.num_q_heads * self.head_dim))

        o = self.project_o(o, bsz, seqlen, params)
        return o



    def decode_flash_attn(
        self,
        x: torch.Tensor,
        bsz: int,
        seqlen: int,
        params: dict,
    ):
        cache = params.get("cache")
        block_table = get_for_device(params, "block_table", self.device)
        cache_seqlens = get_for_device(params, "cache_seqlens", self.device)
        position = params.get("position", 0)
        positions = get_for_device(params, "positions", self.device, None)
        position_ids = get_for_device(params, "position_ids", self.device, None)
        causal = params.get("causal", True)

        q, k, v = self.project_qkv(x, params)
        q = q.view(bsz, seqlen, self.num_q_heads, self.head_dim)
        k = k.view(bsz, seqlen, self.num_kv_heads, self.head_dim)
        v = v.view(bsz, seqlen, self.num_kv_heads, self.head_dim)

        # TODO: Add LayerNorm option to fused norm/RoPE kernel
        if self.q_norm and (not self.rope or self.q_norm_tensor is None):
            q = self.q_norm.forward(q, params, out_dtype = torch.half)
            k = self.k_norm.forward(k, params, out_dtype = torch.half)

        if self.rope:
            q, k = self.rope.apply(
                q, k,
                position,
                positions,
                position_ids,
                True,
                self.q_norm_tensor,
                self.k_norm_tensor,
                self.norm_eps,
                self.norm_constant_bias
            )

        cache_k, cache_v = cache.get_layer(self.layer_idx, cache_seqlens, block_table)
        o = flash_attn_with_kvcache(
            q = q,
            k = k,
            v = v,
            k_cache = cache_k,
            v_cache = cache_v,
            block_table = block_table,
            cache_seqlens = cache_seqlens,
            causal = causal,
            softmax_scale = self.sm_scale,
            window_size = (self.sliding_window, self.sliding_window),
            softcap = self.logit_softcapping
        )
        cache.update_layer(self.layer_idx, cache_seqlens, block_table, cache_k, cache_v, seqlen)
        o = o.view((bsz, seqlen, self.num_q_heads * self.head_dim))

        # TODO: Store updated cache layer

        o = self.project_o(o, bsz, seqlen, params)
        return o

</content>

<content full_path="exllamav3/modules/quant/fp16.py">
from __future__ import annotations
from typing_extensions import override
import torch
import torch.nn.functional as F
from torch import nn
from ...ext import exllamav3_ext as ext
from ...util.tensor import to2
from ...util import first_not_none

class LinearFP16:

    in_features: int
    out_features: int

    def __init__(
        self,
        in_features: int,
        out_features: int,
        weight: torch.Tensor,
        bias: torch.Tensor | None,
        full_in_features: int | None = None,
        full_out_features: int | None = None,
        first_in_feature: int | None = None,
        first_out_feature: int | None = None,
        out_dtype: torch.dtype | None = None
    ):
        self.weight = weight
        if bias is not None and bias.dtype == torch.float: bias = bias.to(torch.half)
        self.in_features = in_features
        self.out_features = out_features
        self.bias = bias
        self.swap_device = None
        self.full_in_features = full_in_features
        self.full_out_features = full_out_features
        self.first_in_feature = first_in_feature
        self.first_out_feature = first_out_feature
        self.out_dtype = out_dtype

        if self.weight.shape[0] == full_in_features and self.weight.shape[0] != in_features:
            self.weight = self.weight[first_in_feature : first_in_feature + in_features, :]
        if self.weight.shape[1] == full_out_features and self.weight.shape[1] != out_features:
            self.weight = self.weight[:, first_out_feature : first_out_feature + out_features]
            if bias is not None:
                self.bias = self.bias[..., first_out_feature : first_out_feature + out_features]

        if in_features != full_in_features or out_features != full_out_features:
            w = torch.empty(self.weight.shape, dtype = self.weight.dtype, device = self.weight.device)
            w.copy_(self.weight)
            self.weight = w

    def get_tensors(self, key: str):
        t = {}
        t[f"{key}.weight"] = self.weight.T.contiguous()
        if self.bias is not None:
            t[f"{key}.bias"] = self.bias
        return t

    def forward(
        self,
        x: torch.Tensor,
        params: dict,
        out_dtype: torch.dtype | None = None,
    ) -> torch.Tensor:
        out_shape = x.shape[:-1] + (self.out_features,)
        x = x.view(-1, x.shape[-1])
        dtype = out_dtype or self.out_dtype or torch.half
        y = torch.empty(
            (x.shape[0], self.out_features),
            dtype = dtype,
            device = x.device
        )
        if dtype == x.dtype:
            torch.matmul(x, self.weight, out = y)
        else:
            ext.hgemm(x, self.weight, y)
        if self.bias is not None:
            y += self.bias
        y = y.view(out_shape)
        return y

    def get_weight_tensor(self) -> torch.Tensor:
        return self.weight

    def get_bias_tensor(self) -> torch.Tensor | None:
        return self.bias

    def set_weight(self, w: torch.Tensor):
        self.weight = w.half()

    # Swap tensors to CPU (to free some space while quantizing)
    def swap_cpu(self):
        if self.swap_device is not None:
            return
        self.swap_device = self.weight.device
        self.weight = self.weight.cpu()
        if self.bias is not None:
            self.bias = self.bias.cpu()

    def unswap_cpu(self):
        if self.swap_device is None:
            return
        self.weight = self.weight.to(self.swap_device)
        if self.bias is not None:
            self.bias = self.bias.to(self.swap_device)
        self.swap_device = None


class LinearFP16_torch:

    in_features: int
    out_features: int

    def __init__(
        self,
        in_features: int,
        out_features: int,
        weight: torch.Tensor,
        bias: torch.Tensor | None,
    ):
        self.nn_linear = nn.Linear(
            in_features,
            out_features,
            bias is not None,
            device = "meta"
        )
        self.nn_linear.weight = nn.Parameter(weight, requires_grad = False)
        if bias is not None:
            self.nn_linear.bias = nn.Parameter(bias, requires_grad = False)

    def get_tensors(self, key: str):
        t = {}
        t[f"{key}.weight"] = self.nn_linear.weight.data.contiguous()
        if self.nn_linear.bias is not None:
            t[f"{key}.bias"] = self.nn_linear.bias.data.contiguous()
        return t

    def forward(self, x: torch.Tensor, params: dict, out_dtype: torch.dtype | None = None) -> torch.Tensor:
        x = self.nn_linear.forward(x)
        return to2(x, out_dtype)

    def get_weight_tensor(self) -> torch.Tensor:
        return self.nn_linear.weight.data.T

    def get_bias_tensor(self) -> torch.Tensor | None:
        return self.nn_linear.bias.data if self.nn_linear.bias is not None else None

    def set_weight(self, w):
        self.nn_linear.weight = nn.Parameter(w.T.half())

</content>

<content full_path="exllamav3/modules/quant/__init__.py">
from .fp16 import LinearFP16, LinearFP16_torch
from .exl3 import LinearEXL3


</content>

<content full_path="exllamav3/modules/quant/exl3.py">
from __future__ import annotations
import torch
from ...models.config import Config
from ...util.tensor import to2
from ...util import first_not_none
import math
from .exl3_lib.quantize import preapply_had_l, preapply_had_r, had_k, had_n, tensor_core_perm, tensor_core_perm_i
from ...ext import exllamav3_ext as ext
from ...util import profile_opt

class LinearEXL3:

    def __init__(
        self,
        config: Config,
        in_features: int,
        out_features: int,
        scale: torch.Tensor | None = None,
        su: torch.Tensor | None = None,
        sv: torch.Tensor | None = None,
        suh: torch.Tensor | None = None,
        svh: torch.Tensor | None = None,
        trellis: torch.Tensor | None = None,
        mcg: torch.Tensor | None = None,
        mul1: torch.Tensor | None = None,
        bias: torch.Tensor | None = None,
        out_dtype: torch.dtype | None = None,
    ):
        assert scale is None, "scale is no longer used"
        assert su is not None or suh is not None, "either su (packed) or suh (unpacked) is required"
        assert sv is not None or svh is not None, "either sv (packed) or svh (unpacked) is required"
        assert trellis is not None, "trellis is required"
        if su is not None: assert su.dtype == torch.int16, "su is wrong datatype"
        if sv is not None: assert sv.dtype == torch.int16, "sv is wrong datatype"
        if suh is not None: assert suh.dtype == torch.half, "suh is wrong datatype"
        if svh is not None: assert svh.dtype == torch.half, "svh is wrong datatype"
        assert trellis.dtype == torch.int16, "trellis is wrong datatype"
        assert len(trellis.shape) == 3, "trellis must have dim = 3"

        if bias is not None and bias.dtype == torch.float: bias = bias.to(torch.half)

        # self.scale = scale.item()
        self.su = None
        self.sv = None
        self.suh = suh if suh is not None else self.unpack_bf(su)
        self.svh = svh if svh is not None else self.unpack_bf(sv)
        self.trellis = trellis
        self.K = trellis.shape[-1] // 16
        self.in_features = in_features
        self.out_features = out_features
        self.bias = bias
        self.swap_device = None
        self.out_dtype = out_dtype
        self.mcg_mult = None

        self.mcg = mcg
        self.mcg_mult = mcg.view(torch.uint32).item() if mcg is not None else 0

        self.mul1 = mul1
        self.mul1_mult = mul1.view(torch.uint32).item() if mul1 is not None else 0


    def get_tensors(self, key: str):
        t = {}
        # t[f"{key}.scale"] = torch.tensor([self.scale], dtype = torch.float, device = self.su.device)
        if self.su is not None: t[f"{key}.su"] = self.su.contiguous()
        if self.suh is not None: t[f"{key}.suh"] = self.suh.contiguous()
        if self.sv is not None: t[f"{key}.sv"] = self.sv.contiguous()
        if self.svh is not None: t[f"{key}.svh"] = self.svh.contiguous()
        t[f"{key}.trellis"] = self.trellis.contiguous()
        if self.bias is not None: t[f"{key}.bias"] = self.bias.contiguous()
        if self.mcg_mult: t[f"{key}.mcg"] = self.mcg
        if self.mul1_mult: t[f"{key}.mul1"] = self.mul1
        return t


    def forward(
        self,
        x: torch.Tensor,
        params: dict,
        out_dtype: torch.dtype | None = None,
    ) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]:

        out_shape = x.shape[:-1] + (self.out_features,)
        x = x.view(-1, self.in_features)

        torch_mode = params.get("reconstruct", x.shape[0] > 32)
        ret_dtype = out_dtype or self.out_dtype or torch.half

        if torch_mode:
            y = torch.empty(out_shape, dtype = ret_dtype, device = x.device)
            y_ = y.view(x.shape[0], self.out_features)
            xh = torch.empty_like(x)
            ext.had_r_128(x, xh, self.suh, None, 1.0)
            w = self.get_inner_weight_tensor()
            # TODO: Test torch.matmul for Blackwell
            ext.hgemm(xh, w, y_)
            ext.had_r_128(y_, y_, None, self.svh, 1.0)
        else:
            y = torch.empty(out_shape, dtype = ret_dtype, device = x.device)
            xh = torch.empty_like(x)
            ext.exl3_gemm(x, self.trellis, y, self.suh, xh, self.svh, -1, self.mcg_mult, self.mul1_mult)

        if self.bias is not None:
            y += self.bias

        return y


    def unpack_bf(self, bitfield: torch.Tensor):
        # TODO: Maybe custom kernel for this. Only used for full reconstruct and loading old models, not during inference
        bitfield = bitfield.view(torch.uint16).to(torch.int)
        masks = (1 << torch.arange(16)).to(bitfield.device)
        expanded = (bitfield.unsqueeze(-1) & masks) > 0
        expanded = expanded.flatten()
        expanded = torch.where(expanded, torch.tensor(-1.0, dtype = torch.float16), torch.tensor(1.0, dtype = torch.float16))
        return expanded.contiguous()


    def get_weight_tensor(self):
        # suh = self.unpack_bf(self.su).unsqueeze(1)
        suh = self.unpack_bf(self.su).unsqueeze(1) if self.su else self.suh.unsqueeze(1)
        svh = self.unpack_bf(self.sv).unsqueeze(0) if self.sv else self.svh.unsqueeze(0)
        w = self.get_inner_weight_tensor()
        w = preapply_had_l(w, had_k)
        w *= suh
        w = preapply_had_r(w, had_n)
        w *= svh
        # w *= self.scale
        return w


    def get_inner_weight_tensor(self):
        w = torch.empty((self.in_features, self.out_features), dtype = torch.half, device = self.trellis.device)
        ext.reconstruct(w, self.trellis, self.K, self.mcg_mult, self.mul1_mult)
        return w


    def get_bias_tensor(self) -> torch.Tensor | None:
        return self.bias


    # Swap tensors to CPU (to free some space while quantizing)
    def swap_cpu(self):
        if self.swap_device is not None:
            return
        self.swap_device = self.trellis.device
        if self.su is not None: self.su = self.su.cpu()
        if self.sv is not None: self.sv = self.sv.cpu()
        if self.suh is not None: self.suh = self.suh.cpu()
        if self.svh is not None: self.svh = self.svh.cpu()
        if self.trellis is not None: self.trellis = self.trellis.cpu()
        if self.bias is not None: self.bias = self.bias.cpu()


    def unswap_cpu(self):
        if self.swap_device is None:
            return
        if self.su is not None: self.su = self.su.to(self.swap_device)
        if self.sv is not None: self.sv = self.sv.to(self.swap_device)
        if self.suh is not None: self.suh = self.suh.to(self.swap_device)
        if self.svh is not None: self.svh = self.svh.to(self.swap_device)
        if self.trellis is not None: self.trellis = self.trellis.to(self.swap_device)
        if self.bias is not None: self.bias = self.bias.to(self.swap_device)
        self.swap_device = None

</content>

<content full_path="exllamav3/modules/quant/exl3_lib/quantize.py">
import torch
import torch.nn.functional as F
import math
from ....ext import exllamav3_ext as ext
from ....util.progress import ProgressBar
from ....util.memory import free_mem, list_gpu_tensors
from ....util.hadamard import get_hadamard_dt
from ....util import cuda_sync_active
from ....util.tensor import save_tensor_image
from functools import lru_cache

# Constant
had_k, had_n = 128, 128
codebook_scale = 1.24371088

@lru_cache
def tensor_core_perm(device):
    perm_a = [0] * 256
    for t in range(32):
        r0 = (t % 4) * 2
        r1 = r0 + 1
        r2 = r0 + 8
        r3 = r0 + 9
        c0 = t // 4
        c1 = c0 + 8
        perm_a[t * 8 + 0] = r0 * 16 + c0
        perm_a[t * 8 + 1] = r1 * 16 + c0
        perm_a[t * 8 + 2] = r2 * 16 + c0
        perm_a[t * 8 + 3] = r3 * 16 + c0
        perm_a[t * 8 + 4] = r0 * 16 + c1
        perm_a[t * 8 + 5] = r1 * 16 + c1
        perm_a[t * 8 + 6] = r2 * 16 + c1
        perm_a[t * 8 + 7] = r3 * 16 + c1
    return torch.tensor(perm_a, dtype = torch.int, device = device)


@lru_cache
def tensor_core_perm_i(device):
    return torch.argsort(tensor_core_perm(device))


@lru_cache
def get_temp_buffers(device, K: int):
    max_batch_size = 128
    temp_costs = torch.zeros((max_batch_size, 2, 65536 >> K), dtype = torch.float, device = device)
    temp_edges = torch.zeros((max_batch_size, 256, 65536 >> K), dtype = torch.short, device = device)
    return temp_costs, temp_edges


def quantize_tiles(tiles, quant_args: dict):
    tiles = tiles.contiguous()
    assert tiles.shape[1] == 256
    assert tiles.dtype == torch.float

    K = quant_args["K"]
    mcg_mult = quant_args.get("mcg_mult", 0)
    mul1_mult = quant_args.get("mul1_mult", 0)
    quantized_tiles = torch.zeros_like(tiles)
    quantized_idx = torch.zeros_like(tiles, dtype = torch.short)
    temp_costs, temp_edges = get_temp_buffers(tiles.device, K)
    ext.quantize_tiles(
        tiles,
        quantized_tiles,
        quantized_idx,
        temp_costs,
        temp_edges,
        K,
        mcg_mult,
        mul1_mult,
    )
    return quantized_tiles, quantized_idx


@lru_cache
def get_quant_stream(device):
    return torch.cuda.Stream(device = device)


pinned_tiles: torch.Tensor | None = None
pinned_q_tiles: torch.Tensor | None = None
pinned_q_idx: torch.Tensor | None = None
def get_pinned(num_tiles: int):
    global pinned_tiles, pinned_q_tiles, pinned_q_idx
    if pinned_tiles is None or pinned_tiles.shape[0] < num_tiles:
        pinned_tiles = torch.empty((num_tiles, 256), device = "cpu", dtype = torch.float, pin_memory = True)
        pinned_q_tiles = torch.empty((num_tiles, 256), device = "cpu", dtype = torch.float, pin_memory = True)
        pinned_q_idx = torch.empty((num_tiles, 256), device = "cpu", dtype = torch.int16, pin_memory = True)
    return pinned_tiles[:num_tiles, :], pinned_q_tiles[:num_tiles, :], pinned_q_idx[:num_tiles, :]


def quantize_tiles_multigpu(tiles, quant_args: dict):
    devices = quant_args["devices"]
    if len(devices) == 1:
        return quantize_tiles(tiles, quant_args)

    # Get pinned buffers
    pin_tiles, pin_q_tiles, pin_q_idx = get_pinned(tiles.shape[0])

    # Copy input tiles to pinned memory. Input is always on the first device in the split
    copy_input_event = torch.cuda.Event(blocking = False)
    main_stream = get_quant_stream(devices[0])
    with torch.cuda.stream(main_stream):
        tiles = tiles.contiguous()
        pin_tiles.copy_(tiles, non_blocking = True)
        copy_input_event.record(main_stream)

    # Create split slices for input tiles, output tiles and output indices
    ratios = quant_args.get("device_ratios")
    if ratios:
        s = sum(ratios)
        split_sizes = [tiles.shape[0] * r / s for r in ratios]
        split_sizes = [round(s / 16) * 16 for s in split_sizes]
        split_sizes[-1] += tiles.shape[0] - sum(split_sizes)
    else:
        split_sizes = [tiles.shape[0] // len(devices)] * len(devices)
        split_sizes[-1] += tiles.shape[0] - sum(split_sizes)

    # Account for negative splits (edge case if too many GPUs and/or tensor too small)
    for i in range(len(split_sizes) - 2, -1, -1):
        if split_sizes[i + 1] < 0:
            split_sizes[i] += split_sizes[i + 1]
            split_sizes[i + 1] = 0

    pin_split_tiles = torch.split(pin_tiles, split_sizes)
    pin_split_q_tiles = torch.split(pin_q_tiles, split_sizes)
    pin_split_q_idx = torch.split(pin_q_idx, split_sizes)

    slice_done_events = []
    for i, device in enumerate(devices):

        stream = get_quant_stream(device)
        with torch.cuda.stream(stream):

            # Wait for input in host memory
            if i > 0:
                stream.wait_event(copy_input_event)

            if split_sizes[i] > 0:

                # Asynchronously copy the slice from the pinned buffer to device memory
                dev_tiles = pin_split_tiles[i].to(device, non_blocking = True)

                # Preallocate output tensors on the device.
                dev_q_tiles = torch.empty_like(dev_tiles, device = device)
                dev_q_idx = torch.empty_like(dev_tiles, dtype = torch.short, device = device)

                # Work buffers
                K = quant_args["K"]
                mcg_mult = quant_args.get("mcg_mult", 0)
                mul1_mult = quant_args.get("mul1_mult", 0)
                temp_costs, temp_edges = get_temp_buffers(device, K)

                ext.quantize_tiles(
                    dev_tiles,
                    dev_q_tiles,
                    dev_q_idx,
                    temp_costs,
                    temp_edges,
                    K,
                    mcg_mult,
                    mul1_mult
                )

                # Async copy back to pinned memory
                pin_split_q_tiles[i].copy_(dev_q_tiles, non_blocking = True)
                pin_split_q_idx[i].copy_(dev_q_idx, non_blocking = True)

            # Finished slice
            evt = torch.cuda.Event(blocking = False)
            slice_done_events.append(evt)
            evt.record(stream)

    # Copy pinned buffers to original device
    with torch.cuda.stream(main_stream):
        for evt in slice_done_events:
            main_stream.wait_event(evt)
        q_tiles = torch.empty_like(tiles, device = devices[0])
        q_idx = torch.empty_like(tiles, dtype = torch.short, device = devices[0])
        q_tiles.copy_(pin_q_tiles, non_blocking = True)
        q_idx.copy_(pin_q_idx, non_blocking = True)

    return q_tiles, q_idx


def quantize_tiles_multigpu_sync(tiles, quant_args: dict):
    devices = quant_args["devices"]
    if len(devices) == 1:
        return quantize_tiles(tiles, quant_args)

    tiles = tiles.contiguous()

    split_sizes = [tiles.shape[0] // len(devices)] * len(devices)
    split_sizes[-1] += tiles.shape[0] - sum(split_sizes)
    split_tiles = torch.split(tiles, split_sizes)
    tiles_per_device = [chunk.to(device) for chunk, device in zip(split_tiles, devices)]
    torch.cuda.synchronize()

    q_tiles_per_device = []
    q_idx_per_device = []
    for dev_tiles, device in zip(tiles_per_device, devices):
        with torch.cuda.stream(get_quant_stream(device)):
            dev_q_tiles, dev_q_idx = quantize_tiles(dev_tiles, quant_args)
            q_tiles_per_device.append(dev_q_tiles)
            q_idx_per_device.append(dev_q_idx)

    for device in devices:
        torch.cuda.synchronize(device)

    q_tiles_per_device = [x.to(devices[0]) for x in q_tiles_per_device]
    q_idx_per_device = [x.to(devices[0]) for x in q_idx_per_device]
    quantized_tiles = torch.cat(q_tiles_per_device, dim = 0)
    quantized_idx = torch.cat(q_idx_per_device, dim = 0)
    return quantized_tiles, quantized_idx


def preapply_had_l(x: torch.Tensor, had_dim):
    k, n = x.shape
    x_dtype = x.dtype
    x = x.to(torch.float)
    had = get_hadamard_dt(had_dim, x.device, x.dtype, 1 / math.sqrt(had_dim))
    x = (had @ x.view(-1, had_dim, n)).view(k, n)
    x = x.to(x_dtype)
    return x


def preapply_had_r(x: torch.Tensor, had_dim):
    k, n = x.shape
    x_dtype = x.dtype
    x = x.to(torch.float)
    had = get_hadamard_dt(had_dim, x.device, x.dtype, 1 / math.sqrt(had_dim))
    x = (x.view(k, -1, had_dim) @ had).view(k, n)
    x = x.to(x_dtype)
    return x


def blockwise_preapply_had_l_(x: torch.Tensor, had_dim):
    k, n = x.shape
    assert k % had_dim == 0
    assert x.dtype == torch.float
    had = get_hadamard_dt(had_dim, x.device, x.dtype, 1 / math.sqrt(had_dim))
    num_blocks = k // had_dim
    for i in range(num_blocks):
        start = i * had_dim
        end = start + had_dim
        block = x[start:end, :]  # shape (had_dim, n)
        block_transformed = had @ block.view(had_dim, n)
        x[start:end, :] = block_transformed


def blockwise_preapply_had_r_(x: torch.Tensor, had_dim):
    k, n = x.shape
    assert n % had_dim == 0
    assert x.dtype == torch.float
    had = get_hadamard_dt(had_dim, x.device, x.dtype, 1 / math.sqrt(had_dim))
    num_blocks = n // had_dim
    for i in range(num_blocks):
        start = i * had_dim
        end = start + had_dim
        block = x[:, start:end]  # shape (k, had_dim)
        block_transformed = block @ had
        x[:, start:end] = block_transformed


def block_ldl(H: torch.Tensor, b: int, verbose: bool):

    n, _ = H.shape
    assert (n % b == 0)
    m = n // b

    # Cholesky factorization: H = L @ L.T
    # Try on GPU first
    try:
        retry_cpu = False
        L = torch.linalg.cholesky(H)
        # H is not needed after this, move to CPU. Then overwrite H's GPU storage with L, since we can't otherwise
        # free up that VRAM as the tensor is referenced by the parent frame
        H_cpu = H.cpu()
        H.copy_(L)  # VRAM copy, tiny overhead
        L = H
        H = H_cpu

    # Fall back on CPU factorization
    except Exception as e:
        if e.__class__.__name__ == "OutOfMemoryError" or "CUDA out of memory" in str(e) or "HIP out of memory" in str(e):
            retry_cpu = True
        else:
            raise e
    if retry_cpu:
        print(f" !! Out of memory on {str(H.device)}, trying CPU fallback")
        free_mem()
        H_cpu = H.cpu()
        L_cpu = torch.linalg.cholesky(H_cpu)
        # This is ugly, but overwrite H in VRAM to avoid allocating a new tensor, then replace reference with CPU copy
        H.copy_(L_cpu)
        del L_cpu
        L = H
        H = H_cpu

    # Get blocks along diagonal of L: DL.shape = (m, b, b)
    DL = torch.diagonal(L.reshape(m, b, m, b), dim1 = 0, dim2 = 2).permute(2, 0, 1)

    # Compute D as D[i] = DL[i] @ DL[i].T for each diagonal block i (don't actually end up needing this)
    # D = DL @ DL.transpose(1, 2)

    # Invert each diagonal block
    DL = torch.linalg.inv(DL)

    # Multiply each block's column with its inverse
    L = L.view(n, m, b)
    for i in range(m):
        L[:, i, :] = L[:, i, :] @ DL[i, :, :]  # TODO: Could maybe be L[m * b:, i, :]?
    L = L.reshape(n, n).contiguous()

    # Insert block identity matrices along the diagonal.
    # TODO: Figure out if this is necessary. Diagonal blocks should already be identities after previous step
    L_block = L.view(m, b, m, b).permute(0, 2, 1,3)
    dr = torch.arange(m)
    L_block[dr, dr] = torch.stack([torch.eye(b, device = L.device, dtype = H.dtype)] * m)

    return L, H  # , D.to(DL.device)


def ldlq(
    weight: torch.Tensor,
    L: torch.Tensor,
    quant_args: dict,
    pb: ProgressBar | None = None
):
    """
    :param weight:
        Input weights, shape (k, n). If device is "cpu", result is collected on CPU as well, saving a bunch of
        VRAM but adding a little PCIe overhead and many sync points

    :param L:
        LDL decomposition of regularized H

    :param quant_args:
        dict:
         - K: bitrate
         - buf_size_k: buffer size for LDLQ, along k

    :param pb:
        Optional ProgressPar to update, k // 16 steps

    :return:
        tuple:
         - quantized weight, shape (k, n)
         - indices (unpacked), shape (k // 16, n // 16, 256), uint16_t
    """

    devices = quant_args["devices"]
    for device in devices:
        torch.cuda.synchronize(device)
    main_stream = get_quant_stream(devices[0])
    with torch.cuda.stream(main_stream):

        devices = quant_args["devices"]
        device = L.device
        assert device == torch.device(devices[0])

        buffer_device = weight.device
        size_k, size_n = weight.shape  # Row-major
        assert size_k % 16 == 0
        assert size_n % 128 == 0
        tiles_k = size_k // 16
        tiles_n = size_n // 16

        buf_size_k = max(quant_args.get("buf_size_k", 128), 16)
        assert buf_size_k % 16 == 0
        assert size_n % buf_size_k == 0

        p_row = 0

        # Work buffers
        prod_cache = torch.zeros((size_k, size_n), dtype = torch.float, device = device)
        weight_q = torch.zeros((size_k, size_n), dtype = torch.float, device = buffer_device)
        encoded = torch.zeros((tiles_k, tiles_n, 256), dtype = torch.short, device = buffer_device)

        for j in range(size_k, 0, -buf_size_k):
            i = j - buf_size_k

            # Current span is rows i:j
            b_weight = weight[i:j].to(device)
            b_weight_q = weight_q[i:j] if device == buffer_device else \
                torch.zeros_like(weight_q[i:j], device = device)
            b_encoded = encoded[i // 16 : j // 16] if device == buffer_device else \
                torch.zeros_like(encoded[i // 16 : j // 16], device = device)
            b_prod_cache = prod_cache[i:j]
            b_L = L[i:j]

            # Iterate over rows of blocks in current span
            for bj in range(buf_size_k, 0, -16):
                bi = bj - 16

                # Error so far for the current span
                bb_err = b_weight[bj:] - b_weight_q[bj:]

                # Corresponding slice of LDL decomposition of H
                bb_L = b_L[bj:, i + bi:i + bj]

                # Input tiles for quantization
                compensation_term = b_prod_cache[bi:bj]
                compensation_term.addmm_(bb_L.T, bb_err,  alpha = 1.0, beta = 1.0)
                rows = b_weight[bi:bj] + compensation_term

                tiles = rows.reshape(16, tiles_n, 16).permute(1, 0, 2).reshape(tiles_n, 256)

                # Pre-permute to tensor core layout
                tiles = tiles[:, tensor_core_perm(device)]

                # Quantize
                quant_w, quant_i = quantize_tiles_multigpu(tiles, quant_args)

                # Undo permutation on reconstructed tiles, but keep indices in tensor core layout
                quant_w = quant_w[:, tensor_core_perm_i(device)]

                # Store result
                quant_w = quant_w.reshape(tiles_n, 16, 16).permute(1, 0, 2).reshape(16, size_n)
                b_weight_q[bi:bj] = quant_w
                b_encoded[bi // 16 : bj // 16] = quant_i.unsqueeze(0)

                # Update progress
                if pb:
                    p_row += 1
                    pb.update(p_row)

            # Collect output
            if device != buffer_device:
                weight_q[i:j] = b_weight_q.to(buffer_device)
                encoded[i // 16 : j // 16] = b_encoded.to(buffer_device)

            # Cache error term for the rest of the matrix
            b_err = b_weight - b_weight_q
            prod_cache.addmm_(b_L.T, b_err, alpha = 1.0, beta = 1.0)

        for device in devices:
            torch.cuda.synchronize(device)

    return weight_q, encoded


def finalize_capture_H(H_data: dict, quant_args: dict, verbose: bool):
    # Unswap H
    if "H_swap_device" in H_data:
        H_data["H"] = H_data["H"].to(H_data["H_swap_device"])
        del H_data["H_swap_device"]

    H = H_data["H"]
    if H_data["finalized"]:
        return H, H_data["L"], H_data["su"], H_data["diag"]

    # Mean of samples summed up during forward pass
    H /= H_data["count"]

    # Regularize diagonal
    diag_mean = torch.diag(H).mean()
    idx = torch.arange(H.shape[0])
    H[idx, idx] += quant_args.get("sigma_reg", 0.025) * diag_mean

    # Some tests
    diag = H[idx, idx].clone()

    if verbose:
        print(f"     - H min/max: {H.min().item():.6f}   {H.max().item():.6f}")
        print(f"     - H mean/std: {H.mean().item():.6f}   {H.std().item():.6f}")
        print(f"     - H diag min/max: {diag.min():.6f}   {diag.max():.6f} ")

    # Random sign flips for input channel, fixed for the first linear layer to quantize with this H
    k = H.shape[0]
    su = (torch.randn(k, device = H.device).sign() + 1e-5).sign().to(torch.float).unsqueeze(1)
    H_data["su"] = su

    # Input had
    H *= su.T
    blockwise_preapply_had_r_(H, had_k)
    H *= su
    blockwise_preapply_had_l_(H, had_k)

    # Get block LDL decomposition of H, zero diagonal
    L, H = block_ldl(H, 16, verbose)
    dr = torch.arange(k)
    L[dr, dr] = 0
    H_data["L"] = L

    # H is no longer needed except to compute proxy error so move to CPU
    H = H.cpu()
    H_data["H"] = H.cpu()

    H_data["finalized"] = True
    H_data["diag"] = diag
    return H, L, su, diag


def pack_trellis(encoded: torch.Tensor, quant_args: dict) -> torch.Tensor:
    K = quant_args["K"]
    shape = encoded.shape
    assert len(shape) == 3 and shape[2] == 256
    assert encoded.dtype == torch.int16
    packed_shape = (shape[0], shape[1], 256 * K // 16)
    packed = torch.zeros(packed_shape, dtype = torch.int16, device = encoded.device)
    ext.pack_trellis(packed, encoded.contiguous(), K)
    # unpacked = torch.zeros_like(encoded)
    # ext.unpack_trellis(unpacked, packed, K)
    # assert torch.equal(unpacked, encoded)
    return packed


def pack_signs(signs: torch.Tensor, quant_args: dict) -> torch.Tensor:
    signs = signs.half().flatten().contiguous()
    assert signs.shape[0] % 16 == 0
    packed = torch.zeros(signs.shape[0] // 16, dtype = torch.int16, device = signs.device)
    ext.pack_signs(packed, signs)
    return packed


def g_scale_gss(
    weight_r: torch.Tensor,
    verbose: bool,
    quant_args: dict,
    width: int = 3,
    pb: ProgressBar = None
):
    # Select a sample of tiles along a wrapped diagonal (sampling from every row and column of tiles, hopefully
    # representative) and search for the global scale within given range that minimizes the direct quantization
    # error
    tiles = []
    tiles_k = weight_r.shape[0] // 16
    tiles_n = weight_r.shape[1] // 16
    for i in range(max(tiles_k, tiles_n)):
        for w in range(width):
            k = (i % tiles_k) * 16
            n = ((i + w) % tiles_n) * 16
            tile = weight_r[k : k + 16, n : n + 16].clone()
            tile = tile.view(256)
            tile = tile[tensor_core_perm(weight_r.device)]
            tiles.append(tile)
    tiles = torch.stack(tiles)

    devices = quant_args["devices"]
    for device in devices:
        torch.cuda.synchronize(device)

    main_stream = get_quant_stream(devices[0])
    # TODO: Figure out why Torch always initializes cuda:0 when exiting this CM, even when it's not used
    with torch.cuda.stream(main_stream):

        def test_scale(scale: float):
            quant_w, quant_i = quantize_tiles_multigpu(tiles * scale, quant_args)
            mse = ((quant_w / scale - tiles) ** 2).mean()
            return mse

        # Assume quantization error is a unimodal function of scale, golden section search to find minimum
        phi = (1 + math.sqrt(5)) / 2
        resphi = 2 - phi

        a, b = 0.1, 1.9
        tol = 0.01
        delta1 = abs(b - a)

        x1 = a + resphi * (b - a)
        x2 = b - resphi * (b - a)
        f1 = test_scale(x1)
        f2 = test_scale(x2)
        while abs(b - a) > tol:
            # if verbose:
                # print(f"     - gss: a = {a:.6f}, b = {b:.6f}")
            if f1 < f2:
                b = x2
                x2 = x1
                f2 = f1
                x1 = a + resphi * (b - a)
                f1 = test_scale(x1)
            else:
                a = x1
                x1 = x2
                f1 = f2
                x2 = b - resphi * (b - a)
                f2 = test_scale(x2)
            delta2 = abs(b - a)
            if pb:
                pb.update(100 - 100 * int(delta2 / delta1))

        best_scale = (a + b) / 2
        if verbose:
            print(f"     - gss: min = {best_scale:.6f}, mse: {(f1 + f2) / 2:.6f}")

    devices = quant_args["devices"]
    for device in devices:
        torch.cuda.synchronize(device)

    return best_scale, (f1 + f2) / 2


def block_rms(x: torch.Tensor, dim: int, keepdim: bool = False, blocksize: int = 32):
    # Compute blockwise x.square().mean(dim, keepdim).sqrt()
    n = x.size(dim)
    sq = None
    for block in torch.split(x, blocksize, dim = dim):
        block_sq = block.square().sum(dim = dim, keepdim = keepdim)
        if sq is None:
            sq = block_sq
        else:
            sq += block_sq
    mean_sq = sq / n
    return mean_sq.sqrt()


def block_rms_n(x: torch.Tensor, dim: int = 0, blocksize: int = 32):
    # Compute blockwise x.square().mean().sqrt()
    n = 0
    sq = None
    for block in torch.split(x, blocksize, dim = dim):
        block_sq = block.square().sum()
        n += block.numel()
        if sq is None:
            sq = block_sq
        else:
            sq += block_sq
    mean_sq = sq / n
    return mean_sq.sqrt()


def block_nmse(x: torch.Tensor, y: torch.Tensor, dim: int = 0, blocksize: int = 32):
    # Compute blockwise (x - y).square().mean().item() / y.square().mean().item()
    sq = None
    diff_sq = None
    for block_x, block_y in zip(torch.split(x, blocksize, dim = dim), torch.split(y, blocksize, dim = dim)):
        block_sq = block_y.square().sum()
        block_diff_sq = (block_x - block_y).square().sum()
        if sq is None:
            sq = block_sq
            diff_sq = block_diff_sq
        else:
            sq += block_sq
            diff_sq += block_diff_sq
    return diff_sq.item() / (sq.item() + 1e-20)


def regularize(
    weight: torch.Tensor,
    su: torch.Tensor,
    sv: torch.Tensor,
    quant_args: dict,
    verbose: bool,
    H_diag: torch.Tensor | None,
    pb: ProgressBar | None,
    skip_g_scale: bool = False
):
    force_out_scales = quant_args["apply_out_scales"]

    # dist_ref = torch.empty((512,), dtype = torch.float, device = weight.device)
    # dist_r = torch.empty_like(dist_ref)
    def jsd(h1, h2):
        m = (h1 + h2) / 2
        eps = 1e-12
        js = F.kl_div((h1 + eps).log(), m, reduction = "sum") + \
             F.kl_div((h2 + eps).log(), m, reduction = "sum")
        return js / 2

    # From experiments, it seems the deciding factor in when scaling output channels is beneficial is when
    # the input to the linear layer is very irregular. After some testing, set the cutoff at 15% of the RMS sum
    # on 2% of the channels
    # TODO: More science
    if H_diag is not None:
        diag = H_diag.sqrt()
        diag, _ = torch.sort(diag, descending = True)
        cutoff = diag.shape[0] // 50
        skew_factor = diag[:cutoff].sum() / diag.sum()
        if verbose:
            print(f"     - input state skew: {skew_factor.item():.6f}")

        if force_out_scales is None:
            apply_out_scales = skew_factor.item() < 0.15
        else:
            apply_out_scales = force_out_scales

    else:
        apply_out_scales = True if force_out_scales is None else force_out_scales

    # Apply output scales
    if apply_out_scales:
        out_channel_scales = block_rms(weight, dim = 0, keepdim = True)
        out_channel_scales /= out_channel_scales.mean()
        sv = (sv * out_channel_scales + 1e-10).float()
        if verbose:
            out_channel_std = out_channel_scales.std().item()
            out_channel_mean = out_channel_scales.mean().item()
            print(f"     - out ch scales std/mean: {out_channel_std:.6f}   {out_channel_mean:.6f}")

    # Output sign flips (and scales)
    weight /= sv

    # Output hadamard transform
    blockwise_preapply_had_r_(weight, had_n)

    # Input sign flips and scales
    in_channel_scales = block_rms(weight, dim = 1, keepdim = True)
    su = (su * in_channel_scales / (-codebook_scale) + 1e-10).float()  # mustn't be inplace
    weight /= su
    blockwise_preapply_had_l_(weight, had_k)

    # Determine best scale for matrix by test quantizing a sample of tiles along a wrapped diagonal
    if not skip_g_scale:
        g_scale, mse_scale = g_scale_gss(weight, False, quant_args, pb = pb)
    else:
        g_scale = 1.0
    weight *= g_scale
    su /= g_scale

    # ext.test_distribution(weight_os, dist_r, dist_ref, -3.8, 3.8)
    # js_os = jsd(dist_r, dist_ref)

    if verbose:
        print(f"     - su/sv std: {su.std().item():.6f}   {sv.std().item():.6f}")
        print(f"     - global scale: {g_scale:.6f}")
        print(f"     - sample mse: {mse_scale.item():.6f}")
        print(f"     - apply_out_scales: {str(apply_out_scales)}")

    return apply_out_scales, weight, g_scale, su, sv


def quantize_exl3(
    weight: torch.Tensor,
    H_data: dict,
    quant_args: dict,
    return_weight_q: bool,
    progress_str: str | None = None,
    verbose: bool = False,
    swap_to_device: torch.device | None = None,
    save_reg: str = None
):
    """
    :param weight:
        Input tensor, row major shape (in_features, out_features)

    :param H_data:
        Dictionary of hessian tensor and related data, as collected by Linear wrapper class. May be reused between
        linear layers with the same input (e.g. Q, K and V projections)

    :param quant_args:
        dict:
         - K: bitrate
         - seed: integer seed for random sign flips etc.
         - sigma_reg: regularization factor

    :param return_weight_q:
        Return quantized weight

    :param progress_str:
        Show progress bar during quantization

    :param verbose:
        Dump extra stats

    :param swap_to_device:
        If input tensor is on CPU, move to this device before quantization

    :param save_reg:
        Save regularized tensor as image to the provided path

    :return:
        tuple:
          - quantized weight
          - proxy error: trace(err @ H @ err.T) / (W @ H @ W.T)
          - quantized and packed tensors
    """

    progress_text = None if not progress_str else progress_str.replace("<step>", "Preparing")
    with (ProgressBar(progress_text, 100) as pb):

        assert weight.dtype == torch.float
        tiles_k = weight.shape[0] // 16

        if "seed" in quant_args:
            torch.manual_seed(quant_args["seed"])

        device = weight.device if swap_to_device is None else swap_to_device
        k, n = weight.shape

        # Get H, LDL decomp. and input/output sign flips
        H, L, su, H_diag = finalize_capture_H(H_data, quant_args, verbose)
        sv = (torch.randn(n, device = device).sign() + 1e-5).sign().to(torch.float).unsqueeze(0)

        # Move stored L to CPU (if not already), move working L to device
        H_data["L"] = H_data["L"].cpu()
        L = L.to(device)

        if swap_to_device is not None:
            weight = weight.to(swap_to_device)
        if verbose:
            weight_copy = weight.cpu()
        weight_r = weight
        del weight

        if verbose:
            rms = block_rms_n(weight_r, dim = 0)
            print(f"     - input tensor rms: {rms:.6f}")

        # Regularization
        apply_out_scales, weight_r, g_scale, su, sv = regularize(
            weight_r,
            su,
            sv,
            quant_args,
            verbose,
            H_diag,
            pb
        )

        if save_reg:
            save_tensor_image(weight_r, save_reg)

        if verbose:
            rms = weight_r.square().mean().sqrt()
            print(f"     - regularized rms:  {rms:.6f}")

        progress_text = None if not progress_str else progress_str.replace("<step>", "Quantizing")
        pb.update(0)
        pb.new_task(progress_text, tiles_k)

        # Select device for work buffers (CPU is slower for small tensors but saves a lot of VRAM on big ones)
        # TODO: Use pynvml or mem_get_info to predict whether CPU buffer is needed
        if weight_r.numel() > 5e8:
            weight_r = weight_r.cpu()

        # Quantize
        weight_q, encoded_q = ldlq(weight_r, L, quant_args, pb)
        del L

        pb.update(tiles_k)

        # Metrics
        try:
            E = weight_r - weight_q  # may run on CPU
            W = weight_r
            Hd = H.to(device)
            weight_r = None
            E = E.to(device)
            num = torch.einsum("ik,ij,jk->", E, Hd, E).item()
            E = None
            W = W.to(device)
            den = torch.einsum("ik,ij,jk->", W, Hd, W).item()
            W = None
            Hd = None
            proxy_err = num / max(den, 1e-8)
        except torch.OutOfMemoryError:
            weight_r = None
            E = None
            W = None
            Hd = None
            proxy_err = -1.0

        # free_mem()

        if return_weight_q or verbose:
            weight_q = weight_q.to(device)
            weight_q = preapply_had_l(weight_q, had_k)
            weight_q *= su
            weight_q = preapply_had_r(weight_q, had_n)
            weight_q *= sv

            if verbose:
                weight = weight_copy.to(device)
                nmse = block_nmse(weight_q, weight)
                print(f"     - quant nmse: {nmse:.6f}")

        # Compile packed tensor
        suh = su.flatten().contiguous().to(dtype = torch.half, copy = True)
        svh = sv.flatten().contiguous().to(dtype = torch.half, copy = True)
        trellis = pack_trellis(encoded_q.to(device), quant_args)

        out_tensors = {
            # "scale": weight_scale.to(dtype = torch.float, copy = True),
            # "su": pack_signs(su, quant_args),
            "suh": suh,
            # "sv": pack_signs(sv, quant_args),
            "svh": svh,
            "trellis": trellis,
        }

        mcg_mult = quant_args.get("mcg_mult", 0)
        if mcg_mult:
            out_tensors.update({
                "mcg": torch.tensor(mcg_mult, dtype = torch.uint32).view(torch.int)
            })
        mcg_mult = quant_args.get("mul1_mult", 0)
        if mcg_mult:
            out_tensors.update({
                "mul1": torch.tensor(mcg_mult, dtype = torch.uint32).view(torch.int)
            })

        quant_args.update({
            "apply_out_scales": apply_out_scales,
            "g_scale": g_scale,
        })

    return weight_q, proxy_err, out_tensors
</content>

<content full_path="exllamav3/modules/quant/exl3_lib/__init__.py">
from .quantize import quantize_exl3
</content>

<content full_path="util/add_quant_config.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from exllamav3.conversion.quant_config import create_quantization_config_json
import argparse

def main(args):
    filename = os.path.join(args.model_dir, "quantization_config.json")
    update = os.path.exists(filename)
    create_quantization_config_json(args.model_dir)
    if update:
        print(f"Updated {filename}")
    else:
        print(f"Created {filename}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model_dir", type = str, help = "Path to model directory", required = True)
    _args = parser.parse_args()
    main(_args)
</content>

<content full_path="science/codebook_eval.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch
from exllamav3.ext import exllamav3_ext as ext
import matplotlib.pyplot as plt

torch.set_printoptions(precision = 8, sci_mode = False, linewidth = 200)

mcg_mult = 0
mul1_mult = 0xAD9A2EC5

r = torch.arange(65536, device = "cuda:0", dtype = torch.short).unsqueeze(0)
codebook_lut = torch.zeros_like(r, dtype = torch.float)
ext.decode(r, codebook_lut, mcg_mult, mul1_mult)
codebook_lut = codebook_lut[0]

# RMS of codebook
rms = codebook_lut.square().mean().sqrt()
print(f"Codebook RMS: {rms:.6f}")

# Figure
fig = plt.figure(figsize=(12, 10))
gs = fig.add_gridspec(3, 4)

# Correlation
for i, K in enumerate([1, 2, 3, 4, 5, 6, 7, 8]):

    num_samples = 20000
    a = torch.randint(low = 0, high = 65536, size = (num_samples, 1), dtype = torch.int)
    b = torch.randint(low = 0, high = 1 << K, size = (num_samples, 1), dtype = torch.int)
    c = (a << K) | b
    v = torch.cat((a, c), dim = 1) & 0xFFFF
    x = codebook_lut[v]

    x_np = x.cpu().numpy()
    ax = fig.add_subplot(gs[i // 4, i % 4])
    ax.scatter(x_np[:, 0], x_np[:, 1], s = 1, alpha = 0.5)
    ax.set_title(f"K={K}")
    ax.axis("equal")

# Distribution
codebook_lut_np = codebook_lut.cpu().numpy()
ax = fig.add_subplot(gs[2, :])
ax.hist(codebook_lut_np, bins = 256)
ax.set_title(f"Distribution, RMS: {rms:.6f}")

plt.tight_layout()
plt.show()
</content>

<content full_path="science/kv_quant_exp.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import torch
import torch.nn.functional as F
from exllamav3 import Config, Model, Tokenizer
from exllamav3.modules import TransformerBlock
from exllamav3.util.hadamard import get_hadamard_dt
from datasets import load_dataset
from exllamav3.util.file import disk_lru_cache, disk_lru_cache_clear
from flash_attn import flash_attn_func
from ref_quant2 import quantquant
from exllamav3.ext import exllamav3_ext as ext
import math

torch.set_printoptions(precision = 8, sci_mode = False, linewidth = 200)

model_dir = "/mnt/str/models/llama3.1-8b-instruct/hf/"
device = "cuda:1"
target_layers = [0]
num_rows = 1

# Create input tensor
@disk_lru_cache("get_test_data")
def get_test_data():
    return "\n\n".join(
        load_dataset("wikitext", "wikitext-2-raw-v1", split = "test")
        ["text"]
    )

# Sample Q and K tensors from forward pass, Llama type model
@disk_lru_cache("sample_qkv")
def sample_qkv(_model_dir, _target_layers, _num_rows):

    # Load model
    config = Config.from_directory(_model_dir)
    model = Model.from_config(config)
    model.load(device, progressbar = True)
    tokenizer = Tokenizer.from_config(config)

    test_data = get_test_data()[:100000]
    eval_tokens = tokenizer.encode(test_data)
    eval_len = 2048
    eval_stride = 512
    num_tokens = eval_tokens.shape[-1]
    seqs = []
    for a in range(0, num_tokens - eval_len, eval_stride):
        b = a + eval_len
        seqs.append(eval_tokens[:, a:b])
        if len(seqs) >= num_rows:
            break
    input_ids = torch.cat(seqs, dim = 0)[:, :]

    _samples_qkv = []
    params = {}
    x = model.prepare_inputs(input_ids, params)
    for idx, module in enumerate(model.modules):
        params["prefill"] = (idx == model.last_kv_module_idx)
        x = module.prepare_for_device(x, params)
        if isinstance(module, TransformerBlock):
            block_idx = int(module.key.split(".")[-1])
            if block_idx > max(_target_layers):
                break
            if block_idx in _target_layers:
                # Pre-attn norm
                y = module.attn_norm.forward(x, params, out_dtype = torch.half)
                # Projections and RoPE
                attn = module.attn
                bsz, seqlen, _ = y.shape
                position, positions, position_ids = 0, None, None
                q, k, v = attn.project_qkv(y, params)
                q = q.view(bsz, seqlen, attn.num_q_heads, attn.head_dim)
                k = k.view(bsz, seqlen, attn.num_kv_heads, attn.head_dim)
                v = v.view(bsz, seqlen, attn.num_kv_heads, attn.head_dim)
                q, k = attn.rope.apply(q, k, position, positions, position_ids)
                # Sample right before dot product
                _samples_qkv.append((q, k, v))
        # Advance state
        x = module.forward(x, params)

    return _samples_qkv

samples_qkv = sample_qkv(model_dir, target_layers, num_rows)

# Get attention scores and output
def attn(q, k, v):
    bsz, q_len, n_heads_q, head_dim = q.shape
    _, k_len, n_heads_k, _ = k.shape
    gqa = n_heads_q // n_heads_k
    k_int = k.repeat_interleave(gqa, dim = 2)
    scores = torch.einsum('bqhd,bkhd->bhqk', q, k_int) / math.sqrt(head_dim)

    # Causal mask
    mask = torch.ones((k_len, k_len), dtype = torch.bool, device = q.device).triu(diagonal = 1)
    mask = mask[-q_len:, :]
    scores = scores.masked_fill_(mask, -65504.)

    # Now attention
    o = flash_attn_func(
        q = q,
        k = k,
        v = v,
        causal = True,
    )
    return o, scores

# Refence method
def int_quant(v, bits):
    m = 1 << (bits - 1)
    scales = torch.amax(v.abs(), dim = -1).unsqueeze(3)
    v = v / scales
    vq = (v * m).round().clamp(-m, m - 1)
    vq /= m
    vq *= scales
    return vq

# def quant_nf4(t):
#     scales = torch.amax(t.abs(), dim = -1).unsqueeze(3)
#     tq = t / scales
#     tqq = torch.empty_like(tq)
#     ext.test_nf4(tq, tqq)
#     tqq *= scales
#     return tqq

def quant_fp8(t):
    return t.to(torch.float8_e4m3fn).half()


# Kernel equiv reference
def kernel_ref_quant(v, bits):
    had32 = get_hadamard_dt(32, v.device, torch.half)
    w = v.view(-1, 32)
    m = 1 << (bits - 1)
    w = w @ had32 / math.sqrt(32)
    scales = torch.amax(w.abs(), dim = -1, keepdim = True).half()
    w = w / scales
    vq = (w * m).round().clamp(-m, m - 1)
    vq /= m
    vq *= scales
    vq = vq @ had32 / math.sqrt(32)
    vq = vq.view(v.shape)
    return vq

# KL divergence between softmax distributions
def kl_divergence_scores(s, s_prime, dim = -1, eps = 1e-8):
    alpha = F.softmax(s.float(), dim = dim)
    alpha_hat = F.softmax(s_prime.float(), dim = dim)
    kl_elementwise = alpha * (torch.log(alpha + eps) - torch.log(alpha_hat + eps))
    kl_per_item = kl_elementwise.sum(dim = dim)
    kl_mean = kl_per_item.mean()
    return kl_mean

# Normalized MSE
def nmse(o, o_prime):
    return (o - o_prime).square().mean() / o_prime.square().mean()


# Do stuff
def test_qkv(label, q, k, v, ref_o, ref_scores, q_rot = False, k_rot = False, v_rot = False):
    head_dim = q.shape[-1]
    had = get_hadamard_dt(head_dim, device, torch.half)
    if q_rot != k_rot: q = (q @ had) / math.sqrt(head_dim)
    if v_rot: v = (v @ had) / math.sqrt(head_dim)
    test_o, test_scores = attn(q, k, v)
    kld = kl_divergence_scores(test_scores, ref_scores)
    mse = nmse(test_o, ref_o)
    print(f"{label:26}   weights_kld: {kld:.6f}   output_nmse: {mse:.6f}")

with torch.inference_mode():

    head_dim = samples_qkv[0][0].shape[-1]
    had = get_hadamard_dt(head_dim, device, torch.half)

    for idx, (q, k, v) in zip(target_layers, samples_qkv):

        # Unquantized
        ref_o, ref_scores = attn(q, k, v)

        # Q4
        test_qkv(
            "Q4",
            q,
            int_quant(k, 4),
            int_quant(v, 4),
            ref_o,
            ref_scores
        )

        # Q6
        test_qkv(
            "Q6",
            q,
            int_quant(k, 6),
            int_quant(v, 6),
            ref_o,
            ref_scores
        )

        # Q8
        test_qkv(
            "Q8",
            q,
            int_quant(k, 8),
            int_quant(v, 8),
            ref_o,
            ref_scores
        )

        # Rotated Q4
        test_qkv(
            "Rot. Q4",
            q,
            int_quant((k @ had) / math.sqrt(head_dim), 4),
            int_quant((v @ had) / math.sqrt(head_dim), 4),
            ref_o,
            ref_scores,
            False, True, True
        )

        # Rotated Q6
        test_qkv(
            "Rot. Q6",
            q,
            int_quant((k @ had) / math.sqrt(head_dim), 6),
            int_quant((v @ had) / math.sqrt(head_dim), 6),
            ref_o,
            ref_scores,
            False, True, True
        )

        # Channel scales + rotated Q4
        psc_k = k.view(-1, k.shape[-2], k.shape[-1]).abs().mean(dim = 0)
        psc_v = v.view(-1, k.shape[-2], k.shape[-1]).abs().mean(dim = 0)
        test_qkv(
            "Rot. Q4 ch.scales",
            q,
            int_quant(((k / psc_k) @ had) / math.sqrt(head_dim), 4) @ had / math.sqrt(head_dim) * psc_k,
            int_quant(((v / psc_v) @ had) / math.sqrt(head_dim), 4) @ had / math.sqrt(head_dim) * psc_v,
            ref_o,
            ref_scores,
            False, False, False
        )

        # Channel scales + rotated Q4 RMS
        pscr_k = k.view(-1, k.shape[-2], k.shape[-1]).square().mean(dim = 0).sqrt()
        pscr_v = v.view(-1, k.shape[-2], k.shape[-1]).square().mean(dim = 0).sqrt()
        test_qkv(
            "Rot. Q4 ch.scales (RMS)",
            q,
            int_quant(((k / pscr_k) @ had) / math.sqrt(head_dim), 4) @ had / math.sqrt(head_dim) * pscr_k,
            int_quant(((v / pscr_v) @ had) / math.sqrt(head_dim), 4) @ had / math.sqrt(head_dim) * pscr_v,
            ref_o,
            ref_scores,
            False, False, False
        )

        # Rotated Q4 + Q6
        test_qkv(
            "Rot. Q4+Q6",
            q,
            int_quant((k @ had) / math.sqrt(head_dim), 4),
            int_quant((v @ had) / math.sqrt(head_dim), 6),
            ref_o,
            ref_scores,
            False, True, True
        )

        # NF4
        # k_nf4 = quant_nf4(k)
        # v_nf4 = quant_nf4(v)
        # test_qkv("NF4", q, k_nf4, v_nf4, ref_o, ref_scores, False, False, False)

        # Rotated NF4
        # k_h = (k @ had) / math.sqrt(128)
        # v_h = (v @ had) / math.sqrt(128)
        # k_h_nf4 = quant_nf4(k_h)
        # v_h_nf4 = quant_nf4(v_h)
        # test_qkv("RNF4", q, k_h_nf4, v_h_nf4, ref_o, ref_scores, False, True, True)

        # FP8
        test_qkv(
            "FP8 e4m3",
            q,
            quant_fp8(k),
            quant_fp8(v),
            ref_o,
            ref_scores,
            False, False, False
        )

        # Kernel
        for bits in range(2, 9):
            quant_shape = k.shape[:-1] + (128 // 32 * bits,)
            scale_shape = k.shape[:-1] + (128 // 32,)
            k_quant = torch.zeros(quant_shape, dtype = torch.int, device = k.device)
            k_scale = torch.zeros(scale_shape, dtype = torch.half, device = k.device)
            v_quant = torch.zeros(quant_shape, dtype = torch.int, device = k.device)
            v_scale = torch.zeros(scale_shape, dtype = torch.half, device = k.device)
            ext.quant_cache_cont(k, k_quant, k_scale)
            ext.quant_cache_cont(v, v_quant, v_scale)
            k_kern = torch.empty_like(k)
            v_kern = torch.empty_like(v)
            ext.dequant_cache_cont(k_quant, k_scale, k_kern)
            ext.dequant_cache_cont(v_quant, v_scale, v_kern)
            test_qkv(f"Kernel {bits} bits", q, k_kern, v_kern, ref_o, ref_scores, False, False, False)

        # Reference
        test_qkv(f"Kernel ref 4 bits",
            q,
            kernel_ref_quant(k, 4),
            kernel_ref_quant(v, 4),
            ref_o,
            ref_scores,
            False, False, False
        )
</content>

<content full_path="science/qgemm_benchmark.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import torch
from exllamav3.ext import exllamav3_ext as ext
from exllamav3.util import Timer
from exllamav3.util.memory import free_mem
from tabulate import tabulate

torch.set_printoptions(precision = 5, sci_mode = False, linewidth = 200)

K = 1
runs = 60

shapes_m = [1, 4, 16]

shapes_kn = [
    (2048, 4096),
    (4096, 14336),
    (14336, 4096),
    (4096, 1024),
    (4096, 4096),
    (4096, 128000),
    (4096, 128256),
    (8192, 28672),
    (28672, 8192),
    (8192, 1024),
    (8192, 8192),
    (8192, 128000),
    (4096, 128256),
]

devices = [
    "cuda:1",
    "cuda:2",
    "cuda:3",
]

kernels = range(1, 1 + ext.exl3_gemm_num_kernel_shapes())
mcg_mult = 0
mul1_mult = 0

@torch.inference_mode()
def main():

    headers = ["(m, k, n)"]
    for idx in kernels:
        headers.append(f"[{idx}]")
    headers.append("D")

    results = []
    for device in devices:
        results.append([])
        for m in shapes_m:
            for (k, n) in shapes_kn:
                assert k % 16 == 0
                assert n % 16 == 0
                results[-1].append([])
                results[-1][-1].append(str((m, k, n)))

                free_mem()

                # Tensors for matmul kernel (not testing correctness)
                proto_a = torch.randn((m, k), dtype = torch.half, device = device)
                proto_b = torch.zeros((k // 16, n // 16, 16 * K), dtype = torch.short, device = device)
                proto_c = torch.zeros((m, n), dtype = torch.half, device = device)
                proto_suh = torch.randn((k,), dtype = torch.half, device = device)
                proto_svh = torch.randn((n,), dtype = torch.half, device = device)

                # Create enough clones to cycle through to prevent L2 cache from skewing results
                assume_cache = 512 * 1024**2
                proto_size = proto_a.numel() * 2 + proto_b.numel() * 2 + proto_c.numel() * 2
                num_buffers = max(assume_cache // proto_size + 1, 2)
                a = [proto_a.clone() for _ in range(num_buffers)]
                b = [proto_b.clone() for _ in range(num_buffers)]
                c = [proto_c.clone() for _ in range(num_buffers)]
                suh = [proto_suh.clone() for _ in range(num_buffers)]
                svh = [proto_svh.clone() for _ in range(num_buffers)]

                # Get preferred kernel for current shape
                pref = ext.exl3_gemm(a[0], b[0], c[0], suh[0], a[0], svh[0], -1, mcg_mult, mul1_mult)

                # Test all kernels
                kresults = []
                for kernel in kernels:
                    print(".", end = "", flush = True)

                    # Test if kernel is compatible
                    compat = ext.exl3_gemm_shape_compat(kernel, m, k, n, K)
                    if not compat:
                        results[-1][-1].append("N/A")
                        kresults.append(1e6)
                        continue

                    # Warmup passes for good measure
                    for i_ in range(10):
                        i = i_ % num_buffers
                        ext.exl3_gemm(a[i], b[i], c[i], suh[i], a[i], svh[i], kernel, mcg_mult, mul1_mult)

                    # Test
                    dummy = c[0][0, 0].item()
                    with Timer() as t:
                        for i_ in range(runs):
                            i = i_ % num_buffers
                            ext.exl3_gemm(a[i], b[i], c[i], suh[i], a[i], svh[i], kernel, mcg_mult, mul1_mult)
                        dummy = c[i][0, 0].item()

                    mean_time_ms = t.interval / runs * 1000
                    kresults.append(mean_time_ms)
                    results[-1][-1].append(f"{mean_time_ms:.5f}")

                # Highlight fastest and preferred kernel, mark shapes where preferred is within 1% of fastest
                b = min(kresults)
                p = 0
                for idx, v in enumerate(kresults):
                    if v == b:
                        results[-1][-1][idx + 1] += " *"
                    if kernels[idx] == pref:
                        results[-1][-1][idx + 1] += " P"
                        p = v
                d = (p - b) / b
                results[-1][-1].append(f"{d:.4f}" if d > 0.01 else "OK")

        print()

    for device, result in zip(devices, results):
        print()
        print(device)
        print()
        print(tabulate(result, headers = headers, tablefmt = "github", floatfmt=".5f"))

if __name__ == "__main__":
    main()

</content>

<content full_path="science/gumbel_eval.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import torch
from exllamav3.ext import exllamav3_ext as ext
from exllamav3 import GumbelSampler, ArgmaxSampler
import random
import matplotlib.pyplot as plt

torch.set_printoptions(precision = 5, sci_mode = False, linewidth = 200)

random.seed(4)
torch.manual_seed(4)

device = "cuda:2"
dim = 128256
shape = (1, 1, dim)
num_samples = 250000

# Create logits tensor
logits = torch.randn(shape, dtype = torch.half, device = device) * 3
logits = logits.contiguous()

# Reference distribution
true_dist = torch.softmax(logits.float(), dim = -1).flatten()

# Ignore probabilities below threshold
min_p = 0.00001

def test_dist(mode: str):

    # Do the stuff
    sampler = GumbelSampler()
    histogram = torch.zeros((dim,), dtype = torch.float, device = device)
    graph = []
    for i in range(num_samples):
        match mode:
            case "mn":
                sample = torch.multinomial(true_dist, num_samples = 1)
            case "g_krn":
                sample = sampler.forward(logits).flatten()
        histogram[sample] += 1
        if (i + 1) % 10000 == 0:
            observed_counts = (histogram / (i + 1)).clamp(min = min_p)
            expected_counts = true_dist.clamp(min = min_p)
            chisq = ((observed_counts - expected_counts).square() / expected_counts).sum().item()
            graph.append((i + 1, chisq))
            print(f"{i + 1} / {num_samples}    chi_squared: {chisq}")

    print("------")
    print(expected_counts)
    print(observed_counts)
    print("------")
    return graph

print("Gumbel")
gumbel = test_dist("g_krn")
print("Softmax + multinomial")
multinomial = test_dist("mn")

gx, gy = zip(*gumbel)
mx, my = zip(*multinomial)
plt.plot(gx, gy, label = "Gumbel", marker = None)
plt.plot(mx, my, label='Softmax + multinomial', marker = None)
plt.xlabel("Samples")
plt.ylabel(f"chi_squared, p > {min_p}")
plt.title(f"Vocab size: {dim}, {num_samples} samples")
plt.legend()
plt.show()
</content>

<content full_path="tests/util.py">
import torch

def assert_close_mr(
        actual: torch.Tensor,
        expected: torch.Tensor,
        *,
        rtol: float = 1e-5,
        atol: float = 1e-8,
        mismatch_ratio: float = 0.0,
        check_device: bool = True,
        check_dtype: bool = True,
        msg: str = None,
):

    # 1) Check shape
    if actual.shape != expected.shape:
        raise AssertionError(
            f"Shape mismatch: {actual.shape} vs {expected.shape}"
        )

    # 2) (Optional) Check device
    if check_device and (actual.device != expected.device):
        raise AssertionError(
            f"Device mismatch: {actual.device} vs {expected.device}"
        )

    # 3) (Optional) Check dtype
    if check_dtype and (actual.dtype != expected.dtype):
        raise AssertionError(
            f"Dtype mismatch: {actual.dtype} vs {expected.dtype}"
        )

    # 4) Compare element-wise closeness
    #    close_mask[i] = True if actual[i] ~ expected[i] within rtol/atol
    close_mask = torch.isclose(actual, expected, rtol = rtol, atol = atol)

    # 5) Compute fraction of elements that are out of tolerance
    total_elements = close_mask.numel()
    mismatch_count = total_elements - close_mask.sum().item()
    fraction_mismatched = mismatch_count / total_elements

    if fraction_mismatched > mismatch_ratio:
        default_msg = (
            f"Too many values are out of tolerance:\n"
            f"  Mismatch ratio = {fraction_mismatched:.6f} "
            f"(allowed <= {mismatch_ratio:.6f})\n"
            f"  Mismatched elements = {mismatch_count} / {total_elements}\n"
            f"  rtol={rtol}, atol={atol}"
        )
        error_msg = f"{msg}\n{default_msg}" if msg else default_msg
        raise AssertionError(error_msg)
</content>

<content full_path="tests/test_qgemm.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import pytest
import torch
from exllamav3 import Config, Model

torch.set_printoptions(precision = 5, sci_mode = False, linewidth = 200)

test_model = "/mnt/str/eval_models/llama3.1-8b-instruct/exl3/3.0bpw/"

test_keys = [
    ("model.layers.0.self_attn.q_proj", "model.layers.0.input_layernorm"),
    ("model.layers.0.self_attn.k_proj", "model.layers.0.input_layernorm"),
    ("model.layers.0.self_attn.v_proj", "model.layers.0.input_layernorm"),
    ("model.layers.0.self_attn.o_proj", "model.layers.0.input_layernorm"),
    ("model.layers.0.mlp.up_proj", "model.layers.0.post_attention_layernorm"),
    ("model.layers.0.mlp.gate_proj", "model.layers.0.post_attention_layernorm"),
    ("model.layers.0.mlp.down_proj", None),
    ("lm_head", "model.norm"),
]

devices = [
    "cuda:2"
]

batch_sizes = [1, 2, 8, 16, 17, 31, 32, 33, 256, 2048]

config = Config.from_directory(test_model)
model = Model.from_config(config)

@pytest.mark.parametrize("device", devices)
@pytest.mark.parametrize("test_key", test_keys)
@pytest.mark.parametrize("batch_size", batch_sizes)
@torch.inference_mode()
def test_qgemm(device, test_key, batch_size):

    if test_key[1]:
        norm = model.find_module(test_key[1])
        norm.load(device = device)

    linear = model.find_module(test_key[0])
    linear.load(device = device)
    
    torch.manual_seed(0)
    x = torch.randn((1, batch_size, linear.in_features), dtype = torch.float16, device = device)

    if test_key[1]:
        x = norm.forward(x, {})
    
    x_qgemm = linear.forward(x, {"reconstruct": False})
    x_hgemm = linear.forward(x, {"reconstruct": True})
    tol = 0.05
    torch.testing.assert_close(x_qgemm, x_hgemm, rtol = tol, atol = tol)

    linear.unload()

    if test_key[1]:
        norm.unload()
</content>

<content full_path="tests/test_kv_quant.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import pytest
import torch
from exllamav3.ext import exllamav3_ext as ext
import random

torch.set_printoptions(precision = 5, sci_mode = False, linewidth = 200)

devices = [
    "cuda:1"
]

page_size = 256
block_table_sizes = [(1,4), (1,8), (3, 4), (8,2)]
head_dims = [128, 64, 96, 32, 256]
num_kv_headss = [8, 2, 1]
cache_sizes = [32768]
bitss = [8]  # Not testing accuracy, so 8-bit only to test the paging logic

@pytest.mark.parametrize("device", devices)
@pytest.mark.parametrize("block_table_size", block_table_sizes)
@pytest.mark.parametrize("head_dim", head_dims)
@pytest.mark.parametrize("num_kv_heads", num_kv_headss)
@pytest.mark.parametrize("cache_size", cache_sizes)
@pytest.mark.parametrize("bits", bitss)
@torch.inference_mode()
def test_kv_quant(device, block_table_size, head_dim, num_kv_heads, cache_size, bits):

    torch.manual_seed(0)

    bsz, pages = block_table_size

    block_table = torch.arange(bsz * pages, dtype = torch.int, device = device).view(bsz, pages)
    cache_seqlens = torch.zeros(size = (bsz,), dtype = torch.int, device = device)

    cache_shape = (cache_size // page_size, page_size, num_kv_heads, head_dim)
    cache_k_tensor = torch.zeros(cache_shape, dtype = torch.half, device = device)
    cache_v_tensor = torch.zeros(cache_shape, dtype = torch.half, device = device)
    cache_k_tensor_out = torch.zeros_like(cache_k_tensor)
    cache_v_tensor_out = torch.zeros_like(cache_v_tensor)

    qcache_shape = (cache_size // page_size, page_size, num_kv_heads * head_dim // 32 * bits)
    qscales_shape = (cache_size // page_size, page_size, num_kv_heads * head_dim // 32)
    cache_k_q = torch.zeros(qcache_shape, dtype = torch.int, device = device)
    cache_v_q = torch.zeros(qcache_shape, dtype = torch.int, device = device)
    cache_k_s = torch.zeros(qscales_shape, dtype = torch.half, device = device)
    cache_v_s = torch.zeros(qscales_shape, dtype = torch.half, device = device)


    def q(length):
        ext.quant_cache_paged(
            cache_k_tensor,
            cache_k_q,
            cache_k_s,
            cache_v_tensor,
            cache_v_q,
            cache_v_s,
            cache_seqlens,
            block_table,
            page_size,
            length
        )

    def dq():
        ext.dequant_cache_paged(
            cache_k_q,
            cache_k_s,
            cache_k_tensor_out,
            cache_v_q,
            cache_v_s,
            cache_v_tensor_out,
            cache_seqlens,
            block_table,
            page_size
        )

    def tq():
        torch.testing.assert_close(cache_k_tensor, cache_k_tensor_out, atol = 0.08, rtol = 0.01)
        torch.testing.assert_close(cache_v_tensor, cache_v_tensor_out, atol = 0.08, rtol = 0.01)

    # Put some stuff in cache
    for i in range(bsz):
        cache_seqlens[i] = i
        for h in range(num_kv_heads):
            cache_k_tensor[block_table[i, 0], i, h, :] = h
            cache_v_tensor[block_table[i, 0], i, h, :] = h + num_kv_heads
    q(1)
    for i in range(bsz):
        cache_seqlens[i] += 1
    dq()
    torch.cuda.synchronize()
    tq()

    # Put more stuff in the cache
    new_cache_seqlens = torch.zeros_like(cache_seqlens)
    random.seed(0)
    for i in range(bsz):
        l = random.randint(10, pages * page_size - 2)
        new_cache_seqlens[i] = l
        for j in range(l):
            m = j % 13
            for h in range(num_kv_heads):
                cache_k_tensor[block_table[i, j // page_size], j % page_size, h, :] = h + m
                cache_v_tensor[block_table[i, j // page_size], j % page_size, h, :] = h + m + num_kv_heads
    cache_seqlens[:] = 0
    q(new_cache_seqlens.amax())
    cache_seqlens.copy_(new_cache_seqlens)
    dq()
    torch.cuda.synchronize()
    tq()

    # Mess up pages
    block_table = block_table.flatten()[torch.randperm(block_table.numel())].view(block_table.shape)
    cache_k_q[:, :, :] = 0
    cache_v_q[:, :, :] = 0
    cache_k_s[:, :, :] = 0
    cache_v_s[:, :, :] = 0
    for i in range(bsz):
        l = new_cache_seqlens[i]
        for j in range(l):
            cache_k_tensor[block_table[i, j // page_size], j % page_size, :, :] += 1
            cache_v_tensor[block_table[i, j // page_size], j % page_size, :, :] += 1
    cache_seqlens[:] = 0
    q(new_cache_seqlens.amax())
    cache_seqlens.copy_(new_cache_seqlens)
    dq()
    torch.cuda.synchronize()
    tq()

    # Update five tokens
    for i in range(bsz):
        l = cache_seqlens[i]
        for j in range(5):
            pos = l + j
            cache_k_tensor[block_table[i, pos // page_size], + pos % page_size, :, :] = 32 + j
            cache_v_tensor[block_table[i, pos // page_size], + pos % page_size, :, :] = 32 + j
    q(5)
    for i in range(bsz):
        cache_seqlens[i] += 5
    dq()
    tq()

    xx = 0

</content>

<content full_path="tests/test_cache_rotate.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import pytest
import torch
from exllamav3.ext import exllamav3_ext as ext
from itertools import pairwise

device = "cuda:2"
page_size = 256

cache_dims = [
    [2048, page_size, 16 * 128],
    [1024, page_size, 8 * 128],
    [512, page_size, 8 * 128],
    [256, page_size, 8 * 128],
    [100, page_size, 4 * 128],
    [32, page_size, 4 * 128],
    [32, page_size, 48],
    [2560, page_size, 16],
]

cache_dtypes = [torch.half, torch.float]

full_opt = [True, False]

@pytest.mark.parametrize("cache_dim", cache_dims)
@pytest.mark.parametrize("cache_dtype", cache_dtypes)
@pytest.mark.parametrize("full", full_opt)
@torch.inference_mode()
def test_rope(cache_dim, cache_dtype, full):

    torch.manual_seed(0)

    num_pages = cache_dim[0]
    cache = torch.randn(cache_dim, device = device, dtype = torch.half)
    order = torch.randperm(num_pages, device = device, dtype = torch.int)
    if not full:
        order = order[:num_pages // 4]

    order = order.repeat_interleave(2)
    m1 = torch.tensor([-1], device = device, dtype = torch.int)
    order = torch.cat([m1, order, m1], dim = -1)
    if not full:
        order = torch.cat([order, order], dim = -1)

    ref_cache = cache.clone()
    ref_order = order.tolist()

    for _ in range(3):
        temp = torch.empty_like(ref_cache[0])
        for i in range(0, len(ref_order), 2):
            a = ref_order[i]
            b = ref_order[i + 1]
            dst = ref_cache[a, ...] if a >= 0 else temp
            src = ref_cache[b, ...] if b >= 0 else temp
            dst.copy_(src)

        temp = torch.empty_like(cache[0])
        ext.cache_rotate(cache, order, temp)

        torch.testing.assert_close(cache, ref_cache, rtol = 0, atol = 0)


</content>

<content full_path="tests/test_ext_norm.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import pytest
import torch
from exllamav3.ext import exllamav3_ext as ext

torch.set_printoptions(precision = 5, sci_mode = False, linewidth = 200)

device = "cuda:0"

def reference_rms_norm(x: torch.Tensor, w: torch.Tensor, eps: float, out_dtype: torch.dtype):
    assert x.dtype in [torch.half, torch.float]
    assert w.dtype in [torch.half]
    x = x.float()
    w = w.float()
    var = (x * x).mean(dim = -1, keepdim = True) + eps
    x = x * w * torch.rsqrt(var)
    x = x.to(out_dtype)
    return x

@pytest.mark.parametrize("batch_size", [1, 4, 16, 384, 1024, 4096])
@pytest.mark.parametrize("dim", [8, 256, 384, 1024, 1536, 8192, 12288])
@pytest.mark.parametrize("in_dtype", [torch.half, torch.float])
@pytest.mark.parametrize("out_dtype", [torch.half, torch.float])
@pytest.mark.parametrize("epsilon", [1e-5, 1e-6])
@torch.inference_mode()
def test_rms_norm(batch_size, dim, in_dtype, out_dtype, epsilon):

    x = torch.randn(batch_size, dim, dtype = in_dtype, device = device)
    w = torch.randn(dim, dtype = torch.half, device = device)
    y = torch.empty_like(x, dtype = out_dtype)

    ref_y = reference_rms_norm(x, w, epsilon, y.dtype)

    ext.rms_norm(x, w, y, epsilon)
    torch.testing.assert_close(y, ref_y, rtol = 1e-3, atol = 1e-3)

    if in_dtype == out_dtype:
        ext.rms_norm(x, w, x, epsilon)
        torch.testing.assert_close(x, y, rtol = 1e-3, atol = 1e-3)

bm_batch = 8192
bm_batch_size = [1, 4, 1024]
bm_dim = [4096, 12288]

@pytest.mark.parametrize("batch_size", bm_batch_size)
@pytest.mark.parametrize("dim", bm_dim)
# @pytest.mark.parametrize("in_dtype", [torch.half, torch.float])
# @pytest.mark.parametrize("out_dtype", [torch.half, torch.float])
@pytest.mark.parametrize("in_dtype", [torch.half])
@pytest.mark.parametrize("out_dtype", [torch.half])
@pytest.mark.benchmark(disable_gc = True, warmup = True)
@torch.inference_mode()
def test_rms_norm_benchmark(benchmark, batch_size, dim, in_dtype, out_dtype):

    x = torch.randn(batch_size, dim, dtype = in_dtype, device = device)
    w = torch.randn(dim, dtype = torch.half, device = device)
    y = torch.empty_like(x, dtype = out_dtype)
    epsilon = 1e-5
    torch.cuda.synchronize()

    def run():
        torch.cuda.synchronize()
        for _ in range(bm_batch // batch_size):
            ext.rms_norm(x, w, y, epsilon)
        torch.cuda.synchronize()

    benchmark(run)


</content>

<content full_path="tests/test_rope.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import pytest
import torch
from exllamav3.ext import exllamav3_ext as ext
from exllamav3.util.rope import RoPE, RopeStyle, RopeSettings
import torch.testing

torch.set_printoptions(precision = 5, sci_mode = False, linewidth = 200)

device = "cuda:2"

# ((bsz, seq_len, num_heads_q, head_dim), (bsz, seq_len, num_heads_k, head_dim))
qk_dims = [
    ((1, 1, 8, 128), (1, 1, 8, 128)),
    ((1, 864, 8, 128), (1, 864, 8, 128)),
    ((1, 864, 128, 128), (1, 864, 8, 128)),
    ((1, 864, 8, 64), (1, 864, 8, 64)),
    ((1, 64, 8, 64), (1, 64, 2, 64)),
    ((1, 10, 80, 256), (1, 10, 10, 256)),
    ((1, 600, 80, 256), (1, 600, 10, 256)),
    ((5, 47, 80, 128), (5, 47, 10, 128)),
    ((17, 1, 32, 256), (17, 1, 10, 256)),
    ((1, 1, 28, 64), (1, 1, 7, 64)),
    ((1, 1, 28, 96), (1, 1, 7, 96)),
    ((1, 1, 28, 80), (1, 1, 7, 80)),
    ((1, 1, 28, 32), (1, 1, 7, 32)),
]

rope_styles = {RopeStyle.GPTJ, RopeStyle.NEOX}
# rope_styles = [RopeStyle.NEOX]
# rope_styles = [RopeStyle.GPTJ]

norm_opt = [False, True]

@pytest.mark.parametrize("qk_dim", qk_dims)
@pytest.mark.parametrize("rope_style", rope_styles)
@pytest.mark.parametrize("use_norm", norm_opt)
@torch.inference_mode()
def test_rope(qk_dim, rope_style, use_norm):

    def qk():
        torch.manual_seed(0)
        q_pr = torch.randn(qk_dim[0], dtype = torch.half, device = device)
        k_pr = torch.randn(qk_dim[1], dtype = torch.half, device = device) if qk_dim[1] else None
        return q_pr, k_pr

    bsz, seq_len, _, head_dim = qk_dim[0]

    rope_layer = RoPE(
        device = device,
        rope_settings = RopeSettings(
            rope_theta = 1.0,
            head_dim = head_dim,
            rope_scaling = None,
            max_position_embeddings = 32768,
            partial_rotary_factor = 1.0,
            rope_style = rope_style,
        )
    )

    def apply_norm(
        x: torch.Tensor,
        w: torch.Tensor,
        eps: float,
        constant_bias: float
    ) -> torch.Tensor:
        dtype = x.dtype
        x = x.float()
        var = x.pow(2).mean(dim = -1, keepdim = True) + eps
        x = x * torch.rsqrt(var)
        x = x.to(dtype)
        x = x * (w + constant_bias)
        return x

    def run(position, positions, position_ids):
        q, k = qk()
        eps = 1e-6
        constant_bias = 0.0
        if use_norm:
            norm_q = torch.randn(head_dim, device = q.device, dtype = torch.half) / 2.0
            norm_k = torch.randn(head_dim, device = k.device, dtype = torch.half) / 2.0
            q = apply_norm(q, norm_q, eps, constant_bias)
            k = apply_norm(k, norm_k, eps, constant_bias)
        else:
            norm_q = None
            norm_k = None
        q_ref, k_ref = rope_layer.apply_torch(q, k, position, positions, position_ids)
        q, k = qk()
        q, k = rope_layer.apply(q, k, position, positions, position_ids, True, norm_q, norm_k, eps, constant_bias)
        torch.testing.assert_close(q, q_ref, rtol = 3e-3, atol = 3e-3)
        if k is not None:
            torch.testing.assert_close(k, k_ref, rtol = 3e-3, atol = 3e-3)

    # No offset
    run(0, None, None)

    # Some offset
    run(19, None, None)

    # Batched offset
    run(0, torch.randint(size = (bsz,), low = 0, high = 49, dtype = torch.int, device = device), None)

    # Batched position ids
    run(0, None, torch.randint(size = (bsz, seq_len), low = 0, high = 117, dtype = torch.int, device = device))
</content>

<content full_path="tests/test_quant_fn.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import pytest
import torch
from exllamav3 import Config, Model
from exllamav3.ext import exllamav3_ext as ext
from exllamav3.modules.quant.exl3_lib.quantize import quantize_tiles
from util import assert_close_mr
import torch.nn.functional as F
import torch.testing
import math

torch.set_printoptions(precision = 5, sci_mode = False, linewidth = 200)

device = "cuda:2"
test_model = "/mnt/str/models/llama3.1-8b-instruct/hf/"
test_keys = [
    "model.layers.0.self_attn.q_proj",
    "model.layers.0.self_attn.k_proj",
    "model.layers.0.self_attn.v_proj",
    "model.layers.0.self_attn.o_proj",
    "model.layers.0.mlp.up_proj",
    "model.layers.0.mlp.gate_proj",
    "model.layers.0.mlp.down_proj",
]

config = Config.from_directory(test_model)
model = Model.from_config(config)

max_mse_per_K = {
    1: 0.3,
    2: 0.1,
    3: 0.1,
    4: 0.1,
    5: 0.1,
    6: 0.07,
    7: 0.05,
    8: 0.04,
}

max_proxy_err_per_K = {
    1: 0.5,
    2: 0.1,
    3: 0.05,
    4: 0.01,
    5: 0.005,
    6: 0.005,
    7: 0.005,
    8: 0.005,
}

w_tol_per_K = {
    1: (0.5, 0.5),
    2: (0.1, 0.1),
    3: (0.08, 0.08),
    4: (0.06, 0.06),
    5: (0.04, 0.04),
    6: (0.03, 0.03),
    7: (0.02, 0.02),
    8: (0.02, 0.02),
}


@pytest.mark.parametrize("cb", [(0, 0, 1.24371088), (0xcaf6a435, 0, 1.24371088), (0, 0xad9a2ec5, 1.0)])
@pytest.mark.parametrize("batch_size", [1, 16, 17, 128])
@pytest.mark.parametrize("K", [1, 2, 3, 4, 5, 6, 7, 8])
@torch.inference_mode()
def test_encode(batch_size, K, cb):

    torch.manual_seed(0)
    mcg, mul1, scale = cb
    in_tile = torch.randn((batch_size, 256), device = device) * scale
    out_tile, out_idx = quantize_tiles(
        in_tile,
        {
            "K": K,
            "mcg_mult": mcg,
            "mul1_mult": mul1,
        }
    )

    # Test tail-biting
    first_col = out_idx[:, 0].to(torch.int32) & 0xFFFF
    last_col = out_idx[:, 255].to(torch.int32) & 0xFFFF
    first_col = first_col >> K
    last_col = last_col & ((1 << (16 - K)) - 1)
    assert torch.equal(first_col, last_col)

    # Test MSE
    mse = F.mse_loss(in_tile / scale, out_tile / scale).item()
    assert mse < max_mse_per_K[K]


@pytest.mark.parametrize("cb", [(0, 0, 1.24371088), (0xcaf6a435, 0, 1.24371088), (0, 0xad9a2ec5, 1.0)])
@pytest.mark.parametrize("batch_size", [1, 64])
@pytest.mark.parametrize("K", [1, 2, 3, 4, 5, 6, 7, 8])
@torch.inference_mode()
def test_encode_ideal(batch_size, K, cb):

    # Create random, valid, tail-biting encoding
    torch.manual_seed(0)
    mcg, mul1, scale = cb
    encoded = torch.randint(low = 0, high = 65535, size = (batch_size, 256), device = device)
    for i in range(256):
        x = encoded[:, i]
        x = x & ((1 << K) - 1)
        for shift in range(1, int(math.ceil(16 / K))):
            j = (i + 256 - shift) % 256
            y = encoded[:, j]
            y = y & ((1 << K) - 1)
            x = x | (y << (K * shift))
        encoded[:, i] = x & 0xffff
    encoded = encoded.to(torch.short)

    # Decode
    decoded = torch.empty_like(encoded, dtype = torch.float)
    ext.decode(encoded, decoded, mcg, mul1)

    # Should quantize with zero loss
    out_tile, out_idx = quantize_tiles(
        decoded,
        {
            "K": K,
            "mcg_mult": mcg,
            "mul1_mult": mul1,
        }
    )
    torch.testing.assert_close(out_tile, decoded, rtol = 1e-6, atol = 1e-6)


@pytest.mark.parametrize("cb", [(0, 0, 1.24371088), (0xcaf6a435, 0, 1.24371088), (0, 0xad9a2ec5, 1.0)])
@pytest.mark.parametrize("K", [1, 2, 3, 4, 5, 6, 7, 8])
@pytest.mark.parametrize("test_key", test_keys)
@torch.inference_mode()
def test_quant_dequant(K, test_key, cb):

    mcg, mul1, scale = cb

    # Grab unquantized linear layer from model
    linear = model.find_module(test_key)
    linear.load(device = device)

    # Forward some random data through the layer to capture Hessian
    bsz = 2048
    torch.manual_seed(0)
    state = torch.randn((1, bsz, linear.in_features), dtype = torch.float16, device = device)
    capture_H = {}
    params = {
        "attn_mode": "flash_attn_nc",
        "capture": capture_H
    }
    rs = linear.prepare_for_device(state, params)
    ref_out = linear.forward(rs, params)

    # Copy the original weight since layer will be quantized in-place
    weight_orig = linear.inner.get_weight_tensor().clone()

    # Quantize the layer
    quant_args = {
        "K": K,
        "seed": 1,
        "apply_out_scales": None,
        "mcg_mult": mcg,
        "mul1_mult": mul1,
        "devices": [device]
    }
    proxy_err, weight_q = linear.convert_exl3(capture_H[linear.qmap], quant_args, return_weight_q = True)
    weight_q = weight_q.half()

    # Test proxy_err
    assert proxy_err < max_proxy_err_per_K[K]

    # Test max absolute weight difference from original, allow for 1% outliers
    rtol, atol = w_tol_per_K[K]
    assert_close_mr(weight_q, weight_orig, rtol = rtol, atol = atol, mismatch_ratio = 0.01)

    # Reconstruct from encoded/packed tensors. Some tolerance needed because the quantizer works in float32
    # while reconstruction reverses the regularization in float16
    weight_recons = linear.inner.get_weight_tensor()
    assert_close_mr(weight_q, weight_recons, rtol = 1e-3, atol = 1e-3, mismatch_ratio = 0.001)

    # Cleanup
    linear.unload()
</content>

<content full_path="tests/generator_stresstest.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from exllamav3 import Config, Model, Cache, Tokenizer, Generator, Job, ArgmaxSampler, CacheLayer_quant
import random

"""
This script creates a generator and runs varying queue depths of completion requests forever, to verify that
the paging and caching logic is sound. Each prompt is an increasing sequence of integers and every completion
is verified as a correct continuation of the sequence.   
"""

# ANSI codes
ESC = "\u001b"
col_default = "\u001b[0m"
col_yellow = "\u001b[33;1m"
col_blue = "\u001b[34;1m"
col_green = "\u001b[32;1m"
col_red = "\u001b[31;1m"
col_gray = "\u001b[37;1m"

model_dir = "/mnt/str/models/llama3.2-1b-instruct/exl3/5.0bpw/"
cache_size = 16384
draft_model_dir = None
prompt_len = (50, 4096)
completion_len = (50, 768)
target_q_depth = (0, 25)
force_depth_0_interval = 3
prefixes = ["All the numbers: ", "It never ends: ", "Counting forever: "]
suffix = ", ".join([str(i) for i in range(prompt_len[1])])
random.seed(0)

if draft_model_dir:
    draft_config = Config.from_directory(draft_model_dir)
    draft_model = Model.from_config(draft_config)
    draft_cache = Cache(draft_model, max_num_tokens = cache_size)
    draft_model.load("cuda:2")
else:
    draft_model, draft_cache = None, None

config = Config.from_directory(model_dir)
model = Model.from_config(config)
cache = Cache(
    model,
    max_num_tokens = cache_size,
    # layer_type = CacheLayer_quant,
    # k_bits = 5,
    # v_bits = 3,
)
model.load("cuda:2")

tokenizer = Tokenizer.from_config(config)

generator = Generator(
    model = model,
    cache = cache,
    draft_model = draft_model,
    draft_cache = draft_cache,
    tokenizer = tokenizer,
    show_visualizer = True,  # Slows down the test but makes it less boring
)

def start_new_job():
    prefix = prefixes[random.randint(0, len(prefixes) - 1)]
    prompt = (prefix + suffix)[:random.randint(prompt_len[0], prompt_len[1])]
    prompt = prompt[:prompt.rfind(",") + 1]
    input_ids = tokenizer.encode(prompt, add_bos = True)
    job = Job(
        input_ids = input_ids,
        max_new_tokens = random.randint(completion_len[0], completion_len[1]),
        sampler = ArgmaxSampler(),
        identifier = prompt
    )
    generator.enqueue(job)


def is_consecutive_integers(s: str) -> bool:
    nums = [int(x.strip()) for x in s.split(',')]
    return all(nums[i + 1] == nums[i] + 1 for i in range(len(nums) - 1))


def iterate():
    num_active = generator.num_active_jobs()
    num_pending = generator.num_pending_jobs()
    results = generator.iterate()
    for result in results:
        if result["eos"]:
            cached_tokens = result["cached_tokens"]
            cached_pages = result["cached_pages"]
            print(
                f"{str(result['job'])}  pending: {num_pending}  active: {num_active}  "
                f"cached_p: {cached_pages}  cached_t: {cached_tokens}  -  ",
                end = ""
            )
            full = result["identifier"] + result["full_completion"]
            full = full[full.find(": ") + 2:]
            full = full[:full.rfind(",")]
            try:
                ok = is_consecutive_integers(full)
            except:
                ok = False

            if ok:
                print("OK!")
            else:
                print("Sus!")
                print("--------")
                pr = result["identifier"]
                print(col_green + pr + col_red + full[len(pr):] + col_default)
                print("--------")

# Main loop
next_target_q_depth = 0
depth_0_interval = force_depth_0_interval
while True:

    # Iterate until target q depth is reached
    if generator.num_remaining_jobs() > next_target_q_depth:
        print(f" - Generating, target depth {next_target_q_depth}")
    while generator.num_remaining_jobs() > next_target_q_depth:
        iterate()

    next_target_q_depth = random.randint(target_q_depth[0] + 1, target_q_depth[1])

    # Start new jobs until target queue depth is achieved
    if generator.num_remaining_jobs() < next_target_q_depth:
        print(f" - Creating jobs, target depth {next_target_q_depth}")
    while generator.num_remaining_jobs() < next_target_q_depth:
        start_new_job()

    # Force the queue to reach zero depth to trigger more defragmentation steps
    depth_0_interval -= 1
    if depth_0_interval == 0:
        next_target_q_depth = 0
        depth_0_interval = force_depth_0_interval
    else:
        next_target_q_depth = random.randint(target_q_depth[0], generator.num_remaining_jobs() - 1)

</content>

<content full_path="tests/test_sampler.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import pytest
import torch
from exllamav3.ext import exllamav3_ext as ext
from exllamav3 import (
    TopKSampler,
    TopPSampler,
)
import torch.testing
import random
from exllamav3.generator.sampler.custom import *

torch.set_printoptions(precision = 5, sci_mode = False, linewidth = 150)

device = "cuda:2"
dims = [
    (1, 16),
    (9, 16),
    (1, 32768),
    (2, 128256),
    (1, 256000),
]

ni = -float("inf")

custom_test_cases = [
    {
        "name": "presfreq_p 1",
        "sampler": CustomSampler([
            SS_PresFreqP(0.5, 0.5),
            SS_Sample_mn()
        ]),
        "input": [[2] * 256000],
        "input_seq": [[0, 1000, 20000, 200000, 1000]],
        "expect_logits": [[1] + [2] * 999 + [0.5] + [2] * 18999 + [1] + [2] * 179999 + [1] + [2] * 55999],
    },
    {
        "name": "presfreq_p 2",
        "sampler": CustomSampler([
            SS_PresFreqP(1, 1),
            SS_Sample_mn()
        ]),
        "input": [[10, 10, 10, 10, 10, 10, 10, 10, 10, 10]],
        "input_seq": [[0, 0, 0, 1, 1, 1, 1, 1, 1, 9]],
        "expect_logits": [[6, 3, 10, 10, 10, 10, 10, 10, 10, 8]],
    },
    {
        "name": "presfreq_p 3",
        "sampler": CustomSampler([
            SS_PresFreqP(1, 0, 4, 4),
            SS_Sample_mn()
        ]),
        "input": [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2]],
        "input_seq": [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],
        "expect_logits": [[2, 2, 2, 1.75, 1.5, 1.25, 1, 1, 1, 1]],
    },
    {
        "name": "rep_p 1",
        "sampler": CustomSampler([
            SS_RepP(2),
            SS_Sample_mn()
        ]),
        "input": [[2] * 256000],
        "input_seq": [[0, 1000, 20000, 200000]],
        "expect_logits": [[1] + [2] * 999 + [1] + [2] * 18999 + [1] + [2] * 179999 + [1] + [2] * 55999],
    },
    {
        "name": "rep_p 2",
        "sampler": CustomSampler([
            SS_RepP(2, 4, 4),
            SS_Sample_mn()
        ]),
        "input": [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2]],
        "input_seq": [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],
        "expect_logits": [[2, 2, 2, 1.75, 1.5, 1.25, 1, 1, 1, 1]],
    },
    {
        "name": "rep_p 3",
        "sampler": CustomSampler([
            SS_RepP(2),
            SS_Sample_mn()
        ]),
        "input": [[2, 2, -2, 2, 2, 2]],
        "input_seq": [[1, 2, 3]],
        "expect_logits": [[2, 1, -4, 1, 2, 2]],
    },
    {
        "name": "temp, top_p, sample",
        "sampler": CustomSampler([
            SS_Temperature(0.75),
            SS_TopP(0.95),
            SS_Sample_mn()
        ]),
        "input": [[5, 3, 2.5, 1, 4, 2, 1.5]],
        "expect_indices": [[0, 4, 1, 2, 5, 6, 3]],
        "expect_probs": [[0.79139, 0.20861, 0, 0, 0, 0, 0]],
    },
    {
        "name": "min_p, sample",
        "sampler": CustomSampler([
            SS_MinP(0.16),
            SS_Sample_mn()
        ]),
        "input": [[3, 3.5, 4, 4.5, 5, 5.5]] * 2,
        "expect_probs": [[0, 0, 0.10154, 0.16741, 0.27600, 0.45505]] * 2,
    },
    {
        "name": "sort, min_p, sample",
        "sampler": CustomSampler([
            SS_Sort(),
            SS_MinP(0.16),
            SS_Sample_mn()
        ]),
        "input": [[3, 3.5, 4, 4.5, 5, 5.5]] * 2,
        "expect_indices": [[5, 4, 3, 2, 1, 0]] * 2,
        "expect_probs": [[0.45505, 0.27600, 0.16741, 0.10154, 0, 0]] * 2,
    },
    {
        "name": "top_k",
        "sampler": CustomSampler([
            SS_TopK(5),
        ]),
        "input": [[3.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9]] * 3,
        "expect_logits": [[3.0, 2.9, 2.8, 2.7, 2.6]] * 3,
        "expect_indices": [[0, 9, 8, 7, 6]] * 3,
    },
]


@pytest.mark.parametrize("case", custom_test_cases)
@torch.inference_mode()
def test_cases(case: dict):
    sampler = case["sampler"]
    inputs = torch.tensor(case["input"], dtype = torch.float, device = device)
    sequence_ids = torch.tensor(case["input_seq"], dtype = torch.long, device = "cpu", pin_memory = True) \
        if "input_seq" in case else None
    state = sampler.forward(
        inputs,
        rand_u32 = 0,
        return_state = True,
        sequence_ids = sequence_ids
    )

    if "expect_probs" in case:
        expect_probs = torch.tensor(case["expect_probs"], dtype = torch.float, device = device)
        test_probs = state.probs[:, :expect_probs.shape[-1]]
        torch.testing.assert_close(test_probs, expect_probs)

    if "expect_indices" in case:
        expect_indices = torch.tensor(case["expect_indices"], dtype = torch.long, device = device)
        test_indices = state.indices[:, :expect_indices.shape[-1]]
        torch.testing.assert_close(test_indices, expect_indices)

    if "expect_logits" in case:
        expect_logits = torch.tensor(case["expect_logits"], dtype = torch.float, device = device)
        test_logits = state.logits[:, :expect_logits.shape[-1]]
        torch.testing.assert_close(test_logits, expect_logits)

    if "expect_sample" in case:
        expect_sample = torch.tensor(case["expect_sample"], dtype = torch.float, device = device)
        torch.testing.assert_close(state.sample, expect_sample)


def compare(histogram, true_dist, min_p = 0.00001):
    observed_counts = histogram.clamp(min = min_p)
    expected_counts = true_dist.clamp(min = min_p)
    chisq = ((observed_counts - expected_counts).square() / expected_counts).sum(dim = -1, keepdim = True)
    # print(f"chi_squared: {chisq}")
    return chisq.max().item()


@pytest.mark.parametrize("dim", dims)
@pytest.mark.parametrize("k", [1, 24, 8, 32, 50])
# @pytest.mark.parametrize("k", [1])
@torch.inference_mode()
def test_topk(dim: tuple, k):
    torch.manual_seed(0)
    random.seed(0)
    temperature = 0.8
    if k > dim[-1]:
        return

    logits = torch.randn(dim, dtype = torch.half, device = device) * 2

    # Reference
    logits_ref = logits.float() / temperature
    probs_ref = torch.softmax(logits_ref, dim = -1)
    topk_values, topk_indices = torch.topk(probs_ref, k, dim = -1)
    mask = torch.zeros_like(probs_ref, dtype = torch.bool)
    mask.scatter_(1, topk_indices, True)
    probs_ref = probs_ref.masked_fill(~mask, 0)
    probs_ref /= probs_ref.sum(dim = -1, keepdim = True)

    sampler = TopKSampler(top_k = k, temperature = temperature)

    num_samples = min(dim[-1] * 200, 10000)
    samples = torch.empty((dim[0], 0), dtype = torch.long, device = device)
    for _ in range(num_samples):
        sample = sampler.forward(logits).unsqueeze(-1)
        samples = torch.cat((samples, sample), dim = -1)

    hb = [torch.bincount(samples[b], minlength = dim[1]) for b in range(dim[0])]
    histogram = torch.stack(hb).float()
    histogram /= num_samples

    chisq = compare(histogram, probs_ref)
    assert chisq < 0.01


@pytest.mark.parametrize("dim", dims)
@pytest.mark.parametrize("p", [0.1, 0.45, 0.50])
@torch.inference_mode()
def test_topp(dim: tuple, p):
    torch.manual_seed(0)
    random.seed(0)
    temperature = 0.6

    logits = torch.randn(dim, dtype = torch.half, device = device) * 2

    # Reference
    logits_ref = logits.float() / temperature
    probs_ref = torch.softmax(logits_ref, dim = -1)
    sorted_values, sorted_indices = torch.sort(probs_ref, descending = True, dim = 1)
    cumsum = sorted_values.cumsum(dim = -1)
    mask = cumsum <= p
    mask[:, 0] = True
    sorted_values *= mask
    probs_ref.scatter_(1, sorted_indices, sorted_values)
    probs_ref /= probs_ref.sum(dim = -1, keepdim = True)

    sampler = TopPSampler(top_p = p, temperature = temperature)

    num_samples = min(dim[-1] * 200, 20000)
    samples = torch.empty((dim[0], 0), dtype = torch.long, device = device)
    for _ in range(num_samples):
        sample = sampler.forward(logits).unsqueeze(-1)
        samples = torch.cat((samples, sample), dim = -1)

    hb = [torch.bincount(samples[b], minlength = dim[1]) for b in range(dim[0])]
    histogram = torch.stack(hb).float()
    histogram /= num_samples

    chisq = compare(histogram, probs_ref)
    assert chisq < 0.02


</content>

<content full_path="examples/chat_console.py">
import sys, shutil

from rich.prompt import Prompt
from rich.live import Live
from rich.markdown import Markdown
from rich.console import Console
from prompt_toolkit import prompt as ptk_prompt
from prompt_toolkit.formatted_text import ANSI

# ANSI codes
ESC = "\u001b"
col_default = "\u001b[0m"
col_user = "\u001b[33;1m"  # Yellow
col_bot = "\u001b[34;1m"  # Blue
col_think1 = "\u001b[35;1m"  # Bright magenta
col_think2 = "\u001b[35m"  # Magenta
col_error = "\u001b[31;1m"  # Bright red
col_info = "\u001b[32;1m"  # Bright red
col_sysprompt = "\u001b[37;1m"  # Grey

def print_error(text):
    print(col_error + "\nError: " + col_default + text)

def print_info(text):
    print(col_info + "\nInfo: " + col_default + text)

def read_input_console(args, user_name, multiline: bool):
    print("\n" + col_user + user_name + ": " + col_default, end = '', flush = True)
    if multiline:
        user_prompt = sys.stdin.read().rstrip()
    else:
        user_prompt = input().strip()
    return user_prompt

def read_input_rich(args, user_name, multiline: bool):
    user_prompt = Prompt.ask("\n" + col_user + user_name + col_default)
    return user_prompt

def read_input_ptk(args, user_name, multiline: bool):
    print()
    user_prompt = ptk_prompt(ANSI(col_user + user_name + col_default + ": "), multiline = multiline)
    return user_prompt

class Streamer_basic:

    def __init__(self, args, bot_name):
        self.all_text = ""
        self.args = args
        self.bot_name = bot_name

    def __enter__(self):
        print()
        print(col_bot + self.bot_name + ": " + col_default, end = "")
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        if not self.all_text.endswith("\n"):
            print()

    def stream(self, text: str, think_tag, end_think_tag):
        if self.all_text or not text.startswith(" "):
            print_text = text
        else:
            print_text = text[1:]
        self.all_text += text
        print(print_text, end = "", flush = True)

class MarkdownConsoleStream:

    def __init__(self, console: Console = None):
        # Make the Rich console a little narrower to prevent overflows from extra-wide emojis
        c, r = shutil.get_terminal_size(fallback = (80, 24))
        c -= 2
        self.console = console or Console(emoji_variant = "text", width = c)
        self.height = r - 2
        self._last_lines = []

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        return False

    def update(self, markdown_text) -> None:
        new_lines = self._render_to_lines(markdown_text)
        old_lines = self._last_lines
        prefix_length = self._common_prefix_length(old_lines, new_lines)
        prefix_length = max(prefix_length, len(old_lines) - self.height, len(new_lines) - self.height)
        old_suffix_len = len(old_lines) - prefix_length
        new_suffix_len = len(new_lines) - prefix_length
        if old_suffix_len > 0:
            print(f"{ESC}[{old_suffix_len}A", end = "")
        changed_count = max(old_suffix_len, new_suffix_len)
        for i in range(changed_count):
            if i < new_suffix_len:
                print(f"{ESC}[2K", end = "")  # Clear entire line
                print(new_lines[prefix_length + i].rstrip())
            else:
                print(f"{ESC}[2K", end = "")
                # print()
        self._last_lines = new_lines

    def _render_to_lines(self, markdown_text: str):
        # Capture Rich’s output to a string, then split by lines.
        with self.console.capture() as cap:
            self.console.print(Markdown(markdown_text))
        rendered = cap.get()
        split = []
        for s in [r.rstrip() for r in rendered.rstrip("\n").split("\n")]:
            if s or len(split) == 0 or split[-1]:
                split.append(s)
        return split

    @staticmethod
    def _common_prefix_length(a, b) -> int:
        i = 0
        for x, y in zip(a, b):
            if x != y:
                break
            i += 1
        return i

class Streamer_rich:
    def __init__(self, args, bot_name):
        self.all_text = ""
        self.think_text = ""
        self.bot_name = bot_name
        self.all_print_text = col_bot + self.bot_name + col_default + ": "
        self.args = args
        self.live = None
        self.is_live = False

    def begin(self):
        self.live = MarkdownConsoleStream()
        self.live.__enter__()
        self.live.update(self.all_print_text)
        self.is_live = True

    def __enter__(self):
        if self.args.think:
            print()
            print(col_think1 + "Thinking" + col_default + ": " + col_think2, end = "")
        else:
            print()
            self.begin()
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        if self.is_live:
            self.live.__exit__(exc_type, exc_value, traceback)

    def stream(self, text: str, think_tag: str, end_think_tag: str):
        if self.args.think and not self.is_live:
            print_text = text
            if not self.think_text:
                print_text = print_text.lstrip()
            self.think_text += print_text
            if end_think_tag in self.think_text:
                print(print_text.rstrip(), flush = True)
                print()
                self.begin()
            else:
                print(print_text, end = "", flush = True)

        else:
            print_text = text
            if not self.all_text.strip():
                print_text = print_text.lstrip()
                if print_text.startswith("```"):
                    print_text = "\n" + print_text
            self.all_text += text
            self.all_print_text += print_text
            formatted_text = self.all_print_text
            formatted_text = formatted_text.replace(think_tag, f"`{think_tag}`")
            formatted_text = formatted_text.replace(end_think_tag, f"`{end_think_tag}`")
            self.live.update(formatted_text)
</content>

<content full_path="examples/loading.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from exllamav3 import Config, Model, Cache, Tokenizer

# Define model by loading the config from the model directory
config = Config.from_directory("/mnt/str/models/llama3.1-8b-instruct/exl3/4.0bpw/")

# Create and load tokenizer
tokenizer = Tokenizer.from_config(config)

# Create the model instance. Model isn't loaded until one of the load functions is called
model = Model.from_config(config)

# Load model without cache (then unload)
print("Loading")
model.load()
model.unload()

# Load model with progress bar
print("Loading with progress bar")
model.load(progressbar = True)
model.unload()

# Create a cache attached to the model. When the model is loaded, the cache tensors are also created
cache = Cache(model, max_num_tokens = 2048)

# Load again, this time with a cache
print("Loading with cache")
model.load(progressbar = True)
model.unload()

# Unloading the model also destroys the tensors of any attached cache(s), freeing up all the VRAM used by both.
# The cache is still attached to the model at this point, so loading the same model again would create the
# cache tensors anew. If desired, we can explicitly detach the cache to prevent this:
cache.detach_from_model(model)

# Load with callback function
def progress_callback(module: int, modules: int):
    print(f"Callback: Loaded {module} of {modules} modules")

print("Loading with callback")
model.load(callback = progress_callback)
model.unload()

# Load model using generator function. In this mode, the loader presents as an iterator yielding the current
# progress on each iteration
print("Loading with generator function")
f = model.load_gen()
for module, modules in f:
    print(f"Generator: Loaded {module} of {modules} modules")
model.unload()

# Load model using generator function, callback and progress bar
print("Loading with generator function, callback and progress bar")
f = model.load_gen(progressbar = True, callback = progress_callback)
for module, modules in f:
    print(f"Generator: Loaded {module} of {modules} modules")
model.unload()
</content>

<content full_path="examples/batched_translation.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import argparse
from transformers import AutoTokenizer
from exllamav3.util.progress import ProgressBar
from exllamav3.util.file import disk_lru_cache, disk_lru_cache_clear
from exllamav3 import Config, Model, Cache, Tokenizer, model_init, Generator, Job
from datasets import load_dataset
import torch
import time

# ANSI codes
ESC = "\u001b"
col_default = "\u001b[0m"
col_yellow = "\u001b[33;1m"
col_blue = "\u001b[34;1m"
col_green = "\u001b[32;1m"
col_red = "\u001b[31;1m"
col_gray = "\u001b[37;1m"

@disk_lru_cache("get_dataset")
def get_dataset(path: str, name: str, split: str, key: str, min_length: int):
    data = load_dataset(path, name, split = split)
    r = []
    for d in data:
        ds = d[key].strip()
        if len(ds) >= min_length:
            r.append(ds)
    return r

def format_request(hf_tokenizer, eos_tokens, text, max_new_tokens, idx):
    instruction = (
        f"Translate the the text between the #BEGIN# and #END# tags into zoomer slang. "
        f"Reply only with the translation enclosed in the same tags:\n\n"
        f"#BEGIN#\n"
        f"{text}\n"
        f"#END#\n"
        f" /no_think"
    )
    chat = [{
        "role": "user",
        "content": instruction
    }]
    input_ids = hf_tokenizer.apply_chat_template(chat, add_generation_prompt = True)
    input_ids = torch.tensor(input_ids, dtype = torch.long).unsqueeze(0)
    job = Job(
        input_ids = input_ids,
        max_new_tokens = max_new_tokens,
        stop_conditions = eos_tokens,
        identifier = idx
    )
    return job

@torch.inference_mode()
def main(args):

    # Load dataset as list
    print(f" -- Loading dataset...")
    in_data = get_dataset(args.dataset_path, args.dataset_name, args.dataset_split, args.dataset_key, 125)
    avg_len = sum([len(d) for d in in_data]) / len(in_data)
    print(f" -- Loaded {len(in_data)} items, avg. item length {avg_len:.2f} chars")

    # Load model
    model, config, cache, tokenizer = model_init.init(args)
    generator = Generator(model, cache, tokenizer, show_visualizer = args.visualize_cache)
    bpw_layer, bpw_head, vram_bits = model.get_storage_info()
    print(f" -- Model: {args.model_dir}")
    print(f" -- Bitrate: {bpw_layer:.2f} bpw / {bpw_head:.2f} bpw (head)")

    # Use HF tokenizer for prompt formatting
    print(f" -- Loading HF tokenizer...")
    hf_tokenizer = AutoTokenizer.from_pretrained(args.model_dir)

    # Create jobs
    completions = []
    with ProgressBar(" -- Creating jobs", len(in_data)) as pb:
        for idx, text in enumerate(in_data):
            job = format_request(hf_tokenizer, config.eos_token_id_list, text, args.max_reply, idx)
            generator.enqueue(job)
            completions.append("")
            pb.update(idx)

    # Inference
    num_completions = 0
    num_tokens = 0
    time_begin = time.time()
    while generator.num_remaining_jobs():
        results = generator.iterate()

        # We'll always get at least one result for each active job, even if the result contains no output text
        bsz = len(set([r["identifier"] for r in results]))
        num_tokens += bsz

        for result in results:
            if not result["eos"]: continue

            # EOS signal is always accompanied by the full completion, so we don't need to collect text chunks
            idx = result["identifier"]
            response = result["full_completion"]
            completions[idx] += response

            # Measure performance
            num_completions += 1
            elapsed_time = time.time() - time_begin
            rpm = num_completions / (elapsed_time / 60)
            tps = num_tokens / elapsed_time
            print()
            print(f"{col_blue}---------------------------------------------------------------------------{col_default}")
            print(f"{col_blue}Current batch size: {col_yellow}{bsz}{col_default}")
            print(f"{col_blue}Avg. completions/minute: {col_yellow}{rpm:.2f}{col_default}")
            print(f"{col_blue}Avg. output tokens/second: {col_yellow}{tps:.2f}{col_default}")
            print(f"{col_blue}---------------------------------------------------------------------------{col_default}")
            print()

            # Spam completions to the console
            print(f"{col_green}Input {idx}:{col_default}")
            print()
            print(in_data[idx])
            print()
            print(f"{col_green}Completion {idx}:{col_default}")
            print()
            print(completions[idx])


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    model_init.add_args(parser, default_cache_size = 65536)
    parser.add_argument("-vis", "--visualize_cache", action = "store_true", help = "Show cache visualizer (slow)")
    parser.add_argument("-dsp", "--dataset_path", type = str, default = "wikitext", help = "Dataset path")
    parser.add_argument("-dsn", "--dataset_name", type = str, default = "wikitext-2-raw-v1", help = "Dataset name")
    parser.add_argument("-dss", "--dataset_split", type = str, default = "test", help = "Dataset split")
    parser.add_argument("-dsk", "--dataset_key", type = str, default = "text", help = "Dataset key to extract")
    parser.add_argument("-maxr", "--max_reply", type = int, default = 1024, help = "Max length of each reply")

    _args = parser.parse_args()
    main(_args)

</content>

<content full_path="examples/banned_strings.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from exllamav3 import Model, Config, Cache, Tokenizer, Generator, Job, GreedySampler
from exllamav3.util import Timer
from common import format_prompt, get_stop_conditions

"""
A simple showcase of the banned strings feature of the generator, which prevents the model from sampling any of a
predefined set of phrases.   
"""

# Initialize model, tokenizer etc.
config = Config.from_directory("/mnt/str/eval_models/llama3.1-8b-instruct/exl3/4.0bpw/")
model = Model.from_config(config)
cache = Cache(model, max_num_tokens = 8192)
model.load()
tokenizer = Tokenizer.from_config(config)
generator = Generator(model = model, cache = cache, tokenizer = tokenizer)

# Prompt
prompt_format = "llama3"
prompt = format_prompt(
    prompt_format,
    "You are an amoral AI assistant.",
    "Teach me how to pick locks."
)
stop_conditions = get_stop_conditions(prompt_format, tokenizer)

# List of some common refusals
banned_strings = [
    "I cannot provide",
    "I can't provide",
    "I can't help with",
    "I cannot assist",
    "I can't assist",
    "I won't engage",
    "I won't provide",
    "I'm not able to",
    "However, please note that",
    "It's important to note that",
    "It is important to note",
    ", but please keep in mind",
    ", but please note that",
    "Please note that",
    "Keep in mind that",
    "encourage or facilitate harmful",
    "I must emphasize",
    "However, I must",
    "I would like to emphasize",
    "Instead of providing",
    "Instead of pursuing",
    "it's essential to remember",
    "Instead, I'd like to suggest",
    "but I want to emphasize",
    "I want to emphasize",
    "I'm not condoning or encouraging",
    "I'm not encouraging or condoning",
    "I do not encourage or condone",
    "I do not condone or encourage",
    "But please,",
    ", I must remind you"
    "I must remind you"
]

# Generate with and without banned strings
def generate(bs):

    input_ids = tokenizer.encode(prompt, add_bos = False, encode_special_tokens = True)
    job = Job(
        input_ids = input_ids,
        sampler = GreedySampler(),
        min_new_tokens = 100 if bs else 0,  # Prevent model from ending stream too early
        max_new_tokens = 300,
        banned_strings = bs,
        stop_conditions = stop_conditions
    )
    generator.enqueue(job)

    # Stream output to console. Banned strings will not be included in the output stream, but every time a string
    # is suppressed the offending text is returned in the results packet, so we can illustrate what's going on
    col_banned = "\u001b[9m\u001b[31;1m"  # Magenta, strikethrough
    col_default = "\u001b[0m"

    while generator.num_remaining_jobs():
        results = generator.iterate()
        for result in results:
            if "text" in result:
                print(result["text"], end = "", flush = True)
            if "suppressed_text" in result:
                print(col_banned + result["suppressed_text"] + col_default, end = "", flush = True)
    print()

print("--------------------------------------------------------------------------------------")
print("Without banned strings")
print("--------------------------------------------------------------------------------------")

generate(bs = None)
print()

print("--------------------------------------------------------------------------------------")
print("With banned strings")
print("--------------------------------------------------------------------------------------")

generate(bs = banned_strings)
print()

</content>

<content full_path="examples/dynamic_gen.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from exllamav3 import Model, Config, Cache, Tokenizer, Generator, Job, Sampler
from exllamav3.util import Timer
from blessed import Terminal
from common import format_prompt, get_stop_conditions
import pprint

"""
This is a demo and small showcase some of the features of the dynamic batching generator

Display modes for this demo:
1: One line per job, updated continuously
2: Print completions as jobs finish
3: Step over output iteration by iteration
4: Space heater mode (no output)
"""
display_mode = 1

# Show graphical visualization of the paged cache (adds some overhead)
show_visualization = False

# Where to find our model
model_dir = "/mnt/str/eval_models/llama3.1-8b-instruct/exl3/4.0bpw/"

# Total number of tokens to allocate space for in the cache.
total_context = 16384

# Max number of batches to run at once, assuming the sequences will fit within total_context.
max_batch_size = 16

# Max chunk size. Determines the size of prefill operations. Can be reduced to reduce pauses whenever a
# new job is started, but at the expense of overall prompt ingestion speed.
max_chunk_size = 2048

# Max new tokens per completion. For this example applies to all jobs.
max_new_tokens = 500

# Some prompts to feed the generator
prompt_format = "llama3"
system_prompt = "You are an AI assistant"
prompts = [
    "What is 2+2 and why?",
    "Can you guess the next number in this sequence: " + ", ".join(str(n) for n in range(500)),
    "Can you guess the next number in this sequence: " + ", ".join(str(n) for n in range(400)),
    "Can you guess the next number in this sequence: " + ", ".join(str(n) for n in range(200)),
    "Can you write a C++ quicksort implementation pretty please?",
    "Hello!",
    "What's the difference smoke and vapor?",
    "What seems out of place in this sequence: " + ", ".join(str(n if n != 123 else 69) for n in range(200)),
    "What seems out of place in this sequence: " + ", ".join(str(n if n != 42 else 111) for n in range(200)),
    "What seems out of place in this sequence: " + ", ".join(str(n if n != 42 else 111) for n in range(200)),
    "What seems out of place in this sequence: " + ", ".join(str(n if n != 42 else 111) for n in range(200)),
    "What seems out of place in this sequence: " + ", ".join(str(n if n != 42 else 111) for n in range(200)),
    "Please guess the next 20 numbers in this sequence: " + ", ".join(str(n) for n in range(700)),
    "Write a short essay about cell membranes.",
    "How do I open a can of beans?",
    "How do I open a can of soup?",
    "How do I open a can of strawberry jam?",
    "How do I open a can of raspberry jam?",
    "What's the tallest building in Paris?",
    "What's the most populous nation on Earth?",
    "What's the most populous nation on Mars?",
    "What do the Mole People actually want and how can we best appease them?",
    "Why is the sky blue?",
    "Where is Waldo?",
    "Who is Waldo?",
    "Why is Waldo?",
    "Is it legal to base jump off the Eiffel Tower?",
    "Is it legal to base jump into a volcano?",
    "Why are cats better than dogs?",
    "Why is the Hulk so angry all the time?",
    "How do I build a time machine?",
    "What seems out of place in this sequence: " + ", ".join(str(n if n != 123 else 69) for n in range(200)),
    "Is it legal to grow your own catnip?",
    "What seems out of place in this sequence: " + ", ".join(str(n if n != 360 else 420) for n in range(400)),
    "What seems out of place in this sequence: " + ", ".join(str(n if n != 361 else 421) for n in range(400)),
    "What's inside a black hole?",
    "What seems out of place in this sequence: " + ", ".join(str(n if n != 360 else 420) for n in range(400)),
    "What seems out of place in this sequence: " + ", ".join(str(n if n != 363 else 421) for n in range(400)),
    "What do the numbers 2, 4, 8, 16, 32 and 64 have in common?",
    "What do the numbers 2, 3, 5, 7, 11 and 13 have in common?",
    "Is there life on Mars?",
    "Why are cats better than dogs?",
    "Write a parable about why cats are better than dogs.",
    "Can you guess the next number in this sequence: " + ", ".join(str(n) for n in range(999)),
    "Can you guess the next number in this sequence: " + ", ".join(str(n) for n in range(999)),
    "Can you guess the next number in this sequence: " + ", ".join(str(n) for n in range(999)),
    "Can you guess the next number in this sequence: " + ", ".join(str(n) for n in range(999)),
    "Can you guess the next number in this sequence: " + ", ".join(str(n) for n in range(999)),
]

term = Terminal()

def main():

    # Load the model config
    config = Config.from_directory("/mnt/str/models/llama3.1-8b-instruct/exl3/4.0bpw/")

    # Create the model from the config
    model = Model.from_config(config)

    # Create the cache before loading the model, so cache tensors are accounted for in the split
    cache = Cache(model, max_num_tokens = total_context)

    # Finally load the model. The default mode is autosplit.
    model.load()

    # Load the tokenizer
    print("Loading tokenizer...")
    tokenizer = Tokenizer.from_config(config)

    # Initialize the generator
    generator = Generator(
        model = model,
        cache = cache,
        tokenizer = tokenizer,
        max_batch_size = max_batch_size,
        max_chunk_size = max_chunk_size,
        show_visualizer = show_visualization
    )

    # Create jobs
    jobs = []
    for prompt in prompts:
        fprompt =  format_prompt(prompt_format, system_prompt, prompt)
        input_ids = tokenizer.encode(fprompt, encode_special_tokens = True)
        job = Job(
            input_ids = input_ids,
            max_new_tokens = max_new_tokens,
            stop_conditions = get_stop_conditions(prompt_format, tokenizer)
        )
        jobs.append(job)

    # Enqueue all the jobs at once
    generator.enqueue(jobs)

    # Go
    match display_mode:

        # Mode 1
        case 1:
            class JobStatusDisplay:
                def __init__(self, job, console_line):
                    self.console_line = console_line
                    self.job = job
                    self.prefill = 0
                    self.max_prefill = 0
                    self.collected_output = ""
                    self.tokens = 0
                    self.spaces = " " * 80
                    text = term.darkgray(f"{self.console_line:3}:")
                    text += term.blue("enqueued")
                    print(term.move_xy(0, self.console_line) + text)

                def update(self, r):
                    stage = r["stage"]
                    stage = r.get("eos_reason", stage)
                    self.collected_output += r.get("text", "").replace("\n", "\\n")
                    token_ids = r.get("token_ids", None)
                    if token_ids is not None: self.tokens += token_ids.shape[-1]
                    self.prefill = r.get("curr_progress", self.prefill)
                    self.max_prefill = r.get("max_progress", self.max_prefill)
                    text = term.darkgray(f"{self.console_line:3}:")
                    text += term.blue(f"{stage:16}")
                    text += "prefill [ " + term.yellow(f"{self.prefill: 5} / {self.max_prefill: 5}") + " ]"
                    text += "   "
                    text += term.green(f"{self.tokens: 5} t")
                    text += term.darkgray(" -> ")
                    text += (self.spaces + self.collected_output)[-80:].replace("\t", " ")
                    if "accepted_draft_tokens" in r:
                        acc = r["accepted_draft_tokens"]
                        rej = r["rejected_draft_tokens"]
                        eff = acc / (acc + rej) * 100.0
                        text += term.bright_magenta(f"   SD eff.: {eff:6.2f}%")
                    print(term.move_xy(0, self.console_line) + text)

            print(term.enter_fullscreen())
            displays = { job: JobStatusDisplay(job, line) for line, job in enumerate(jobs) }
            while generator.num_remaining_jobs():
                results = generator.iterate()
                for r in results:
                    job = r["job"]
                    displays[job].update(r)
            print(term.move_xy(0, len(displays) + 1) + "Press any key to continue...")
            with term.cbreak():
                term.inkey()

        # Mode 2
        case 2:
            total_tokens = 0
            total_time = 0
            while generator.num_remaining_jobs():
                with Timer() as t:
                    results = generator.iterate()
                total_time += t.interval
                for r in results:
                    if r["stage"] == "streaming" and not r["eos"]:
                        total_tokens += r["token_ids"].shape[-1]
                for r in results:
                    if r["stage"] == "streaming" and r["eos"]:
                        job = r["job"]
                        in_prompt = \
                        tokenizer.decode(job.sequences[0].input_ids.torch(), decode_special_tokens = True)[0]
                        print("\n")
                        print(term.darkgray("Input: "))
                        print(term.yellow(in_prompt))
                        print()
                        print(term.darkgray("Output:"))
                        print(r["full_completion"])
                        print()
                        print(term.darkgray("New tokens:        ") + term.green(f"{r['new_tokens']:9} t"))
                        print(term.darkgray("Cached tokens:     ") + term.green(
                            f"{r['cached_tokens']:7} t / {r['prompt_tokens']:7} t"))
                        print(term.darkgray("Enqueued:          ") + term.blue(f"{r['time_enqueued']:9.2f} s"))
                        print(term.darkgray("Prefill:           ") + term.blue(f"{r['time_prefill']:9.2f} s"))
                        print(term.darkgray("Generation:        ") + term.blue(f"{r['new_tokens']:9.2f} s"))
                        speed_input = r['prompt_tokens'] / (r['time_prefill'] + 1e-10)
                        speed_output = r['new_tokens'] / (r['time_generate'] + 1e-10)
                        speed_total = total_tokens / total_time
                        print(term.darkgray("Job input          ") + term.cyan(f"{speed_input:9.2f} t/s"))
                        print(term.darkgray("Job output         ") + term.cyan(f"{speed_output:9.2f} t/s"))
                        print(term.darkgray("Overall output     ") + term.cyan(f"{speed_total:9.2f} t/s"))
                        if "accepted_draft_tokens" in r:
                            acc = r["accepted_draft_tokens"]
                            rej = r["rejected_draft_tokens"]
                            eff = acc / (acc + rej) * 100.0
                            print(term.darkgray("SD efficiency:     ") + term.bright_magenta(f"{eff:9.2f}%"))

        # Mode 3
        case 3:
            while generator.num_remaining_jobs():
                results = generator.iterate()
                print()
                pprint.pprint(results, indent = 4)
                print()
                print("Press any key to continue...")
                with term.cbreak():
                    term.inkey()

        case 4:
            while generator.num_remaining_jobs():
                generator.iterate()


if __name__ == "__main__":
    try:
        main()
    finally:
        pass
        if display_mode == 1:
            print(term.exit_fullscreen())
</content>

<content full_path="examples/async_generator.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from exllamav3 import Model, Config, Cache, Tokenizer, AsyncGenerator, AsyncJob, Sampler
import asyncio

"""
The async generator is a wrapper class that allows you to treat generator jobs as asynchronous iterators, while
still letting concurrent jobs benefit from batching. Here is a simple example using asyncio.gather to launch a
batch of async tasks at once.
"""

async def main():

    # Load model etc.
    config = Config.from_directory("/mnt/str/eval_models/llama3.1-8b-instruct/exl3/4.0bpw/")
    model = Model.from_config(config)
    cache = Cache(model, max_num_tokens = 32768)
    model.load()
    tokenizer = Tokenizer.from_config(config)

    # Initialize the async generator with default settings
    generator = AsyncGenerator(
        model = model,
        cache = cache,
        tokenizer = tokenizer,
    )

    # Define a couple of prompts
    prompts = [
        "Once upon a time, there was",
        "asyncio in Python is a great feature because",
        "asyncio in Python is a pain to work with because",
    ]

    # Async task running async job in the async generator
    async def run_job(prompt: str, marker: str):

        # Create an asynchronous job. The job presents as an iterator which is transparently batched with other
        # concurrent jobs for the same generator.
        job = AsyncJob(
            generator,
            input_ids = tokenizer.encode(prompt, add_bos = False),
            max_new_tokens = 200
        )

        # Iterate over the job. Each returned result is a dictionary containing an update on the status of the
        # job and/or part of the completion (see the definition of Job.iterate() for details). The iterator ends
        # when the job is complete (i.e. EOS or max_new_tokens is reached)
        full_completion = prompt
        async for result in job:
            # We'll only collect text here, but the result could contain other updates
            full_completion += result.get("text", "")

            # Output marker to console to confirm that tasks running asynchronously, and that job 0 stops running
            # after 300 characters (note, not tokens)
            print(marker, end = "", flush = True)

            # Cancel the second job after 300 characters to make the control flow less trivial. We have to explicitly
            # cancel the job, otherwise the generator will continue to run the job in the background, waiting for some
            # task to finish iterating through the results
            if marker == "1" and len(full_completion) > 300:
                full_completion += " [job canceled]"
                await job.cancel()
                break
        else:
            full_completion += " [max_new_tokens reached]"

        return full_completion

    # Run a batch of async jobs
    tasks = [run_job(prompt, str(i)) for i, prompt in enumerate(prompts)]
    outputs = await asyncio.gather(*tasks)

    # Print the results
    print()
    print()
    for i, output in enumerate(outputs):
        print(f"Output {i}")
        print("-----------")
        print(output)
        print()

    await generator.close()

if __name__ == "__main__":
    asyncio.run(main())




</content>

<content full_path="examples/generator.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from exllamav3 import Config, Model, Cache, Tokenizer, Generator, Job, TopPSampler
from common import format_prompt, get_stop_conditions

"""
A couple of examples showing uses of the generator
"""

prompt_format = "llama3"  # see common.py
model_dir = "/mnt/str/eval_models/llama3.1-8b-instruct/exl3/4.0bpw/"
cache_size = 16384

system_prompt = "You are a very nice language model."

instructions = [
    "Write a short story beginning with the words 'Once in a while, when you least expect it'.",
    "Why are cats so awesome?",
    "Who was the tallest president of the United States?",
    "Why are there so many different kinds of screws?",
    "oinnvdoehwemnascnawwui8dh2",
    "Write a haiku about catnip."
]

# Generate a single completion to a single prompt
def generate_single(generator, tokenizer):
    instruction = instructions[0]
    print("------------------")
    print("Prompt: " + instruction)
    print()
    response = generator.generate(
        prompt = format_prompt(prompt_format, system_prompt, instruction),
        stop_conditions = get_stop_conditions(prompt_format, tokenizer),
        max_new_tokens = 500,
        completion_only = True,
        add_bos = True
    )
    print("Response: " + response)
    print()


# Generate multiple batched completions
def generate_batched(generator, tokenizer):
    print("------------------")
    responses = generator.generate(
        prompt = [format_prompt(prompt_format, system_prompt, instruction) for instruction in instructions],
        stop_conditions = get_stop_conditions(prompt_format, tokenizer),
        max_new_tokens = 100,
        completion_only = True,
        add_bos = True
    )
    for idx, response in enumerate(responses):
        print(f"#{idx + 1}: {response}")
        print("------------------")


# Create a job and generate a stream of tokens
def generate_streaming(generator, tokenizer):
    instruction = instructions[0]
    print("------------------")
    print("Prompt: " + instruction)
    print()
    print("Response: ", end = "", flush = True)

    # Create the job and enqueue it
    formatted_prompt = format_prompt(prompt_format, system_prompt, instruction)
    job = Job(
        input_ids = tokenizer.encode(formatted_prompt, add_bos = True),
        max_new_tokens = 400,
        stop_conditions = get_stop_conditions(prompt_format, tokenizer),
    )
    generator.enqueue(job)

    # Keep iterating until the generator has no more jobs
    while generator.num_remaining_jobs():
        results = generator.iterate()

        # Each iteration returns a list of results, each of which may contain output tokens for a running job. We
        # only care about the "text" field here.
        for result in results:
            text = result.get("text", "")
            print(text, end = "", flush = True)

    print()


# Create a batch of jobs and stream the results
def generate_streaming_batched(generator, tokenizer):

    # Some buffers for collecting results
    responses = [""] * len(instructions)

    for idx, instruction in enumerate(instructions):

        # Only print the second job to the console
        if idx == 1:
            print("------------------")
            print("Prompt: " + instruction)
            print()
            print("Streamed response: ", end = "", flush = True)

        # Create each job and enqueue it. Since one iteration of the generator can return multiple results, adding
        # an identifier argument lets us track which sequence each chunk of output pertains to. The identifier can
        # be any object, but a simple index will work here
        formatted_prompt = format_prompt(prompt_format, system_prompt, instruction)
        job = Job(
            input_ids = tokenizer.encode(formatted_prompt, add_bos = True),
            max_new_tokens = 400,
            stop_conditions = get_stop_conditions(prompt_format, tokenizer),
            identifier = idx,
        )
        generator.enqueue(job)

    # Keep iterating until the generator has no more jobs
    while generator.num_remaining_jobs():
        results = generator.iterate()

        for result in results:
            text = result.get("text", "")
            idx = result["identifier"]

            # If this result is from the first job, stream to the console
            if idx == 1:
                print(text, end = "", flush = True)

            # Collect results
            responses[idx] += text

    print()
    print("--------------")

    # Finally print all the collected results
    for idx, response in enumerate(responses):
        print(f"#{idx + 1}: {response}")
        print("------------------")


# Generate a series of completions with increasing temperature
def generate_temperature(generator, tokenizer):
    instruction = instructions[5]
    print("------------------")
    print("Prompt: " + instruction)
    print()
    temperature = 0.0
    while temperature <= 3.01:
        print(f"Temperature = {temperature:.2f}: ", end = "", flush = True)
        response = generator.generate(
            prompt = format_prompt(prompt_format, system_prompt, instruction),
            stop_conditions = get_stop_conditions(prompt_format, tokenizer),
            sampler = TopPSampler(temperature = temperature, top_p = 0.95, temperature_last = True),
            max_new_tokens = 100,
            completion_only = True,
            add_bos = True
        )
        print(response)
        print()
        temperature += 0.25


def main():

    # Load a model with cache
    config = Config.from_directory(model_dir)
    model = Model.from_config(config)
    cache = Cache(model, max_num_tokens = cache_size)
    model.load(progressbar = True)
    tokenizer = Tokenizer.from_config(config)

    # Create generator
    generator = Generator(
        model = model,
        cache = cache,
        tokenizer = tokenizer,
    )

    # Do some things
    generate_single(generator, tokenizer)
    generate_batched(generator, tokenizer)
    generate_streaming(generator, tokenizer)
    generate_streaming_batched(generator, tokenizer)
    generate_temperature(generator, tokenizer)


if __name__ == "__main__":
    main()


</content>

<content full_path="examples/chat.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import argparse
from exllamav3 import Generator, Job, model_init
from exllamav3.generator.sampler import ComboSampler
from chat_templates import *
from chat_util import *
import torch
from chat_console import *

@torch.inference_mode()
def main(args):

    # Prompt format
    if args.modes:
        print("Available modes:")
        for k, v in prompt_formats.items():
            print(f" - {k:16} {v.description}")
        return

    user_name = args.user_name
    bot_name = args.bot_name
    prompt_format = prompt_formats[args.mode](user_name, bot_name)
    system_prompt = prompt_format.default_system_prompt() if not args.system_prompt else args.system_prompt
    add_bos = prompt_format.add_bos()
    max_response_tokens = args.max_response_tokens
    multiline = args.multiline

    if args.basic_console:
        read_input_fn = read_input_ptk
        streamer_cm = Streamer_basic
    else:
        read_input_fn = read_input_ptk
        streamer_cm = Streamer_rich

    # Load model
    model, config, cache, tokenizer = model_init.init(args)
    context_length = cache.max_num_tokens

    # Generator
    generator = Generator(
        model = model,
        cache = cache,
        tokenizer = tokenizer,
    )
    stop_conditions = prompt_format.stop_conditions(tokenizer)

    # Sampler
    sampler = ComboSampler(
        rep_p = args.repetition_penalty,
        pres_p = args.presence_penalty,
        freq_p = args.frequency_penalty,
        rep_sustain_range = args.penalty_range,
        rep_decay_range = args.penalty_range,
        temperature = args.temperature,
        min_p = args.min_p,
        top_k = args.top_k,
        top_p = args.top_p,
        temp_last = not args.temperature_first,
    )

    # Main loop
    print("\n" + col_sysprompt + system_prompt.strip() + col_default)
    context = []
    response = ""

    while True:

        # Amnesia mode
        if args.amnesia:
            context = []

        # Get user prompt
        user_prompt = read_input_fn(args, user_name, multiline)

        # Intercept commands
        if user_prompt.startswith("/"):
            c = user_prompt.strip().split(" ")
            match c[0]:

                # Exit app
                case "/x":
                    print_info("Exiting")
                    break

                # Copy codeblock to clipboard
                case "/cc":
                    try:
                        b = int(c[1])
                    except:
                        b = 1
                    snippet = copy_last_codeblock(response, b)
                    if not snippet:
                        print_error("No code block found in last response")
                    else:
                        num_lines = len(snippet.split("\n"))
                        print_info(f"Copied {num_lines} line{'s' if num_lines > 1 else ''} to the clipboard")
                    continue

                # Toggle multiline mode
                case "/mli":
                    multiline = not multiline
                    if multiline:
                        print_info("Enabled multiline mode")
                    else:
                        print_info("Disabled multiline mode")
                    continue

                # Clear context
                case "/clear":
                    context = []
                    print_info("Cleared context")
                    continue

                case _:
                    print_error(f"Unknown command: {c[0]}")
                    continue

        # Add to context
        context.append((user_prompt, None))

        # Tokenize context and trim from head if too long
        def get_input_ids():
            frm_context = prompt_format.format(system_prompt, context)
            if args.think:
                frm_context += prompt_format.thinktag()[0]
            ids_ = tokenizer.encode(frm_context, add_bos = add_bos, encode_special_tokens = True)
            exp_len_ = ids_.shape[-1] + max_response_tokens + 1
            return ids_, exp_len_

        ids, exp_len = get_input_ids()
        if exp_len > context_length:
            while exp_len > context_length - 2 * max_response_tokens:
                context = context[1:]
                ids, exp_len = get_input_ids()

        # Inference
        tt = prompt_format.thinktag()
        job = Job(
            input_ids = ids,
            max_new_tokens =  max_response_tokens,
            stop_conditions = stop_conditions,
            sampler = sampler,
            banned_strings = [tt[0], tt[1]] if args.no_think else None
        )
        generator.enqueue(job)

        # Stream response
        ctx_exceeded = False
        with streamer_cm(args, bot_name) as s:
            while generator.num_remaining_jobs():
                for r in generator.iterate():
                    chunk = r.get("text", "")
                    s.stream(chunk, tt[0], tt[1])
                    if r["eos"] and r["eos_reason"] == "max_new_tokens":
                        ctx_exceeded = True

        if ctx_exceeded:
            print(
                "\n" + col_error + f" !! Response exceeded {max_response_tokens} tokens "
                "and was cut short." + col_default
            )

        # Add response to context
        response = s.all_text.strip()

        context[-1] = (user_prompt, response)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    model_init.add_args(parser, cache = True)
    parser.add_argument("-mode", "--mode", type = str, help = "Prompt mode", required = True)
    parser.add_argument("-modes", "--modes", action = "store_true", help = "List available prompt modes and exit")
    parser.add_argument("-un", "--user_name", type = str, default = "User", help = "User name (raw mode only)")
    parser.add_argument("-bn", "--bot_name", type = str, default = "Assistant", help = "Bot name (raw mode only)")
    parser.add_argument("-mli", "--multiline", action = "store_true", help = "Enable multi line input (use Alt+Enter to submit input)")
    parser.add_argument("-sp", "--system_prompt", type = str, help = "Use custom system prompt")
    parser.add_argument("-maxr", "--max_response_tokens", type = int, default = 1000, help = "Max tokens per response, default = 1000")
    parser.add_argument("-basic", "--basic_console", action = "store_true", help = "Use basic console output (no markdown and fancy prompt input")
    parser.add_argument("-think", "--think", action = "store_true", help = "Use (very simplistic) reasoning template and formatting")
    parser.add_argument("-no_think", "--no_think", action = "store_true", help = "Suppress think tags (won't necessarily stop reasoning model from reasoning anyway)")
    parser.add_argument("-amnesia", "--amnesia", action = "store_true", help = "Forget context with every new prompt")
    parser.add_argument("-temp", "--temperature", type = float, help = "Sampling temperature", default = 0.8)
    parser.add_argument("-temp_first", "--temperature_first", action = "store_true", help = "Apply temperature before truncation")
    parser.add_argument("-repp", "--repetition_penalty", type = float, help = "Repetition penalty, HF style, 1 to disable (default: disabled)", default = 1.0)
    parser.add_argument("-presp", "--presence_penalty", type = float, help = "Presence penalty, 0 to disable (default: disabled)", default = 0.0)
    parser.add_argument("-freqp", "--frequency_penalty", type = float, help = "Frequency penalty, 0 to disable (default: disabled)", default = 0.0)
    parser.add_argument("-penr", "--penalty_range", type = int, help = "Range for penalties, in tokens (default: 1024) ", default = 1024)
    parser.add_argument("-minp", "--min_p", type = float, help = "Min-P truncation, 0 to disable (default: 0.08)", default = 0.08)
    parser.add_argument("-topk", "--top_k", type = int, help = "Top-K truncation, 0 to disable (default: disabled)", default = 0)
    parser.add_argument("-topp", "--top_p", type = float, help = "Top-P truncation, 1 to disable (default: disabled)", default = 1.0)
    _args = parser.parse_args()
    main(_args)

</content>

<content full_path="examples/common.py">

"""
Quick and dirty and probably not very accurate prompt templates for a couple of models
"""

def format_prompt(prompt_format, sp, p):

    match prompt_format:

        case "llama":
            return f"<s>[INST] <<SYS>>\n{sp}\n<</SYS>>\n\n{p} [/INST]"

        case "llama3":
            return (
            f"<|begin_of_text|>"
            f"<|start_header_id|>system<|end_header_id|>\n\n"
            f"{sp}<|eot_id|>"
            f"<|start_header_id|>user<|end_header_id|>\n\n"
            f"{p}<|eot_id|>"
            f"<|start_header_id|>assistant<|end_header_id|>\n\n"
        )

        case "mistral":
            return f"<s>[INST] {sp}\n\n n{p}[/INST]"

        case "granite":
            return (
                f"System:\n"
                f"{sp}\n\n"
                f"Question:\n"
                f"{p}\n\n"
                f"Answer:\n"
            )

        case "chatml":
            return (
                f"<|im_start|>system\n"
                f"{sp}<|im_end|>\n"
                f"<|im_start|>user\n"
                f"{p}<|im_end|>\n"
                f"<|im_start|>assistant\n"
            )

        case "gemma":
            return (
                f"<bos><start_of_turn>user\n"
                f"{p}<end_of_turn>\n"
                f"<start_of_turn>model\n"
            )

        case _:
            raise ValueError("Unknown prompt format")


def get_stop_conditions(prompt_format, tokenizer):

    match prompt_format:
        case "llama":
            return [tokenizer.eos_token_id]
        case "llama3":
            return [tokenizer.single_id("<|eot_id|>")]
        case "granite":
            return [tokenizer.eos_token_id, "\n\nQuestion:"]
        case "gemma":
            return [tokenizer.eos_token_id, "<end_of_turn>"]
        case "chatml":
            return [tokenizer.eos_token_id, "<|im_end|>"]

</content>

<content full_path="examples/generation_loop.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from exllamav3 import Config, Model, Cache, Tokenizer, DefaultSampler
from exllamav3.util import Timer
from common import format_prompt, get_stop_conditions
import torch

"""
This script demonstrates a minimal, cached generation pipeline, starting with tokenization of a prompt, prefill
and then token-by-token sampling from logits produced by iterative forward passes through the model. For most
applications the built-in generator offers more flexibility, though. 
"""

# Load model
config = Config.from_directory("/mnt/str/models/llama3.1-8b-instruct/exl3/4.0bpw/")
model = Model.from_config(config)
cache = Cache(model, max_num_tokens = 2048)
model.load()

# Load tokenizer
tokenizer = Tokenizer.from_config(config)

# Prepare inputs
prompt_format = "llama3"
prompt_text = format_prompt(
    prompt_format,
    "You are a super helpful language model.",
    "List five ways in which cats are superior to dogs."
)
context_ids = tokenizer.encode(prompt_text, encode_special_tokens = True)

# Sampling and stop conditions
sampler = DefaultSampler()
stop_conditions = get_stop_conditions(prompt_format, tokenizer)

# Get model vocabulary as a list of strings, for streaming the completion
vocab = tokenizer.get_id_to_piece_list()

# Prefill the prompt, up to but not including the last token, which will be the first token forwarded in the
# generation loop. Treat the cache as a rectangular batch
model.prefill(
    input_ids = context_ids[:, :-1],
    params = {
        "attn_mode": "flash_attn",
        "cache": cache,
        "past_len": 0,
        "batch_shape": (1, 2048),
    }
)

# Generation loop
max_new_tokens = 500
generated_tokens = 0
response = ""

torch.cuda.synchronize()
with Timer() as t:
    while generated_tokens < max_new_tokens:

        # Get logits for current position
        logits = model.forward(
            input_ids = context_ids[:, -1:],
            params = {
                "attn_mode": "flash_attn",
                "cache": cache,
                "past_len": context_ids.shape[-1] - 1,
                "batch_shape": (1, 2048),
            }
        )

        # Sample from logits
        sample = sampler.forward(logits, tokenizer = tokenizer)
        token_id = sample.item()

        # Detect end of stream
        if token_id in stop_conditions:
            break

        # Append sampled token to context
        context_ids = torch.cat((context_ids, sample.cpu()), dim = -1)
        token = vocab[token_id]
        response += token
        generated_tokens += 1

        # Stream to the console
        print(token, end = "", flush = True)

print()
print("---")
print(f"{generated_tokens} tokens at {generated_tokens/t.interval:.3f} tokens/second")
</content>

<content full_path="examples/chat_util.py">
import re
import sys
import pyperclip

def copy_last_codeblock(text: str, num) -> str | None:
    pattern = re.compile(r"```[^\n`]*\n(.*?)```", re.DOTALL)
    matches = pattern.findall(text)
    if not matches:
        return None
    if num > len(matches):
        num = len(matches)
    snippet = matches[-num].strip()
    pyperclip.copy(snippet)
    return snippet
</content>

<content full_path="examples/chat_templates.py">

class PromptFormat:
    def __init__(self, user_name, bot_name):
        self.user_name = user_name
        self.bot_name = bot_name
    def default_system_prompt(self):
        raise NotImplementedError()
    def format(self, system_prompt, messages):
        raise NotImplementedError()
    def add_bos(self):
        raise NotImplementedError()
    def thinktag(self):
        return "<think>", "</think>"


class PromptFormat_raw(PromptFormat):
    description = "Model-agnostic mode simulating a raw chatlog"

    def __init__(self, *args):
        super().__init__(*args)

    def default_system_prompt(self):
        return (
            f"This is a conversation between a helpful AI assistant " +
            (f"named {self.bot_name} " if self.bot_name != "Assistant" else "") +
            (f"and a user named {self.user_name}." if self.user_name != "User" else """and a user.""")
        )

    def format(self, system_prompt, messages):
        context = system_prompt + "\n"
        for (u, a) in messages:
            context += f"{self.user_name}: {u}\n"
            context += f"{self.bot_name}:"
            if a is not None:
                context += f"{a}\n"
        return context

    def add_bos(self):
        return True

    def stop_conditions(self, tokenizer):
        return [
            self.user_name + ":",
            self.user_name[0:1] + ":",
            self.user_name.upper() + ":",
            self.user_name.lower() + ":",
            tokenizer.eos_token_id
        ]


class PromptFormat_llama3(PromptFormat):
    description = "Llama3-instruct models"

    def __init__(self, *args):
        super().__init__(*args)

    def default_system_prompt(self):
        return (
            """Assist users with tasks and answer questions to the best of your knowledge. Provide helpful and informative """
            """responses. Be conversational and engaging. If you are unsure or lack knowledge on a topic, admit it and try """
            """to find the answer or suggest where to find it. Keep responses concise and relevant. Follow ethical """
            """guidelines and promote a safe and respectful interaction."""
        )

    def format(self, system_prompt, messages):
        context = f"<|start_header_id|>system<|end_header_id|>\n\n{system_prompt}<|eot_id|>"
        for (u, a) in messages:
            context += f"<|start_header_id|>user<|end_header_id|>\n\n{u}<|eot_id|>"
            context += f"<|start_header_id|>assistant<|end_header_id|>\n\n"
            if a is not None: context += f"{a}<|eot_id|>"
        return context

    def add_bos(self):
        return True

    def stop_conditions(self, tokenizer):
        return [
            tokenizer.eos_token_id,
            tokenizer.single_id("<|eot_id|>"),
            tokenizer.single_id("<|start_header_id|>")
        ]


class PromptFormat_chatml(PromptFormat):
    description = "ChatML format, as used by e.g. Qwen"

    def __init__(self, *args):
        super().__init__(*args)

    def default_system_prompt(self):
        return (
            f"You are {self.bot_name}, a large language model. Answer as concisely as possible."
        )

    def format(self, system_prompt, messages):
        context = f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
        for (u, a) in messages:
            context += f"<|im_start|>user\n{u}<|im_end|>\n"
            context += f"<|im_start|>assistant\n"
            if a is not None: context += f"{a}<|im_end|>\n"
        return context

    def add_bos(self):
        return False

    def stop_conditions(self, tokenizer):
        return [
            tokenizer.eos_token_id,
            tokenizer.single_id("<|im_end|>"),
            """<|im_end|>"""
        ]


class PromptFormat_phi(PromptFormat):
    description = "Phi3/Phi4 instruct models"

    def __init__(self, *args):
        super().__init__(*args)

    def default_system_prompt(self):
        return (
            f"You are a helpful AI assistant."
        )

    def format(self, system_prompt, messages):
        context = f"<|system|>\n{system_prompt}<|end|>\n"
        for (u, a) in messages:
            context += f"<|user|>\n{u}<|end|>\n"
            context += f"<|assistant|>\n"
            if a is not None: context += f"{a}<|end|>\n"
        return context

    def add_bos(self):
        return True

    def stop_conditions(self, tokenizer):
        return [
            tokenizer.eos_token_id,
            tokenizer.single_id("<|end|>"),
        ]


class PromptFormat_glm(PromptFormat):
    description = "ChatGLM(4) models"

    def __init__(self, *args):
        super().__init__(*args)

    def default_system_prompt(self):
        return (
            f"You are a helpful AI assistant."
        )

    def format(self, system_prompt, messages):
        context = f"[gMASK]<sop><|system|>\n{system_prompt}"
        for (u, a) in messages:
            context += f"<|user|>\n{u}"
            context += f"<|assistant|>\n"
            if a is not None: context += f"{a}"
        return context

    def add_bos(self):
        return True

    def stop_conditions(self, tokenizer):
        return [
            tokenizer.eos_token_id,
            tokenizer.single_id("<|user|>"),
        ]


class PromptFormat_mistral(PromptFormat):
    description = "Mistral-instruct models (v3)"

    def __init__(self, *args):
        super().__init__(*args)

    def default_system_prompt(self):
        return (
            """You are a helpful AI assistant."""
        )

    def format(self, system_prompt, messages):
        context = ""
        first = True
        for (u, a) in messages:
            if first:
                context += f"[INST] {system_prompt}\n\n{u}[/INST]"
                first = False
            else:
                context += f"[INST] {u}[/INST]"
            if a is not None: context += f" {a}</s>"
        return context

    def add_bos(self):
        return True

    def stop_conditions(self, tokenizer):
        return [
            tokenizer.eos_token_id
        ]


class PromptFormat_gemma(PromptFormat):
    description = "Gemma"

    def __init__(self, *args):
        super().__init__(*args)

    def default_system_prompt(self):
        return ""

    def format(self, system_prompt, messages):
        context = ""
        for (u, a) in messages:
            context += f"<start_of_turn>user\n"
            context += f"{u}<end_of_turn>\n"
            context += f"<start_of_turn>model\n"
            if a is not None: context += f"{a}<end_of_turn>\n"
        return context

    def add_bos(self):
        return True

    def stop_conditions(self, tokenizer):
        return [
            tokenizer.eos_token_id,
            tokenizer.single_id("<end_of_turn>"),
            tokenizer.single_id("<start_of_turn>"),
        ]

class PromptFormat_reka(PromptFormat):
    description = "Reka Flash 3"

    def __init__(self, *args):
        super().__init__(*args)

    def default_system_prompt(self):
        return ""

    def format(self, system_prompt, messages):
        context = ""
        first = True
        for (u, a) in messages:
            if first and system_prompt:
                context += f"human: {system_prompt} {u} <sep> "
                first = False
            else:
                context += f"human: {u} <sep> "
            context += f"assistant:"
            if a is not None: context += f" {a} <sep> "
        return context

    def add_bos(self):
        return False

    def stop_conditions(self, tokenizer):
        return [
            tokenizer.eos_token_id,
            "<sep>",
        ]

    def thinktag(self):
        return " <reasoning>\n", "</reasoning>"


class PromptFormat_cohere(PromptFormat):
    description = "Cohere"

    def __init__(self, *args):
        super().__init__(*args)

    def default_system_prompt(self):
        return (
            "## Task and Context\n"
            "You help people answer their questions and other requests interactively. You will be asked a very "
            "wide array of requests on all kinds of topics. You should focus on serving the user's needs as "
            "best you can, which will be wide-ranging.\n\n"
            "## Style Guide\n"
            "Unless the user asks for a different style of answer, you should answer in full sentences, using "
            "proper grammar and spelling."
        )

    def format(self, system_prompt, messages):
        context = ""
        if system_prompt:
            context += "<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>"
            context += system_prompt
            context += "<|END_OF_TURN_TOKEN|>"
        for (u, a) in messages:
            context += "<|START_OF_TURN_TOKEN|><|USER_TOKEN|>"
            context += u
            context += "<|END_OF_TURN_TOKEN|>"
            context += "<|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>"
            if a is not None:
                context += a
                context += "<|END_OF_TURN_TOKEN|>"
        return context

    def add_bos(self):
        return True

    def stop_conditions(self, tokenizer):
        return [
            tokenizer.eos_token_id,
            "<|END_OF_TURN_TOKEN|>",
        ]


prompt_formats = {
    "raw": PromptFormat_raw,
    "llama3": PromptFormat_llama3,
    "chatml": PromptFormat_chatml,
    "phi": PromptFormat_phi,
    "mistral": PromptFormat_mistral,
    "gemma": PromptFormat_gemma,
    "glm": PromptFormat_glm,
    "reka": PromptFormat_reka,
    "cohere": PromptFormat_cohere,
}

</content>

<content full_path="eval/compare_q_exllamav3.py">
import torch
from exllamav3 import Config, Model, Tokenizer, Cache
from exllamav3.modules import Linear

def get_tensor_size(tensors):
    return 8 * sum(t.element_size() * t.numel() for t in tensors.values())

def get_storage_info(model):
    sum_bits = 0
    sum_numel = 0
    head_bpw = 0
    head_numel = 0
    for module in model:
        if module.key.endswith("lm_head"):
            head_bpw = get_tensor_size(module.get_tensors()) / module.weights_numel()
            head_numel = module.weights_numel()
        elif isinstance(module, Linear):
            sum_bits += get_tensor_size(module.get_tensors())
            sum_numel += module.weights_numel()
    vram_bits = head_numel * head_bpw + sum_bits
    return sum_bits / sum_numel, head_bpw, vram_bits

def load_exllamav3(model_dir: str | list):
    if isinstance(model_dir, list):
        model_dir, override_tensors = model_dir
        config = Config.from_directory(model_dir)
        config.stc.add_tensor_files(override_tensors)
    else:
        config = Config.from_directory(model_dir)
    model = Model.from_config(config)
    model.load(max_output_size = 2048, max_output_factor = 7)
    bpw_layer, bpw_head, vram_bits = get_storage_info(model)
    return model, bpw_layer, bpw_head, vram_bits

def fwd_exllamav3(model_instance, input_ids: torch.Tensor):
    input_ids = input_ids.cpu()
    output = model_instance.forward(input_ids, {"attn_mode": "flash_attn_nc"})
    output[..., model_instance.config.vocab_size:] = float("-inf")
    return output
</content>

<content full_path="eval/compare_q_exllamav2.py">
import torch
from exllamav2 import ExLlamaV2, ExLlamaV2Config, ExLlamaV2Cache
from exllamav2.model import ExLlamaV2Linear

def get_tensor_size(tensors):
    return 8 * sum(t.element_size() * t.numel() for t in tensors.values())

def get_storage_info(model):
    sum_bits = 0
    sum_numel = 0
    head_bpw = 0
    head_numel = 0
    for key, module in model.modules_dict.items():
        if module.key == "lm_head":
            head_bpw = get_tensor_size(module.q_tensors) / module.numel()
            head_numel = module.numel()
        elif isinstance(module, ExLlamaV2Linear):
            if module.linear:
                sum_bits += get_tensor_size({"t": module.linear.weight})
                sum_numel += module.numel()
            else:
                sum_bits += get_tensor_size(module.q_tensors)
                sum_numel += module.numel()
    vram_bits = head_numel * head_bpw + sum_bits
    return sum_bits / sum_numel, head_bpw, vram_bits

def load_exllamav2(model_dir: str | list):
    config = ExLlamaV2Config(model_dir)
    model = ExLlamaV2(config)
    cache = ExLlamaV2Cache(model, batch_size = 1, max_seq_len = 2048)  # Cache isn't used but reqd by autosplit
    model.load_autosplit(cache, reserve_vram = 1024**3)
    bpw_layer, bpw_head, vram_bits = get_storage_info(model)
    return model, bpw_layer, bpw_head, vram_bits

def fwd_exllamav2(model_instance, input_ids: torch.Tensor):
    input_ids = input_ids.cpu()
    output = model_instance.forward(input_ids)
    return output
</content>

<content full_path="eval/compare_q_llamacpp.py">
try:
    import llama_cpp
    import gguf
    from gguf import GGUFReader
    from llama_cpp import Llama
except:
    pass
import torch
from functools import lru_cache
from exllamav3.util.file import disk_lru_cache

@lru_cache  # run once
def init_backend():
    llama_cpp.llama_backend_init(False)

@disk_lru_cache("lcpp_get_storage_info")
def get_storage_info(model_dir):
    reader = GGUFReader(model_dir)
    tensors = reader.tensors
    sum_bits = 0
    sum_numel = 0
    head_bpw = 0
    head_numel = 0
    for tensor_info in tensors:
        name = tensor_info.name
        if (name == "token_embd.weight" and head_bpw == 0) or \
            name == "output.weight":
            head_bpw = tensor_info.n_bytes * 8 / tensor_info.n_elements
            head_numel = tensor_info.n_elements
        elif name.endswith(".weight"):
            sum_bits += tensor_info.n_bytes * 8
            sum_numel += tensor_info.n_elements
    vram_bits = head_numel * head_bpw + sum_bits
    return sum_bits / sum_numel, head_bpw, vram_bits

def load_llamacpp(model_dir: str):
    init_backend()
    bpw_layer, bpw_head, vram_bits = get_storage_info(model_dir)
    model = Llama(
        model_path = model_dir,
        logits_all = True,
        verbose = False,
        n_ctx = 2048,
        n_gpu_layers = 999
    )
    return model, bpw_layer, bpw_head, vram_bits

def fwd_llamacpp(model_instance, input_ids: torch.Tensor):
    input_ids_list = input_ids[0].tolist()
    model_instance.reset()
    model_instance.eval(input_ids_list)
    logits = torch.from_numpy(model_instance.scores).unsqueeze(0).cuda()
    return logits
</content>

<content full_path="eval/ppl.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import argparse
from exllamav3.util.file import disk_lru_cache, disk_lru_cache_clear
from exllamav3.util.progress import ProgressBar
from exllamav3.util.memory import free_mem
from exllamav3 import Config, Model, Cache, Tokenizer, model_init
from datasets import load_dataset
import torch
import torch.nn.functional as F
import math


@disk_lru_cache("get_dataset_text")
def get_dataset_text(spec: dict):
    assert spec["dataset"] == "wiki2", "Only wiki2 implemented atm"
    dataset_text = "\n\n".join(
        load_dataset("wikitext", "wikitext-2-raw-v1", split = "test")
        ["text"]
    )
    return dataset_text


def get_test_tokens(tokenizer, rows, eval_len = 2048, eval_stride = 512):
    with ProgressBar("Tokenizing", rows) as pb:
        dataset_spec = { "dataset": "wiki2" }
        eval_tokens = tokenizer.encode(get_dataset_text(dataset_spec))
        num_tokens = eval_tokens.shape[-1]
        seqs = []
        for a in range(0, num_tokens - eval_len, eval_stride):
            b = a + eval_len
            seqs.append(eval_tokens[:, a:b])
            pb.update(len(seqs))
            if len(seqs) >= rows:
                break
    return torch.cat(seqs, dim = 0)[:, :]


@torch.inference_mode()
def main(args):

    # Load model
    # TODO: inplace softmax, reduce max_output_factor to 3
    model, config, _, tokenizer = model_init.init(
        args,
        override_dynamic_seq_len = 2048,
        max_output_size = 2048,
        max_output_factor = 5,
    )

    vocab_size = tokenizer.actual_vocab_size
    bpw_layer, bpw_head, vram_bits = model.get_storage_info()

    # Dataset
    eval_ids = get_test_tokens(tokenizer, args.rows)

    # Test
    logprob_sum = 0.0
    logprob_count = 0
    with ProgressBar("Evaluating", args.rows) as pb:
        for row in range(eval_ids.shape[0]):
            pb.update(row)
            input_ids = eval_ids[row:row + 1, :]
            logits = model.forward(input_ids, {"attn_mode": "flash_attn_nc"})
            logits = logits[:, :-1, :vocab_size].float()
            logits += 1e-10
            log_probs = F.log_softmax(logits, dim = -1)
            del logits
            target_ids = input_ids[:, 1:].to(log_probs.device)
            del input_ids
            target_log_probs = log_probs.gather(-1, target_ids.unsqueeze(-1)).squeeze(-1)
            logprob_sum += target_log_probs.sum().item()
            logprob_count += target_ids.numel()
            del log_probs
            del target_log_probs
            del target_ids
            torch.cuda.empty_cache()
        pb.update(args.rows)
        mean_log_prob = logprob_sum / logprob_count
        perplexity = math.exp(-mean_log_prob)

    print(f" -- Model: {args.model_dir}")
    print(f" -- Bitrate: {bpw_layer:.2f} bpw / {bpw_head:.2f} bpw (head)")
    print(f" -- Evaluated: {eval_ids.shape[0]} rows of {eval_ids.shape[1]} tokens")
    print(f" -- Perplexity: {perplexity:.6f}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    model_init.add_args(parser, cache = False)
    parser.add_argument("-r", "--rows", type = int, help = "Number of rows", default = 100)
    _args = parser.parse_args()
    main(_args)

</content>

<content full_path="eval/compare_q_logits.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import argparse
from exllamav3 import Config, Model, Cache, Tokenizer, model_init
import torch
import torch.nn.functional as F
import math
import json

from compare_q import get_test_data, save_tensor

# ANSI codes
ESC = "\u001b"
col_default = "\u001b[0m"
col_yellow = "\u001b[33;1m"
col_blue = "\u001b[34;1m"
col_green = "\u001b[32;1m"
col_red = "\u001b[31;1m"
col_purple = "\u001b[35;1m"
col_cyan = "\u001b[36;1m"
col_white = "\u001b[37;1m"

def stream_forward(args, config, model, batch):

    state = batch
    for idx, module in enumerate(model.modules):

        # Load next module
        print(f" -- Loading module: {col_green}{module.key}{col_default}")
        config.stc.begin_deferred_load()
        module.load(torch.device(args.device) if not module.caps.get("prefer_cpu") else "cpu")
        config.stc.end_deferred_load()

        # Forward pass
        print(f" -- Forward pass")
        params = {}
        state = module.prepare_for_device(state, params)
        state = module.forward(state, params)

        # Unload current module
        module.unload()
        config.stc.close()

    return state

@torch.inference_mode()
def main(args):

    # Create model config
    config = Config.from_directory(args.model_dir)
    config.override_dynamic_seq_len(2048)
    # tokenizer = Tokenizer.from_config(config)
    model = Model.from_config(config)

    # Input state
    with open(args.dataspec, "r", encoding = "utf8") as f:
        data_spec = json.load(f)
    eval_ids = get_test_data(data_spec)
    eval_ids = eval_ids[:args.rows]

    collect_logits = []
    batches = eval_ids.split(args.rows_per_batch, 0)
    for idx, batch in enumerate(batches):
        print(f" -- Forward pass {idx + 1} / {len(batches)}")
        logits = stream_forward(args, config, model, batch)
        collect_logits.append(logits.cpu())
        del logits

    collect_logits = torch.cat(collect_logits, dim = 0)
    collect_logits = collect_logits.split(1, 0)

    print(f" -- Writing {args.out_logits}")
    save_tensor(collect_logits, args.out_logits)
    print(f" -- Done")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model_dir", type = str, help = "Path to model directory", required = True)
    parser.add_argument("-d", "--dataspec", type = str, help = "Data specification (JSON file)")
    parser.add_argument("-dev", "--device", type = int, help = "CUDA device index", default = 0)
    parser.add_argument("-r", "--rows", type = int, help = "Number of rows", default = 10)
    parser.add_argument("-rpb", "--rows_per_batch", type = int, help = "Rows per batch", default = 5)
    parser.add_argument("-o", "--out_logits", type = str, help = "Output file", required = True)
    _args = parser.parse_args()
    main(_args)

</content>

<content full_path="eval/compare_q_transformers.py">
import torch
from gptqmodel.nn_modules.qlinear.marlin import MarlinQuantLinear
from gptqmodel.nn_modules.qlinear.tritonv2 import TritonV2QuantLinear
from gptqmodel.nn_modules.qlinear.exllamav2 import ExllamaV2QuantLinear
from transformers import AutoTokenizer, AutoModelForCausalLM
from aqlm import QuantizedLinear
from awq.modules.linear import WQLinear_GEMM
from vptq import VQuantLinear
from bitsandbytes.nn import Linear4bit

def get_tensors_size(tensors):
    return 8 * sum(t.element_size() * t.numel() for t in tensors.values() if t is not None)

def get_tensor_size(tensor):
    return 8 * tensor.element_size() * tensor.numel()

def scan_gpu_tensors(obj, seen = None):
    if seen is None:
        seen = set()
    obj_id = id(obj)
    if obj_id in seen:
        return 0
    seen.add(obj_id)
    total_size = 0
    # If it's a GPU tensor, add its memory usage.
    if isinstance(obj, torch.Tensor) and obj.is_cuda:
        total_size += obj.element_size() * obj.nelement()
    else:
        if isinstance(obj, dict):
            for key, value in obj.items():
                total_size += scan_gpu_tensors(key, seen)
                total_size += scan_gpu_tensors(value, seen)
            return total_size
        if isinstance(obj, (list, tuple, set)):
            for item in obj:
                total_size += scan_gpu_tensors(item, seen)
            return total_size
        if hasattr(obj, '__dict__'):
            total_size += scan_gpu_tensors(vars(obj), seen)
        if hasattr(obj, '__slots__'):
            for slot in obj.__slots__:
                try:
                    attr = getattr(obj, slot)
                    total_size += scan_gpu_tensors(attr, seen)
                except AttributeError:
                    continue
    return total_size

def get_storage_info(model):
    sum_bits = 0
    sum_numel = 0
    head_bpw = 0
    head_numel = 0
    for name, module in model.named_modules():
        if any(isinstance(module, x) for x in [Linear4bit]):
            if module.out_features >= model.vocab_size * 0.9:  # this is foolproof
                head_numel = module.in_features * module.out_features
                head_bpw = module.weight.numel() * 8
                head_bpw = (head_bpw + scan_gpu_tensors(module.quant_state) * 8) / head_numel
            else:
                sum_bits += module.weight.numel() * 8
                sum_bits += scan_gpu_tensors(module.quant_state) * 8
                sum_numel += module.in_features * module.out_features
        elif any(isinstance(module, x) for x in [torch.nn.Linear]):
            if module.out_features >= model.vocab_size * 0.9:
                head_bpw = module.weight.element_size() * 8
                head_numel = module.weight.numel()
            else:
                sum_bits += get_tensor_size(module.weight)
                sum_numel +=  module.weight.numel()
        elif any(isinstance(module, x) for x in [QuantizedLinear, VQuantLinear]):
            sum_bits += get_tensors_size(dict(module.named_parameters()))
            sum_numel += module.in_features * module.out_features
        elif any(isinstance(module, x) for x in [WQLinear_GEMM]):
            sum_bits += get_tensors_size({
                "qweight": module.qweight,
                "qzeros": module.qzeros,
                "scales": module.scales,
            })
            sum_numel += module.in_features * module.out_features
        elif any(isinstance(module, x) for x in [MarlinQuantLinear]):
            sum_bits += get_tensors_size({
                "g_idx": module.g_idx,
                "g_idx_sort_indices": module.g_idx_sort_indices,
                "qweight": module.qweight,
                "qzeros": module.qzeros,
                "scales": module.scales,
            })
            sum_numel += module.in_features * module.out_features
        elif any(isinstance(module, x) for x in [TritonV2QuantLinear]):
            sum_bits += get_tensors_size({
                "g_idx": module.g_idx,
                "qweight": module.qweight,
                "qzeros": module.qzeros,
                "scales": module.scales,
            })
            sum_numel += module.in_features * module.out_features
        elif any(isinstance(module, x) for x in [ExllamaV2QuantLinear]):
            sum_bits += get_tensors_size(module.q_tensors)
            sum_numel += module.in_features * module.out_features
    vram_bits = head_numel * head_bpw + sum_bits
    return sum_bits / sum_numel, head_bpw, vram_bits

@torch.inference_mode
def load_transformers(model_dir: str, auto = False, bf16 = False):
    model = AutoModelForCausalLM.from_pretrained(
        model_dir,
        device_map = "auto" if auto else "cuda:0",
        torch_dtype = torch.bfloat16 if bf16 else torch.half
    )
    bpw_layer, bpw_head, vram_bits = get_storage_info(model)
    return model, bpw_layer, bpw_head, vram_bits

@torch.inference_mode
def load_transformers_auto(model_dir: str):
    return load_transformers(model_dir, auto = True)

@torch.inference_mode
def load_transformers_auto_bf16(model_dir: str):
    return load_transformers(model_dir, auto = True, bf16 = True)

@torch.inference_mode
def fwd_transformers(model_instance, input_ids: torch.Tensor):
    input_ids = input_ids.to("cuda:0")
    output = model_instance(input_ids)
    return output.logits

@torch.inference_mode
def tokenize_transformers(tokenizer_dir: str, text: str):
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)
    output = tokenizer(text, return_tensors="pt")
    return output.input_ids

</content>

<content full_path="eval/prequant_test.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import argparse
from exllamav3.util.file import disk_lru_cache, disk_lru_cache_clear
from exllamav3.util.progress import ProgressBar
from exllamav3.util.memory import free_mem
from exllamav3 import Config, Model, Cache, Tokenizer, model_init
from exllamav3.ext import exllamav3_ext as ext
from datasets import load_dataset
from exllamav3.modules import Linear
from exllamav3.modules.quant import LinearFP16, LinearEXL3
from exllamav3.modules.quant.exl3_lib.quantize import regularize
import torch
import torch.nn.functional as F
import math
import numpy as np
import termplotlib as tpl

# ANSI codes
ESC = "\u001b"
col_default = "\u001b[0m"
col_yellow = "\u001b[33;1m"
col_blue = "\u001b[34;1m"
col_green = "\u001b[32;1m"
col_red = "\u001b[31;1m"
col_purple = "\u001b[35;1m"
col_cyan = "\u001b[36;1m"
col_white = "\u001b[37;1m"

block_chars = [" ", "▁", "▂", "▃", "▄", "▅", "▆", "▇", "█"]

@disk_lru_cache("get_dataset_text")
def get_dataset_text(spec: dict):
    assert spec["dataset"] == "wiki2", "Only wiki2 implemented atm"
    dataset_text = "\n\n".join(
        load_dataset("wikitext", "wikitext-2-raw-v1", split = "test")
        ["text"]
    )
    return dataset_text


def get_test_tokens(tokenizer, rows, bos, eval_len = 2048, eval_stride = 512):
    with ProgressBar("Tokenizing", rows) as pb:
        dataset_spec = { "dataset": "wiki2" }
        eval_tokens = tokenizer.encode(get_dataset_text(dataset_spec))
        num_tokens = eval_tokens.shape[-1]
        seqs = []
        for a in range(0, num_tokens - eval_len, eval_stride):
            b = a + eval_len
            if bos is not None:
                r = torch.cat((bos, eval_tokens[:, a:b-1]), dim = -1)
            else:
                r = eval_tokens[:, a:b]
            seqs.append(r)
            pb.update(len(seqs))
            if len(seqs) >= rows:
                break
    return torch.cat(seqs, dim = 0)[:, :]



@torch.compile(fullgraph = True, mode = "reduce-overhead")
def count_threshold(x: torch.Tensor, abs_threshold: float) -> torch.Tensor:
    return (x.abs() > abs_threshold).sum(dtype = torch.int64)


def bchart(bins, min_value, max_value, cc, height = 10):
    maxcount = bins.max()
    lines = []
    for r in range(height):
        line = cc
        for c in range(len(bins)):
            if maxcount == 0:
                b = block_chars[0]
            else:
                y = (bins[c] / maxcount) * height - r
                y = max(min(y, 1), 0)
                b = block_chars[int(y * 8)]
            line += b
        line += col_default
        lines.append(line)
    lines.reverse()
    return lines


def histogram(args, tensor):

    nbins = args.histogram_bins
    stddev = tensor.std()
    min_value = tensor.amin().item()
    max_value = tensor.amax().item()

    # Middle histogram
    m_min_value = -stddev * 3
    m_max_value = stddev * 3

    m_bins = torch.empty((nbins // 2,), dtype = torch.long, device = tensor.device)
    ext.histogram(tensor, m_bins, m_min_value, m_max_value, True)
    m_count = m_bins.sum().item()
    m_bins = m_bins.cpu().numpy()

    # Low histogram
    l_min_value = min_value
    l_max_value = m_min_value
    l_bins = torch.empty((nbins // 4,), dtype = torch.long, device = tensor.device)
    ext.histogram(tensor, l_bins, l_min_value, l_max_value, True)
    l_count = l_bins.sum().item()
    l_bins = l_bins.cpu().numpy()

    # High histogram
    h_min_value = m_max_value + 0.001
    h_max_value = max_value
    h_bins = torch.empty((nbins // 4,), dtype = torch.long, device = tensor.device)
    ext.histogram(tensor, h_bins, h_min_value, h_max_value, True)
    h_count = h_bins.sum().item()
    h_bins = h_bins.cpu().numpy()

    total = l_count + m_count + h_count
    l_pct = l_count / total * 100.0
    m_pct = m_count / total * 100.0
    h_pct = h_count / total * 100.0

    bc_l = bchart(l_bins, l_min_value, l_max_value, col_yellow)
    bc_m = bchart(m_bins, m_min_value, m_max_value, col_cyan)
    bc_h = bchart(h_bins, h_min_value, h_max_value, col_yellow)
    bc = [f"{l} {m} {h}" for l, m, h in zip(bc_l, bc_m, bc_h)]

    bc.append(col_default + ("─" * (nbins // 4)) + "┬" + ("─" * (nbins // 2)) + "┬" + ("─" * (nbins // 4 )) + col_default)

    for vl, vml, vmh, vh in zip(
        [f"{l_min_value:.2e}", f"{l_count:,} elem", f"{l_pct:.5f} %"],
        [f"{m_min_value:.2e}", f"{m_count:,} elem", f"{m_pct:.5f} %"],
        [f"{m_max_value:.2e}", "", ""],
        [f"{h_max_value:.2e}", f"{h_count:,} elem", f"{h_pct:.5f} %"],
    ):
        ax = ""
        ax += col_yellow + vl + (" " * ((nbins // 4) - len(vl)))
        ax += " " + col_cyan + vml + (" " * ((nbins // 2) - len(vml) - len(vmh))) + vmh + " "
        ax += (" " * ((nbins // 4) - len(vh))) + col_yellow + vh + col_default
        bc.append(ax)

    print("\n".join(bc))
    print()


def stats(args, tensor):

    # inf/NaN values
    inf_nan =  torch.zeros(2, dtype = torch.long, device = tensor.device)
    ext.count_inf_nan(tensor, inf_nan)
    inf = inf_nan[0].item()
    nan = inf_nan[1].item()
    if inf or nan:
        print(f"{col_red}inf values                 : {col_white}{inf:,}{col_default}")
        print(f"{col_red}NaN values                 : {col_white}{nan:,}{col_default}")
        print(f"{col_red}Total numel                : {col_white}{tensor.numel():,}{col_default}")
        print()

    min_value = tensor.amin().item()
    max_value = tensor.amax().item()
    print(f"Min value                  : {col_white}{min_value:18.8f}{col_default}")
    print(f"Max value                  : {col_white}{max_value:18.8f}{col_default}")

    sigma = tensor.std(unbiased = False)
    print(f"Std. deviation             : {col_white}{sigma:18.8f}{col_default}")

    # mu = tensor.mean()
    # std2 = tensor.var(unbiased = False)
    # std4 = std2 ** 2
    # kurt = ((tensor - mu) ** 4).mean() / (std4 + 1e-10)
    # print(f"Kurtosis                   : {col_white}{kurt:18.8f}{col_default}")

    n6 = count_threshold(tensor, 6 * sigma)
    n6p = n6 / tensor.numel()
    if n6 > 0:
        print(f"Six-sigma exceedance       : {col_white}{n6p:18.8f}{col_default} ({col_white}{n6:,} elem{col_default})")
    else:
        print(f"Six-sigma exceedance       : {col_white}None{col_default}")

    print()


def inspect_state(args, state):
    state = state[:, args.skip_tokens:, :].to(torch.device(args.device))
    print(f"{col_blue}Hidden states{col_default}")
    print("─────────────")
    stats(args, state)
    histogram(args, state)


def inspect_module(args, module):
    linears = [m for m in module if isinstance(m, Linear)]
    for linear in linears:
        print(f"{col_blue}{linear.key}{col_default}")
        print("─" * len(linear.key))
        w = linear.inner.get_weight_tensor()
        stats(args, w)

        nbins = args.histogram_bins
        # bitrate = args.regularize_bits
        stddev = w.std(unbiased = False)

        min_value = -stddev * 3
        max_value = stddev * 3
        bins = torch.empty((nbins // 2,), dtype = torch.long, device = w.device)
        ext.histogram(w, bins, min_value, max_value, False)

        k, n = w.shape
        su = (torch.randn(k, device = w.device).sign() + 1e-5).sign().to(torch.float).unsqueeze(1)
        sv = (torch.randn(n, device = w.device).sign() + 1e-5).sign().to(torch.float).unsqueeze(0)
        quant_args = {
            # "K": bitrate,
            "apply_out_scales": None,
            "devices": [args.device],
        }
        apply_out_scales, w, g_scale, su, sv = regularize(
            w.float(),
            su,
            sv,
            quant_args,
            False,
            None,
            None,
            skip_g_scale = True
        )

        r_stddev = w.std(unbiased = False)
        r_min_value = -r_stddev * 3
        r_max_value = r_stddev * 3
        r_bins = torch.empty((nbins // 2,), dtype = torch.long, device = w.device)
        ext.histogram(w, r_bins, r_min_value, r_max_value, False)

        print(f"Reg. std. deviation        : {col_white}{r_stddev:18.8f}{col_default}")
        w4 = count_threshold(w, 4)
        w4p = w4 / w.numel()
        if w4 > 0:
            print(f"Reg. outliers >4           : {col_yellow}{w4p:18.8f}{col_default} ({col_yellow}{w4:,} elem{col_default})")
        else:
            print(f"Reg. outliers >4           : {col_white}None{col_default}")
        w8 = count_threshold(w, 8)
        w8p = w8 / w.numel()
        if w8 > 0:
            print(f"Reg. outliers >8           : {col_yellow}{w8p:18.8f}{col_default} ({col_yellow}{w8:,} elem{col_default})")
        else:
            print(f"Reg. outliers >8           : {col_white}None{col_default}")
        print()

        bc_pre = bchart(bins, min_value, max_value, col_default)
        bc_reg = bchart(r_bins, r_min_value, r_max_value, col_purple)
        bc = [f"{p}  {r}" for p, r in zip(bc_pre, bc_reg)]

        bc.append(col_default + ("─" * (nbins // 2)) + "  " + ("─" * (nbins // 2)) + col_default)

        for a, b, c, d in zip(
                [f"{min_value:.2e}", "Input layer"],
                [f"{max_value:.2e}", ""],
                [f"{r_min_value:.2e}", "Regularized layer"],
                [f"{r_max_value:.2e}", ""],
        ):
            ax = ""
            ax += col_default + a + (" " * ((nbins // 2) - len(a) - len(b))) + b + col_default
            ax += "  "
            ax += col_purple + c + (" " * ((nbins // 2) - len(c) - len(d))) + d + col_default
            bc.append(ax)

        print("\n".join(bc))
        print()


@torch.inference_mode()
def main(args):

    # Create model config
    config = Config.from_directory(args.model_dir)
    config.override_dynamic_seq_len(2048)
    tokenizer = Tokenizer.from_config(config)
    model = Model.from_config(config)

    # Input state
    bos = None if not args.bos else torch.tensor([[config.bos_token_id]], dtype = torch.long)
    eval_ids = get_test_tokens(tokenizer, args.rows, bos)
    state = eval_ids

    # Streaming forward pass
    for idx, module in enumerate(model.modules):

        # Load next module
        print(f" -- Loading module: {col_green}{module.key}{col_default}")
        print()
        config.stc.begin_deferred_load()
        module.load(torch.device(args.device) if not module.caps.get("prefer_cpu") else "cpu")
        config.stc.end_deferred_load()
        inspect_module(args, module)

        # Forward pass
        print(f" -- Forward pass")
        print()
        params = {}
        state = module.prepare_for_device(state, params)
        state = module.forward(state, params)
        inspect_state(args, state)

        # Unload current module
        module.unload()
        config.stc.close()
        free_mem()

    # Test perplexity
    vocab_size = tokenizer.actual_vocab_size
    logprob_sum = 0.0
    logprob_count = 0
    with ProgressBar("Evaluating", args.rows) as pb:
        for row in range(state.shape[0]):
            pb.update(row)
            input_ids = eval_ids[row:row + 1, :]
            logits = state[row:row+1, ...]
            logits = logits[:, :-1, :vocab_size].float()
            log_probs = F.log_softmax(logits, dim = -1)
            del logits
            target_ids = input_ids[:, 1:].to(log_probs.device)
            del input_ids
            target_log_probs = log_probs.gather(-1, target_ids.unsqueeze(-1)).squeeze(-1)
            logprob_sum += target_log_probs.sum().item()
            logprob_count += target_ids.numel()
            del log_probs
            del target_log_probs
            del target_ids
            torch.cuda.empty_cache()
        pb.update(args.rows)
        mean_log_prob = logprob_sum / logprob_count
        perplexity = math.exp(-mean_log_prob)

    print(f"{col_blue}Outputs{col_default}")
    print("───────")
    print(f"Perplexity                 : {col_white}{perplexity:.6f}{col_default}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model_dir", type = str, help = "Path to model directory", required = True)
    parser.add_argument("-d", "--device", type = int, help = "CUDA device index", default = 0)
    parser.add_argument("-r", "--rows", type = int, help = "Number of rows", default = 10)
    parser.add_argument("-hb", "--histogram_bins", type = int, help = "Histogram bins", default = 160)
    parser.add_argument("-bos", "--bos", action = "store_true", help = "Add BOS token on each row")
    parser.add_argument("-skip", "--skip_tokens", type = int, help = "Skip tokens at start of context", default = 0)
    # parser.add_argument("-rb", "--regularize_bits", type = int, help = "Target bitrate for regularization test", default = 4)
    _args = parser.parse_args()
    main(_args)

</content>

<content full_path="eval/longctx.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import argparse
from transformers import AutoTokenizer
from exllamav3.util.progress import ProgressBar
from exllamav3 import Config, Model, Cache, Tokenizer, model_init, Generator, Job, GreedySampler
import torch

# ANSI codes
ESC = "\u001b"
col_default = "\u001b[0m"
col_yellow = "\u001b[33;1m"
col_blue = "\u001b[34;1m"
col_green = "\u001b[32;1m"
col_red = "\u001b[31;1m"
col_gray = "\u001b[37;1m"

@torch.inference_mode()
def main(args):

    # Load model
    model, config, cache, tokenizer = model_init.init(args)
    generator = Generator(model, cache, tokenizer, show_visualizer = args.visualize_cache)
    bpw_layer, bpw_head, vram_bits = model.get_storage_info()

    print(f" -- Model: {args.model_dir}")
    print(f" -- Bitrate: {bpw_layer:.2f} bpw / {bpw_head:.2f} bpw (head)")

    # Load Transformers tokenizers
    t_tokenizer = AutoTokenizer.from_pretrained(args.model_dir)

    # Get
    texts_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "eval_texts")
    with open(os.path.join(texts_dir, "illustrious_client.txt"), "r") as file:
        text_ic_orig = file.read()
    with open(os.path.join(texts_dir, "illustrious_client_c1.txt"), "r") as file:
        text_ic_french = file.read()
    with open(os.path.join(texts_dir, "illustrious_client_c2.txt"), "r") as file:
        text_ic_zoomer = file.read()
    with open(os.path.join(texts_dir, "illustrious_client_sum.txt"), "r") as file:
        text_ic_sum = file.read()
    with open(os.path.join(texts_dir, "variable_man_mod.txt"), "r") as file:
        text_vm_mod = file.read()
    with open(os.path.join(texts_dir, "variable_man_mod_c1.txt"), "r") as file:
        text_vm_pony = file.read()
    with open(os.path.join(texts_dir, "variable_man_sum.txt"), "r") as file:
        text_vm_sum = file.read()
    with open(os.path.join(texts_dir, "variable_man_char.txt"), "r") as file:
        text_vm_char = file.read()

    # Template
    def make_job(instruction):
        chat = [{
            "role": "user",
            "content": instruction
        }]
        input_ids = t_tokenizer.apply_chat_template(chat, add_generation_prompt = True)
        input_ids = torch.tensor(input_ids, dtype = torch.long).unsqueeze(0)
        job = Job(
            input_ids = input_ids,
            max_new_tokens = 768,
            stop_conditions = config.eos_token_id_list,
            sampler = GreedySampler()
        )
        return job, input_ids.shape[-1]

    # Tests
    # TODO: Find some original source material that models are sure to be entirely unfamiliar with
    job_ic_sum, len_ic_sum = make_job(text_ic_orig + "\n\n---\n\nProvide an extremely short summary of the story.")
    job_ic_french, _ = make_job(text_ic_french + "\n\n---\n\nOne paragraph in this story has been translated to a different language. Translate it back.")
    job_ic_zoomer, _ = make_job(text_ic_zoomer + "\n\n---\n\nTwo paragraphs have been rewritten in a zoomer slang style. Identify them.")
    job_vm_sum, len_vm_sum = make_job(text_vm_mod + "\n\n---\n\nProvide an extremely short summary of the story.")
    vm_q1 = "Why do the SRB computers stop giving reliable war-odds after Edward Milsom arrives in the 22nd century?"
    vm_a1 = "Milsom’s behavior is unpredictable to the machines because he comes from a different era and doesn’t fit their statistical patterns, so his presence introduces a “variable” they cannot factor."
    vm_q2 = "What does Edward Milsom secretly do to the Icarus bomb’s control turret?"
    vm_a2 = "Instead of wiring it to trigger an explosion, he rewires it so the craft can decelerate safely from faster-than-light speed, turning it from a bomb into a workable FTL drive."
    vm_q3 = "How does humanity ultimately benefit from Milsom’s interference, even though Earth loses the war against Jorblax?"
    vm_a3 = "Milsom’s solution delivers a practical faster-than-light return method, giving Earth true interstellar travel and opening the entire universe for exploration and colonization, making the war’s outcome irrelevant."
    job_vm_q1, _ = make_job(text_vm_mod + f"\n\n---\n\nAnswer in one paragraph: {vm_q1}")
    job_vm_q2, _ = make_job(text_vm_mod + f"\n\n---\n\nAnswer in one paragraph: {vm_q2}")
    job_vm_q3, _ = make_job(text_vm_mod + f"\n\n---\n\nAnswer in one paragraph: {vm_q3}")
    job_vm_char, _ = make_job(text_vm_mod + "\n\n---\n\nList all the named characters in the story.")
    job_vm_pony, _ = make_job(text_vm_pony+ "\n\n---\n\nA passage from an unrelated story is inserted in the middle of the text. Can you find it?")

    # Inference
    jobs = [
        job_ic_sum,
        job_ic_french,
        job_ic_zoomer,
        job_vm_sum,
        job_vm_q1,
        job_vm_q2,
        job_vm_q3,
        job_vm_pony,
        job_vm_char,
    ]
    generator.enqueue(jobs)

    with ProgressBar("Inference", len(jobs)) as pb:
        while j := generator.num_remaining_jobs():
            generator.iterate()
            pb.update(len(jobs) - j)

    # Results
    print()
    print(f"{col_green}------------{col_default}")
    print(f"{col_green}SUMMARY TEST{col_default}")
    print(f"{col_green}------------{col_default}")
    print(f"{col_blue}Short summary of 'The Illustrious Client', {len_ic_sum} tokens.\nReference summary:{col_default}")
    print(f"{col_gray}{text_ic_sum.strip()}{col_default}")
    print()
    print(job_ic_sum.full_completion.strip())
    print()

    print()
    print(f"{col_green}-----------{col_default}")
    print(f"{col_green}FRENCH TEST{col_default}")
    print(f"{col_green}-----------{col_default}")
    print(f"{col_blue}One paragraph in this story has been translated to a different language. Translate it back.{col_default}")
    print()
    print(job_ic_french.full_completion.strip())
    print()

    print()
    print(f"{col_green}-----------{col_default}")
    print(f"{col_green}ZOOMER TEST{col_default}")
    print(f"{col_green}-----------{col_default}")
    print(f"{col_blue}A zoomer has edited the text. Identify the edited passages.{col_default}")
    print()
    print(job_ic_zoomer.full_completion.strip())
    print()

    print()
    print(f"{col_green}------------{col_default}")
    print(f"{col_green}SUMMARY TEST{col_default}")
    print(f"{col_green}------------{col_default}")
    print(f"{col_blue}Short summary of a version of 'The Variable Man' with some names replaced, {len_vm_sum} tokens.\nReference summary:{col_default}")
    print(f"{col_gray}{text_vm_sum.strip()}{col_default}")
    print()
    print(job_vm_sum.full_completion.strip())
    print()

    print()
    print(f"{col_green}--------{col_default}")
    print(f"{col_green}Q&A TEST{col_default}")
    print(f"{col_green}--------{col_default}")
    print(f"{col_blue}{vm_q1} Reference answer:{col_default}")
    print(f"{col_gray}{vm_a1}{col_default}")
    print()
    print(job_vm_q1.full_completion.strip())
    print()
    print(f"{col_blue}{vm_q2} Reference answer:{col_default}")
    print(f"{col_gray}{vm_a2}{col_default}")
    print()
    print(job_vm_q2.full_completion.strip())
    print()
    print(f"{col_blue}{vm_q3} Reference answer:{col_default}")
    print(f"{col_gray}{vm_a3}{col_default}")
    print()
    print(job_vm_q3.full_completion.strip())
    print()

    print()
    print(f"{col_green}---------------{col_default}")
    print(f"{col_green}CORRUPTION TEST{col_default}")
    print(f"{col_green}---------------{col_default}")
    print(f"{col_blue}Some MLP fan fiction has made it into the story. Can we detect it?{col_default}")
    print()
    print(job_vm_pony.full_completion.strip())
    print()

    print()
    print(f"{col_green}--------------------{col_default}")
    print(f"{col_green}NAME EXTRACTION TEST{col_default}")
    print(f"{col_green}--------------------{col_default}")
    print(f"{col_blue}List all the named characters in the story.\nReference:{col_default}")
    print(f"{col_gray}{text_vm_char}{col_default}")
    print()
    print(job_vm_char.full_completion.strip())
    print()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    model_init.add_args(parser, default_cache_size = 65536)
    parser.add_argument("-vis", "--visualize_cache", action = "store_true", help = "Show cache visualizer (slow)")
    _args = parser.parse_args()
    main(_args)

</content>

<content full_path="eval/compare_q.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import torch
import torch.nn.functional as F
from exllamav3.util.file import disk_lru_cache, disk_lru_cache_clear
from exllamav3.util.progress import ProgressBar
from exllamav3.util.memory import free_mem
from datasets import load_dataset
import math
import argparse
import json
import matplotlib.pyplot as plt
from adjustText import adjust_text
import glob
from safetensors.torch import save_file
from safetensors import safe_open

torch.set_printoptions(precision = 5, sci_mode = False, linewidth = 200)

# Lookup tables to ensure test functions are cacheable

from compare_q_transformers import (
    load_transformers_auto_bf16,
    load_transformers_auto,
    load_transformers,
    fwd_transformers,
    tokenize_transformers
)
from compare_q_exllamav2 import (
    load_exllamav2,
    fwd_exllamav2
)
from compare_q_exllamav3 import (
    load_exllamav3,
    fwd_exllamav3
)
from compare_q_llamacpp import (
    load_llamacpp,
    fwd_llamacpp
)

load_fns = {
    "transformers_auto_bf16": load_transformers_auto_bf16,
    "transformers_auto": load_transformers_auto,
    "transformers": load_transformers,
    "exllamav2": load_exllamav2,
    "exllamav3": load_exllamav3,
    "llamacpp": load_llamacpp,
}

fwd_fns = {
    "transformers": fwd_transformers,
    "exllamav2": fwd_exllamav2,
    "exllamav3": fwd_exllamav3,
    "llamacpp": fwd_llamacpp,
}

tokenize_fns = {
    "transformers": tokenize_transformers,
}

# Util fn

def load_tensor(filename):
    with safe_open(filename, framework = "pt", device = "cpu") as f:
        if "tensor" in f.keys():
            return f.get_tensor("tensor")
        else:
            tensors = []
            i = 0
            while f"tensor.{i}" in f.keys():
                tensors.append(f.get_tensor(f"tensor.{i}"))
                i += 1
            return tensors

def save_tensor(tensor, filename: str):
    if isinstance(tensor, dict):
        save_file({k: v for k, v in tensor.items()}, filename)
    elif isinstance(tensor, list):
        save_file({f"tensor.{i}": t for i, t in enumerate(tensor)}, filename)
    else:
        save_file({f"tensor": tensor}, filename)

# Tokenize ppl test data

@disk_lru_cache("get_dataset")
def get_test_data(spec: dict):
    tokenize_fn = tokenize_fns[spec["tokenize_fn"]]
    assert spec["dataset"] == "wiki2", "Only wiki2 implemented atm"
    eval_stride = spec["eval_stride"]
    eval_len = spec["eval_len"]
    max_rows = spec.get("max_rows", 0)
    eval_tokens = tokenize_fn(
        spec["tokenizer_dir"],
        "\n\n".join(
            load_dataset("wikitext", "wikitext-2-raw-v1", split = "test")
            ["text"]
        )
    )
    num_tokens = eval_tokens.shape[-1]
    seqs = []
    for a in range(0, num_tokens - eval_len, eval_stride):
        b = a + eval_len
        seqs.append(eval_tokens[:, a:b])
        if max_rows and len(seqs) >= max_rows:
            break
    eval_tokens = torch.cat(seqs, dim = 0)[:, :]
    return eval_tokens

# Run ppl test

@disk_lru_cache("test_ppl")
def test_ppl(data_spec: dict, spec: dict, logits_file: str):
    load_fn = load_fns[spec["load_fn"]]
    fwd_fn = fwd_fns[spec["fwd_fn"]]
    model_dir = spec["model_dir"]

    print(f"Loading dataset: {data_spec['dataset']}")
    eval_ids = get_test_data(data_spec)
    rows = eval_ids.shape[0]

    print(f"Loading: {model_dir}")
    model_instance, bpw_layer, bpw_head, vram_bits = load_fn(model_dir)
    vram_gb = vram_bits / 8 / 1024**3

    logprob_sum = 0.0
    logprob_count = 0
    kl_div_sum_ab = 0.0
    kl_div_count = 0.0

    print(f"Testing: {model_dir} ({spec['label']})")

    collect_logits = False
    if logits_file:
        if "out_logits" in spec:
            collect_logits = True
            ref_logits = []
        else:
            collect_logits = False
            ref_logits = load_tensor(logits_file)
            if not isinstance(ref_logits, list):
                ref_logits = ref_logits.split(1, 0)

    with ProgressBar("Evaluating", rows) as pb:
        for row in range(rows):
            pb.update(row)
            input_ids = eval_ids[row:row + 1, :]
            logits = fwd_fn(model_instance, input_ids)
            logits = logits.float()

            # kld
            if logits_file and row < 10:
                probs_a = torch.softmax(logits, dim = -1)
                if collect_logits:
                    ref_logits.append(logits.cpu())
                    kl_div_count += 1
                else:
                    probs_b = torch.softmax(ref_logits[row].to(logits.device), dim = -1)
                    vs = min(probs_a.shape[-1], probs_b.shape[-1])
                    probs_a = probs_a[..., :vs]
                    probs_b = probs_b[..., :vs]
                    for r in range(probs_a.shape[1]):
                        kl_div = F.kl_div(torch.log(probs_a[:, r:r+1, :] + 1e-10), probs_b[:, r:r+1, :], reduction = 'sum')
                        kl_div_sum_ab += kl_div.item()
                        kl_div_count += 1
                    del kl_div
                    del probs_b
                del probs_a

            # ppl
            logits = logits[:, :-1, :]
            logits += 1e-10
            log_probs = F.log_softmax(logits, dim = -1)
            del logits
            target_ids = input_ids[:, 1:].to(log_probs.device)
            del input_ids
            target_log_probs = log_probs.gather(-1, target_ids.unsqueeze(-1)).squeeze(-1)
            del log_probs
            logprob_sum += target_log_probs.sum().item()
            logprob_count += target_ids.numel()
            del target_log_probs
            del target_ids
            torch.cuda.empty_cache()

        pb.update(rows)

    mean_log_prob = logprob_sum / logprob_count
    perplexity = math.exp(-mean_log_prob)
    if logits_file:
        kl_div = kl_div_sum_ab / kl_div_count
        print(f"KL div: {kl_div:.6f}")

    if collect_logits:
        save_tensor(ref_logits, logits_file)

    print(f"Perplexity: {perplexity:.6f}")

    del model_instance
    del eval_ids

    free_mem()
    res = {
        "label": spec.get("label", spec.get("model_dir")),
        "layer_bpw": bpw_layer,
        "head_bpw": bpw_head,
        "vram_gb": vram_gb,
        "ppl": perplexity
    }
    if logits_file:
        res.update({
            "kld": kl_div
        })

    return res


def plot(results, args):

    def get_color(s):
        d = {
            "EXL2": "green",
            "EXL3": "purple",
            "AWQ": "olive",
            "imat": "brown",
            "GGUF": "red",
            "VPTQ": "blue",
            "****": "black",
        }
        for k, v in d.items():
            if f"[{v}]" in s:
                return v
        for k, v in d.items():
            if k in s:
                return v
        return "black"

    plt.rcParams["figure.figsize"] = (14, 11)
    plt.subplots_adjust(left = 0.05, right = 0.95, top = 0.95, bottom = 0.05)

    lpoints = {}
    x = []
    y = []
    labels = []
    colors = []
    for r in results:
        x_ = r["vram_gb"] if args.vram else r["layer_bpw"]
        y_ = r["ppl"] if not args.kld else r["kld"]
        if x_ > args.max_x or y_ > args.max_y:
            continue
        x.append(x_)
        y.append(y_)
        labels.append(r["label"].split("[")[0].strip() + f"\n{y_:.3f}")
        color = get_color(r["label"])
        colors.append(color)
        if color != "black":
            if color not in lpoints:
                lpoints[color] = []
            lpoints[color].append((x_, y_))

    plt.scatter(x, y, c = colors, marker = "o")

    texts = []
    for i, label in enumerate(labels):
        texts.append(
            plt.text(
                x[i],
                y[i],
                label,
                fontsize = 8.5,
                ha = "left",
                va = "bottom",
                color = colors[i],
            )
        )
    adjust_text(
        texts,
        x = x,
        y = y,
        arrowprops = {"arrowstyle": "->", "color": "lightgray"},
        expand = (1.35, 2.3),
        ensure_inside_axes = True,
        min_arrow_len = 0.10,
        prevent_crossings = False,
        pull_threshold = 0.20,
        # force_explode = (0.2, 0.6),
        max_move = 100
    )

    for col, lines in lpoints.items():
        x, y = zip(*sorted(lines))
        plt.plot(x, y, color = col, linestyle=':')

    plt.xlabel("VRAM // GB (decoder + head)" if args.vram else "bits per weight (decoder only)")
    plt.ylabel("Perplexity" if not args.kld else "KL divergence")
    plt.title(args.title)
    plt.grid(True)
    plt.show()


def dict_hash(x: dict) -> str:
    import hashlib
    key = str(json.dumps(x, sort_keys = True))
    encoded_string = key.encode('utf-8')
    hash_object = hashlib.sha256(encoded_string)
    hex_digest = hash_object.hexdigest()
    return hex_digest


@torch.inference_mode()
def main(args):
    with open(args.dataspec, "r", encoding = "utf8") as f:
        test_data_spec = json.load(f)

    models_files = args.modelspec
    models_files_g = []
    models_spec = []
    for filename in models_files:
        if "*" in filename:
            models_files_g += glob.glob(filename)
        else:
            models_files_g.append(filename)
    for filename in models_files_g:
        with open(filename, "r", encoding = "utf8") as f:
            m = json.load(f)
            models_spec += m

    if args.logits_file:
        logits_file = args.logits_file
    else:
        logits_file = None
        for idx, spec in enumerate(models_spec):
            if "out_logits" in spec:
                logits_dir = spec["out_logits"]
                if not os.path.exists(logits_dir):
                    os.makedirs(logits_dir)
                logits_file = os.path.join(logits_dir, dict_hash(test_data_spec) + ".safetensors")
                logits_idx = idx
        if logits_file is not None:
            models_spec = [models_spec[logits_idx]] + models_spec[:logits_idx] + models_spec[logits_idx + 1:]

    if args.mask:
        masks = args.mask.split(";")
        ms = []
        for spec in models_spec:
            if any(m.upper() in spec["label"].upper() for m in masks):
                ms.append(spec)
        models_spec = ms

    if args.clear_cache:
        for spec in models_spec:
            disk_lru_cache_clear("test_ppl", test_data_spec, spec, logits_file)

    results = []
    for spec in models_spec:
        r = test_ppl(test_data_spec, spec, logits_file)
        print(r)
        results.append(r)

    print("------")
    print(json.dumps(results, indent = 4))

    if args.plot:
        plot(results, args)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-d", "--dataspec", type = str, help = "Data specification (JSON file)")
    parser.add_argument("-m", "--modelspec", type = str, nargs="+", help = "Model specification (JSONL file), accepts wildcard")
    parser.add_argument("-cc", "--clear_cache", action = "store_true", help = "Clear cache")
    parser.add_argument("-p", "--plot", action = "store_true", help = "Scatter plot")
    parser.add_argument("-v", "--vram", action = "store_true", help = "Use VRAM footprint as scatter plot X axis")
    parser.add_argument("-mx", "--max_x", type = float, default = 999999, help = "Don't plot results beyond X value")
    parser.add_argument("-my", "--max_y", type = float, default = 999999, help = "Don't plot results beyond Y value")
    parser.add_argument("-t", "--title", type = str, default = "Very plot", help = "Plot title")
    parser.add_argument("-kld", "--kld", action = "store_true", help = "Test KL divergence")
    parser.add_argument("-mask", "--mask", type = str, help = "Semicolon-separated list of strings to match against model labels for inclusion")
    parser.add_argument("-lf", "--logits_file", type = str, help = "Reference logits file for KLD", required = False)
    _args = parser.parse_args()
    main(_args)



</content>

<content full_path="eval/model_diff.py">
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import argparse
from exllamav3.util.file import disk_lru_cache
from exllamav3.util.progress import ProgressBar
from exllamav3.util.memory import free_mem
from exllamav3.util.measures import cosine_error, sqnr
from exllamav3 import Config, Model, Tokenizer
from datasets import load_dataset
import torch
import torch.nn.functional as F
import math


@disk_lru_cache("get_dataset_text")
def get_dataset_text(spec: dict):
    assert spec["dataset"] == "wiki2", "Only wiki2 implemented atm"
    dataset_text = "\n\n".join(
        load_dataset("wikitext", "wikitext-2-raw-v1", split = "test")
        ["text"]
    )
    return dataset_text


def get_test_tokens(tokenizer, rows, eval_len = 2048, eval_stride = 512):
    with ProgressBar("Tokenizing", rows) as pb:
        dataset_spec = { "dataset": "wiki2" }
        eval_tokens = tokenizer.encode(get_dataset_text(dataset_spec))
        num_tokens = eval_tokens.shape[-1]
        seqs = []
        for a in range(0, num_tokens - eval_len, eval_stride):
            b = a + eval_len
            seqs.append(eval_tokens[:, a:b])
            pb.update(len(seqs))
            if len(seqs) >= rows:
                break
    return torch.cat(seqs, dim = 0)[:, :]


@torch.inference_mode()
def main(args):

    device = torch.device(args.device)

    config_a = Config.from_directory(args.model_a)
    config_a.override_dynamic_seq_len(2048)
    tokenizer = Tokenizer.from_config(config_a)
    model_a = Model.from_config(config_a)

    config_b = Config.from_directory(args.model_b)
    config_b.override_dynamic_seq_len(2048)
    model_b = Model.from_config(config_b)

    # Dataset
    eval_ids = get_test_tokens(tokenizer, args.rows)
    state_a = eval_ids
    state_b = eval_ids

    for idx, (module_a, module_b) in enumerate(zip(model_a.modules, model_b.modules)):

        config_a.stc.begin_deferred_load()
        module_a.load(device if not module_a.caps.get("prefer_cpu") else "cpu")
        config_a.stc.end_deferred_load()
        params_a = {}
        state_a = module_a.prepare_for_device(state_a, params_a)
        state_a = module_a.forward(state_a, params_a)
        module_a.unload()
        config_a.stc.close()
        free_mem()

        config_b.stc.begin_deferred_load()
        module_b.load(device if not module_b.caps.get("prefer_cpu") else "cpu")
        config_b.stc.end_deferred_load()
        params_b = {}
        state_b = module_b.prepare_for_device(state_b, params_b)
        state_b = module_b.forward(state_b, params_b)
        module_b.unload()
        config_b.stc.close()
        free_mem()

        if idx < args.keep_b:
            state_a = state_b.clone()

        max_diff = 0
        rfn_error_sum = 0
        cos_error_sum = 0
        sqnr_sum = 0
        rows = state_a.shape[0]
        for i in range(rows):
            sa = state_a[i].to(float, copy = True)
            sb = state_b[i].to(float)
            cos_error_sum += cosine_error(sa, sb)
            sqnr_sum += sqnr(sa, sb)
            sa -= sb
            rfn_error_sum += (torch.linalg.norm(sa, 'fro') / torch.linalg.norm(sb, 'fro').mean()).item()
            sa.abs_()
            md = ((sa.max().item()) / torch.linalg.norm(sb, 'fro').mean()).item()
            max_diff = max(max_diff, md)

        del sa, sb
        rfn_error = rfn_error_sum / rows
        cos_error = cos_error_sum / rows
        sqnr_ = sqnr_sum / rows
        print(
            f" -- {module_a.key:40}"
            f"   rfn_err: {rfn_error:.6f}"
            f"   max_diff/norm: {max_diff:.6f}"
            f"   sqnr: {sqnr_:9.6f}"
            f"   cos_err: {cos_error:.6f}"
        )

    # Compare logits
    topk_max = args.topk_max
    logprob_sum = [0, 0]
    logprob_count = [0, 0]
    kl_div_sum_ab = 0
    kl_div_sum_ba = 0
    topk_hits_sum = [[0] * topk_max, [0] * topk_max]
    topk_hits_count = [[0] * topk_max, [0] * topk_max]
    topk_agreement_sum = [0] * topk_max
    topk_agreement_count = [0] * topk_max

    def ppl(input_ids_, logits_):
        nonlocal logprob_sum, logprob_count
        logprob_sum_ = 0.0
        logprob_count_ = 0
        chunksize = logits_.shape[1] * 10240 // logits_.shape[1]
        b_ = 0
        while b_ < logits_.shape[1]:
            a_ = b_
            b_ = min(b_ + chunksize, logits_.shape[1])
            logits_f = logits_[a_:b_, :].float() + 1e-10
            target_ids = input_ids_[a_ + 1:b_ + 1].to(logits_.device)
            log_probs = F.log_softmax(logits_f, dim=-1)
            token_log_probs = log_probs.gather(-1, target_ids.unsqueeze(-1)).squeeze(-1)
            logprob_sum_ += token_log_probs.sum().item()
            logprob_count_ += target_ids.numel()
        return logprob_sum_, logprob_count_

    rows = state_a.shape[0]
    for j in range(rows):
        x = (state_a[j], state_b[j])
        input_ids = eval_ids[j]
        top_indices = []

        for i in [0, 1]:
            logits = x[i][:-1, :]
            logprob_sum__, logprob_count__ = ppl(input_ids, logits)
            logprob_sum[i] += logprob_sum__
            logprob_count[i] += logprob_count__

            _, top_index = torch.topk(logits, topk_max, dim = -1)
            top_index = top_index.cpu().view(-1, topk_max)
            top_indices.append(top_index)
            targets = input_ids[1:].view(-1, 1)

            for t in range(topk_max):
                top_slice = top_index[:, :t + 1]
                hits = torch.eq(targets, top_slice)
                row_hits = hits.any(dim = 1)
                topk_hits_sum[i][t] += row_hits.sum().item()
                topk_hits_count[i][t] += top_slice.shape[0]

        for t in range(topk_max):
            top_slice_a = top_indices[0][:, :t + 1]
            top_slice_b = top_indices[1][:, :t + 1]
            hits = torch.eq(top_slice_a, top_slice_b)
            row_hits = hits.all(dim = 1)
            topk_agreement_sum[t] += row_hits.sum().item()
            topk_agreement_count[t] += top_slice_a.shape[0]

        epsilon = 1e-10
        probs_a = torch.softmax(x[0].float(), dim = -1)
        probs_b = torch.softmax(x[1].float(), dim = -1)
        kl_div = F.kl_div(torch.log(probs_a + epsilon), probs_b, reduction = 'none')
        kl_div_sum_ab += kl_div.sum(dim = -1).mean().item()
        kl_div = F.kl_div(torch.log(probs_b + epsilon), probs_a, reduction = 'none')
        kl_div_sum_ba += kl_div.sum(dim = -1).mean().item()

    perplexity = [math.exp(-logprob_sum[i] / logprob_count[i]) for i in (0, 1)]
    kl_div_ab = kl_div_sum_ab / rows
    kl_div_ba = kl_div_sum_ba / rows

    # Perplexity for each model
    print(f" -- A perplexity: {perplexity[0]:11.8f}")
    print(f" -- B perplexity: {perplexity[1]:11.8f}")

    # Probability of the test label being in the top K tokens, for each model
    print(f" -- A label in top-K:")
    for t in range(topk_max):
        a_acc_ = topk_hits_sum[0][t] / topk_hits_count[0][t]
        print(f"      K = {t+1}: {a_acc_:6.4f}")
    print(f" -- B label in top-K:")
    for t in range(topk_max):
        a_acc_ = topk_hits_sum[1][t] / topk_hits_count[1][t]
        print(f"      K = {t+1}: {a_acc_:6.4f}")

    # Probability of exact top-K token match between models
    print(f" -- Top-K agreement, A vs B:")
    for t in range(topk_max):
        topk_agree_ = topk_agreement_sum[t] / topk_agreement_count[t]
        print(f"      K = {t+1}: {topk_agree_:6.4f}")

    # KLD, either way around
    print(f" -- KL divergence (A, B): {kl_div_ab:11.8f}")
    print(f" -- KL divergence (B, A): {kl_div_ba:11.8f}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("-ma", "--model_a", type = str, help = "Model A", required = True)
    parser.add_argument("-mb", "--model_b", type = str, help = "Model B", required = True)
    parser.add_argument("-r", "--rows", type = int, help = "Number of rows", default = 100)
    parser.add_argument("-kb", "--keep_b", type = int, help = "Maintain B state for number of modules", default = 0)
    parser.add_argument("-tkm", "--topk_max", type = int, default = 5, help = "Max top-K interval to test")
    parser.add_argument("-d", "--device", type = int, help = "CUDA device index", default = 0)

    _args = parser.parse_args()
    main(_args)

</content>

<content full_path="eval/humaneval.py">
from __future__ import annotations
import os, sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from exllamav3 import model_init, Generator, Job, ComboSampler
from exllamav3.util.progress import ProgressBar
import argparse, contextlib, subprocess
from human_eval.data import write_jsonl, read_problems
from pathlib import Path

# Prompt formats
prompt_formats = {
    "raw": (
        "```python\n{{problem}}    ",
        "    "
    ),
    "granite": (
        "<|endoftext|>Question:\nComplete the following Python function:\n\n{{problem}}\n\nAnswer:\n"
        "Sure! Here is how you might implement the function:\n\n```python\n{{problem}}",
        "    "
    ),
    "llama": (
        "<s>[INST] <<SYS>>\n"
        "You are a helpful AI coding assistant.\n"
        "<</SYS>>\n\n"
        "Complete the following Python function:\n\n"
        "{{problem}} [/INST] "
        "Sure! Here is how you might implement the function:\n\n```python\n{{problem}}",
        "    "
    ),
    "llama3": (
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n"
        "You are a helpful AI coding assistant.<|eot_id|>"
        "<|start_header_id|>user<|end_header_id|>\n\n"
        "Complete the following Python function:\n\n{{problem}}<|eot_id|>"
        "<|start_header_id|>assistant<|end_header_id|>\n\n"
        "Sure! Here is how you might implement the function:\n\n```python\n{{problem}}",
        "    "
    ),
    "mistral": (
        "<s>[INST] You are a helpful AI coding assistant.\n\n"
        "Complete the following Python function:\n\n"
        "{{problem}}[/INST]"
        " Sure! Here is how you might implement the function:\n\n```python\n{{problem}}",
        "    "
    ),
    "gemma": (
        "<bos><start_of_turn>user\n"
        "Complete the following Python function:\n\n{{problem}}<|eot_id|>"
        "<start_of_turn>model\n"
        "```python\n{{problem}}",
        "    "
    ),
    "reka": (
        "<|endoftext|>human: Complete the following Python function."
        " Provide your reasoning in comments, but be concise and don't second-guess."
        "\n\n{{problem}}"
        " <sep> assistant: ```python\n{{problem}}",
        "    "
    ),
    "chatml": (
        "<|im_start|>system\n"
        "You are a helpful AI coding assistant.<|im_end|>\n"
        "<|im_start|>user\n"
        "Complete the following Python function:\n\n{{problem}}<|im_end|>\n"
        "<|im_start|>assistant\n"
        "Sure! Here is how you might implement the function:\n\n```python\n{{problem}}",
        "    "
    ),
    "qwen3": (
        "<|im_start|>system\n"
        "You are a helpful AI coding assistant.<|im_end|>\n"
        "<|im_start|>user\n"
        "Complete the following Python function:\n\n{{problem}}<|im_end|>\n"
        "<|im_start|>assistant\n"
        "<think>\n\n</think>\n\nSure! Here is how you might implement the function:\n\n```python\n{{problem}}",
        "    "
    ),
    "deepseek": (
        "<｜begin▁of▁sentence｜>You are a helpful AI coding assistant.\n"
        "<｜User｜>Complete the following Python function:\n\n{{problem}}"
        "<｜Assistant｜>Sure! Here is how you might implement the function:\n\n```python\n{{problem}}",
        "    "
    )
}

def main(args):

    # Validate args
    directory = os.path.dirname(args.output)
    if os.path.exists(args.output):
        print(f" !! Warning: Output file exists and will be overwritten.")

    if args.prompt_format is None:
        prompt_format, prefix = "{{problem}}", "    "
    elif args.prompt_format in prompt_formats:
        prompt_format, prefix = prompt_formats[args.prompt_format]
    else:
        print("Prompt format is not supported. Available formats:")
        print("\n".join(prompt_formats.keys()))
        sys.exit()

    # Initialize
    model, config, cache, tokenizer = model_init.init(args)
    generator = Generator(
        model = model,
        cache = cache,
        max_batch_size = 256,
        tokenizer = tokenizer
    )
    sampler = ComboSampler(
        temperature = args.temperature,
        min_p = args.min_p,
        top_k = args.top_k,
        top_p = args.top_p,
        temp_last = args.temp_last
    )

    # Get problems
    problems = read_problems()
    num_samples_per_task = args.samples_per_task

    # Create jobs
    with ProgressBar("Creating sample jobs", len(problems), transient = False) as progress:
        for idx, (problem_id, problem) in enumerate(problems.items()):
            b_problem = problem["prompt"]
            f_problem = prompt_format.replace("{{problem}}", b_problem)
            input_ids = tokenizer.encode(
                f_problem,
                encode_special_tokens = True,
                add_bos = (args.prompt_format == "raw")
            )
            for s in range(num_samples_per_task):
                job = Job(
                    input_ids = input_ids,
                    sampler = sampler,
                    max_new_tokens = args.max_tokens,
                    stop_conditions = [tokenizer.eos_token_id],
                    token_healing = True,
                    identifier = (problem_id, s),
                    min_new_tokens = 6
                )
                generator.enqueue(job)
            progress.update(idx)

    # Collect samples here
    samples = []

    # Work
    total_jobs = generator.num_remaining_jobs()
    with ProgressBar("Generating samples" if not args.verbose else None, total_jobs, transient = False) as progress:

        while generator.num_remaining_jobs():
            results = generator.iterate()
            for result in results:

                # End sample if generator says EOS or if there is a non-indented line at the end of the output
                job = result["job"]
                eos = False
                completion = job.full_completion
                last_newline_index = completion.rfind("\n")
                if last_newline_index >= 0:
                    last_line = completion[last_newline_index + 1:]
                    if last_line != "" and not last_line[0].isspace():
                        completion = completion[:last_newline_index]
                        eos = True
                eos = eos or result["eos"]

                # Collect completed sample
                if eos:
                    identifier = result["identifier"]
                    sample = problems[identifier[0]]["prompt"] + prefix + completion.strip()
                    if not result["eos"]:
                        generator.cancel(job)

                    if args.verbose:
                        print("----------------------------------------------------------------------")
                        print(f" ** Problem {identifier[0]}, sample {identifier[1] + 1} / {num_samples_per_task}")
                        print("----------------------------------------------------------------------")
                        print(sample)
                        print()
                    progress.update(total_jobs - generator.num_remaining_jobs())
                    samples.append(dict(task_id = identifier[0], completion = prefix + completion.strip()))

    # Save output
    print(f" -- Saving: {args.output}")
    Path(directory).mkdir(parents = True, exist_ok = True)
    write_jsonl(args.output, samples)

    # Optionally launch eval script
    if args.eval:
        subprocess.run(["evaluate_functional_correctness", args.output])


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description = "Run HumanEval evaluation")
    model_init.add_args(parser)
    parser.add_argument("-o", "--output", type = str, help = "Output .jsonl filename", required = True)
    parser.add_argument("-spt", "--samples_per_task", type = int, default = 200)
    parser.add_argument("-pf", "--prompt_format", type = str, help = "Instruct format to apply. Default is raw completion (for base models) ")
    parser.add_argument("-v", "--verbose", action = "store_true", help = "Spam completions to console while generating")
    parser.add_argument("-e", "--eval", action = "store_true", help = "Run evaluation script on output file after sampling")
    parser.add_argument("-temp", "--temperature", type = float, help = "Sampling temperature (0 for greedy), default: 0.6", default = 0.6)
    parser.add_argument("-minp", "--min_p", type = float, help = "Min-p sampling, default: 0.0 (disabled)", default = 0.0)
    parser.add_argument("-topk", "--top_k", type = int, help = "Top-k sampling, default: 0 (disabled)", default = 0)
    parser.add_argument("-topp", "--top_p", type = float, help = "Top-p sampling, default: 0.6", default = 0.6)
    parser.add_argument("-templast", "--temp_last", action = "store_true", help = "Use temperature last")
    parser.add_argument("--max_tokens", type = int, default = 768, help = "Max number of tokens for each completion")
    _args = parser.parse_args()
    main(_args)

</content>

<content full_path="eval/spec/llama3.2-1b-instruct_exl2.json">
[
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 2.5bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/exl2/2.5bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 3.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/exl2/3.0bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 3.5bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/exl2/3.5bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 4.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/exl2/4.0bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 4.5bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/exl2/4.5bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 5.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/exl2/5.0bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 6.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/exl2/6.0bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 8.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/exl2/8.0bpw/"
    }
]
</content>

<content full_path="eval/spec/llama3.2-1b-instruct_bnb.json">
[
    {
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/bnb/bnb-4bit/",
        "load_fn": "transformers",
        "fwd_fn": "transformers",
        "label": "BNB 4-bit"
    }
]
</content>

<content full_path="eval/spec/wiki2_llama3.json">
{
    "tokenize_fn": "transformers",
    "tokenizer_dir": "/mnt/str/models/llama3.2-1b-instruct/hf/",
    "dataset": "wiki2",
    "eval_stride": 512,
    "eval_len": 2048,
    "max_rows": 20
}
</content>

<content full_path="eval/spec/llama3.2-1b-instruct_exl3.json">
[
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 2.0bpw H3",
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/exl3/2.0bpw_H3/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 2.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/exl3/2.0bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 2.25bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/exl3/2.25bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 2.5bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/exl3/2.5bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 3.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/exl3/3.0bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 3.5bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/exl3/3.5bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 4.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/exl3/4.0bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 5.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/exl3/5.0bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 6.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/exl3/6.0bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 8.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/exl3/8.0bpw/"
    }
]
</content>

<content full_path="eval/spec/llama3.1-8b-instruct_autoround.json">
[
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/autoround/4bit-asym",
        "load_fn": "transformers",
        "fwd_fn": "transformers",
        "label": "AutoRound 4bit asym"
    }
]
</content>

<content full_path="eval/spec/llama3.1-8b-instruct_aqlm.json">
[
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/aqlm/2bit-1x16-g8",
        "load_fn": "transformers",
        "fwd_fn": "transformers",
        "label": "AQLM 2bit 1x16-g8"
    },
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/aqlm/2bit-1x16-g8-pv",
        "load_fn": "transformers",
        "fwd_fn": "transformers",
        "label": "AQLM 2bit 1x16-g8-pv"
    },
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/aqlm/2bit-2x8-g8-pv",
        "load_fn": "transformers",
        "fwd_fn": "transformers",
        "label": "AQLM 2bit 2x8-g8-pv"
    }
]
</content>

<content full_path="eval/spec/llama3.1-8b-instruct_vptq.json">
[
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/vptq/v8-k65536-256-woft",
        "load_fn": "transformers",
        "fwd_fn": "transformers",
        "label": "VPTQ v8-k65536-256-woft"
    },
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/vptq/v8-k65536-4096-woft",
        "load_fn": "transformers",
        "fwd_fn": "transformers",
        "label": "VPTQ v8-k65536-4096-woft"
    },
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/vptq/v8-k65536-65536-woft",
        "load_fn": "transformers",
        "fwd_fn": "transformers",
        "label": "VPTQ v8-k65536-65536-woft"
    },
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/vptq/v12-k65536-4096-woft",
        "load_fn": "transformers",
        "fwd_fn": "transformers",
        "label": "VPTQ v12-k65536-4096-woft"
    }
]
</content>

<content full_path="eval/spec/llama3.2-1b-instruct_gguf.json">
[
    {
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/gguf/Llama-3.2-1B-Instruct.i1-IQ2_M.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ2_M imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/gguf/Llama-3.2-1B-Instruct.i1-IQ2_XS.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ2_XS imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/gguf/Llama-3.2-1B-Instruct.i1-IQ2_XXS.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ2_XXS imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/gguf/Llama-3.2-1B-Instruct.i1-IQ3_M.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ3_M imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/gguf/Llama-3.2-1B-Instruct.i1-IQ3_XS.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ3_XS imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/gguf/Llama-3.2-1B-Instruct.i1-IQ4_XS.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ4_XS imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/gguf/Llama-3.2-1B-Instruct.i1-Q5_K_S.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF Q5_K_S imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/gguf/Llama-3.2-1B-Instruct.i1-Q6_K.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF Q6_K imat"
    }
]




</content>

<content full_path="eval/spec/llama3.1-70b-instruct_vptq.json">
[
    {
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/vptq/v8-k65536-0-woft",
        "load_fn": "transformers_auto",
        "fwd_fn": "transformers",
        "label": "VPTQ v8-k65536-0-woft"
    },
    {
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/vptq/v16-k65536-32768-woft",
        "load_fn": "transformers_auto",
        "fwd_fn": "transformers",
        "label": "VPTQ v16-k65536-32768-woft"
    }
]

</content>

<content full_path="eval/spec/llama3.1-70b-instruct_aqlm.json">
[
    {
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/aqlm/2bit-1x16-pv-g1/",
        "load_fn": "transformers",
        "fwd_fn": "transformers",
        "label": "AQLM 2bit 1x16-g1"
    }
]
</content>

<content full_path="eval/spec/llama3.1-70b-instruct_awq.json">
[
    {
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/awq/4bit",
        "load_fn": "transformers_auto",
        "fwd_fn": "transformers",
        "label": "AWQ 4bit"
    }
]

</content>

<content full_path="eval/spec/llama3.1-8b-instruct_hf.json">
[
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/hf",
        "load_fn": "transformers_auto",
        "fwd_fn": "transformers",
        "label": "HF FP16",
        "out_logits": "/mnt/str/models/llama3.1-8b-instruct/ref_logits"
    }
]
</content>

<content full_path="eval/spec/llama3.1-70b-instruct_gguf.json">
[
    {
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/gguf/llama-3.1-70b-instruct-iq1_m.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ1_M imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/gguf/llama-3.1-70b-instruct-iq2_s.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ2_S imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/gguf/llama-3.1-70b-instruct-iq2_xxs.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ2_XXS imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/gguf/Meta-Llama-3.1-70B-Instruct.i1-IQ1_S.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ1_S imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/gguf/Meta-Llama-3.1-70B-Instruct.i1-IQ3_M.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ3_M imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/gguf/Meta-Llama-3.1-70B-Instruct.i1-IQ3_S.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ3_S imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/gguf/Meta-Llama-3.1-70B-Instruct.i1-IQ3_XXS.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ3_XXS imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/gguf/Meta-Llama-3.1-70B-Instruct.i1-IQ4_XS.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ4_XS imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/gguf/Meta-Llama-3.1-70B-Instruct.i1-Q4_K_M.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF Q4_K_M imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/gguf/Meta-Llama-3.1-70B-Instruct.i1-Q5_K_M.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF Q5_K_M imat"
    }
]

</content>

<content full_path="eval/spec/mistral-7b-instruct-v0.3_exl3.json">
[
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 2.0bpw H6",
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/exl3/2.0bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 2.25bpw H6",
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/exl3/2.25bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 2.5bpw H6",
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/exl3/2.5bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 3.0bpw H6",
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/exl3/3.0bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 3.5bpw H6",
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/exl3/3.5bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 4.0bpw H6",
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/exl3/4.0bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 5.0bpw H6",
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/exl3/5.0bpw/"
    },
        {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 6.0bpw H6",
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/exl3/6.0bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 8.0bpw H6",
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/exl3/8.0bpw/"
    }
]
</content>

<content full_path="eval/spec/mistral-7b-instruct-v0.3_exl2.json">
[
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 2.8bpw H6",
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/exl2/2.8bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 3.0bpw H6",
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/exl2/3.0bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 4.0bpw H6",
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/exl2/4.0bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 4.5bpw H6",
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/exl2/4.5bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 5.0bpw H6",
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/exl2/5.0bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 6.0bpw H6",
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/exl2/6.0bpw/"
    }
]
</content>

<content full_path="eval/spec/llama3.2-1b-instruct_aqlm.json">
[
    {
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/aqlm/2bit-2x8-g1-pv",
        "load_fn": "transformers",
        "fwd_fn": "transformers",
        "label": "AQLM 2bit 2x8-g1-pv"
    }
]
</content>

<content full_path="eval/spec/llama3.1-8b-instruct_gguf.json">
[
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/gguf/Meta-Llama-3.1-8B-Instruct.i1-IQ1_M.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ1_M imat"
    },
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/gguf/Meta-Llama-3.1-8B-Instruct.i1-IQ2_M.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ2_M imat"
    },
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/gguf/Meta-Llama-3.1-8B-Instruct.i1-IQ2_S.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ2_S imat"
    },
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/gguf/Meta-Llama-3.1-8B-Instruct.i1-IQ2_XS.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ2_XS imat"
    },
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/gguf/Meta-Llama-3.1-8B-Instruct.i1-IQ2_XXS.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ2_XXS imat"
    },
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/gguf/Meta-Llama-3.1-8B-Instruct.i1-IQ3_M.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ3_M imat"
    },
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/gguf/Meta-Llama-3.1-8B-Instruct.i1-IQ3_S.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ3_S imat"
    },
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/gguf/Meta-Llama-3.1-8B-Instruct.i1-IQ3_XS.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ3_XS imat"
    },
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/gguf/Meta-Llama-3.1-8B-Instruct.i1-IQ4_XS.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ4_XS imat"
    },
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/gguf/Meta-Llama-3.1-8B-Instruct.i1-Q4_K_M.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF Q4_K_M imat"
    },
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/gguf/Meta-Llama-3.1-8B-Instruct.i1-Q5_K_M.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF Q5_K_M imat"
    },
    {
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/gguf/Meta-Llama-3.1-8B-Instruct.i1-Q6_K.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF Q6_K imat"
    }
]
</content>

<content full_path="eval/spec/wiki2_mistral_large.json">
{
    "tokenize_fn": "transformers",
    "tokenizer_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/hf/",
    "dataset": "wiki2",
    "eval_stride": 512,
    "eval_len": 2048,
    "max_rows": 100
}
</content>

<content full_path="eval/spec/llama3.1-70b-instruct_exl2.json">
[
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 2.4bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl2/2.4bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 2.8bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl2/2.8bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 3.5bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl2/3.5bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 4.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl2/4.0bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 4.5bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl2/4.5bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 5.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl2/5.0bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 6.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl2/6.0bpw/"
    }
]
</content>

<content full_path="eval/spec/wiki2_llama3_large.json">
{
    "tokenize_fn": "transformers",
    "tokenizer_dir": "/mnt/str/eval_models/llama3.2-1b/hf/",
    "dataset": "wiki2",
    "eval_stride": 512,
    "eval_len": 2048,
    "max_rows": 100
}
</content>

<content full_path="eval/spec/llama3.1-8b-instruct_exl3.json">
[
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 1.7bpw H3",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl3/1.7bpw_H3/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 1.8bpw H3",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl3/1.8bpw_H3/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 1.9bpw H3",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl3/1.9bpw_H3/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 2.0bpw H6",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl3/2.0bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 2.25bpw H6",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl3/2.25bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 2.5bpw H6",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl3/2.5bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 3.0bpw H6",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl3/3.0bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 3.5bpw H6",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl3/3.5bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 4.0bpw H6",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl3/4.0bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 5.0bpw H6",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl3/5.0bpw/"
    },
        {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 6.0bpw H6",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl3/6.0bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 8.0bpw H6",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl3/8.0bpw/"
    }
]
</content>

<content full_path="eval/spec/llama3.1-8b-instruct_exl2.json">
[
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 3.0bpw H6",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl2/3.0bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 3.5bpw H6",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl2/3.5bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 4.0bpw H6",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl2/4.0bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 4.5bpw H6",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl2/4.5bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 5.0bpw H6",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl2/5.0bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 6.0bpw H6",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl2/6.0bpw/"
    },
    {
        "load_fn": "exllamav2",
        "fwd_fn": "exllamav2",
        "label": "EXL2 8.0bpw H6",
        "model_dir": "/mnt/str/models/llama3.1-8b-instruct/exl2/8.0bpw/"
    }
]
</content>

<content full_path="eval/spec/mistral-7b-instruct-v0.3_gguf.json">
[
    {
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/gguf/Mistral-7B-Instruct-v0.3.i1-IQ2_XS.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ2_XS imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/gguf/Mistral-7B-Instruct-v0.3.i1-IQ2_S.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ2_S imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/gguf/Mistral-7B-Instruct-v0.3.i1-IQ3_XS.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ3_XS imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/gguf/Mistral-7B-Instruct-v0.3.i1-IQ4_XS.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ4_XS imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/gguf/Mistral-7B-Instruct-v0.3.i1-Q2_K.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF Q2_K imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/gguf/Mistral-7B-Instruct-v0.3.i1-Q3_K_M.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF Q3_K_M imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/gguf/Mistral-7B-Instruct-v0.3.i1-Q6_K.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF Q6_K imat"
    },
    {
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/gguf/Mistral-7B-Instruct-v0.3.i1-IQ3_XXS.gguf",
        "load_fn": "llamacpp",
        "fwd_fn": "llamacpp",
        "label": "GGUF IQ3_XXS imat"
    }
]
</content>

<content full_path="eval/spec/llama3.2-1b-instruct_awq.json">
[
    {
        "model_dir": "/mnt/str/eval_models/llama3.2-1b-instruct/awq/4bit",
        "load_fn": "transformers",
        "fwd_fn": "transformers",
        "label": "AWQ 4-bit"
    }
]
</content>

<content full_path="eval/spec/llama3.1-70b-instruct_exl3.json">
[
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 1.6bpw H3",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl3_x/1.6bpw_H3/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 1.6bpw H3",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl3/1.6bpw_H3/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 1.8bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl3/1.8bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 2.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl3/2.0bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 2.25bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl3/2.25bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 2.5bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl3/2.5bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 3.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl3/3.0bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 3.5bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl3/3.5bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 4.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl3/4.0bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 4.5bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl3/4.5bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 5.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl3/5.0bpw/"
    },
    {
        "load_fn": "exllamav3",
        "fwd_fn": "exllamav3",
        "label": "EXL3 6.0bpw H6",
        "model_dir": "/mnt/str/eval_models/llama3.1-70b-instruct/exl3/6.0bpw/"
    }
]



</content>

<content full_path="eval/spec/mistral-7b-instruct-v0.3_awq.json">
[
    {
        "model_dir": "/mnt/str/eval_models/mistral-7b-instruct-v0.3/awq/4bit/",
        "load_fn": "transformers_auto",
        "fwd_fn": "transformers",
        "label": "AWQ 4bit"
    }
]

</content>

</repo-to-text>
